Built-in optimizer classes.
### Aliases:
- Module tf.compat.v2.optimizers
## Modules
[schedules](https://tensorflow.google.cn/api_docs/python/tf/compat/v2/keras/optimizers/schedules) module: Public API for tf.keras.optimizers. namespace.

## Classes
[class Adadelta](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adadelta): Optimizer that implements the Adadelta algorithm.

[class Adagrad](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adagrad): Optimizer that implements the Adagrad algorithm.

[class Adam](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adam): Optimizer that implements the Adam algorithm.

[class Adamax](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adamax): Optimizer that implements the Adamax algorithm.

[class Ftrl](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Ftrl): Optimizer that implements the FTRL algorithm.

[class Nadam](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Nadam): Optimizer that implements the NAdam algorithm.

[class Optimizer](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Optimizer): Updated base class for optimizers.

[class RMSprop](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/RMSprop): Optimizer that implements the RMSprop algorithm.

[class SGD](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/SGD): Stochastic gradient descent and momentum optimizer.

## Functions
[deserialize(...)](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/deserialize): Inverse of the serialize function.

[get(...)](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/get): Retrieves a Keras Optimizer instance.

[serialize(...)](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/serialize)

