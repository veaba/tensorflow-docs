Library for running a computation across multiple devices.
See the guide for overview and examples: TensorFlow v1.x, TensorFlow v2.x.
The intent of this library is that you can write an algorithm in a stylized way and it will be usable with a variety of different `tf.distribute.Strategy` implementations. Each descendant will implement a different strategy for distributing the algorithm across multiple devices/machines. Furthermore, these changes can be hidden inside the specific layers and other library classes that need special treatment to run in a distributed setting, so that most users' model definition code can run unchanged. The `tf.distribute.Strategy` API works the same way with eager and graph execution.
Glossary
- ``D``a``t``a`` ``p``a``r``a``l``l``e``l``i``s``m`` ``i``s`` ``w``h``e``r``e`` ``w``e`` ``r``u``n`` ``m``u``l``t``i``p``l``e`` ``c``o``p``i``e``s`` ``o``f`` ``t``h``e`` ``m``o``d``e``l`` ``o``n`` ``d``i``f``f``e``r``e``n``t`` ``s``l``i``c``e``s`` ``o``f`` ``t``h``e`` ``i``n``p``u``t`` ``d``a``t``a``.`` ``T``h``i``s`` ``i``s`` ``i``n`` ``c``o``n``t``r``a``s``t`` ``t``o`` ``m``o``d``e``l`` ``p``a``r``a``l``l``e``l``i``s``m`` ``w``h``e``r``e`` ``w``e`` ``d``i``v``i``d``e`` ``u``p`` ``a`` ``s``i``n``g``l``e`` ``c``o``p``y`` ``o``f`` ``a`` ``m``o``d``e``l`` ``a``c``r``o``s``s`` ``m``u``l``t``i``p``l``e`` ``d``e``v``i``c``e``s``.`` ``N``o``t``e``:`` ``w``e`` ``o``n``l``y`` ``s``u``p``p``o``r``t`` ``d``a``t``a`` ``p``a``r``a``l``l``e``l``i``s``m`` ``f``o``r`` ``n``o``w``,`` ``b``u``t`` ``h``o``p``e`` ``t``o`` ``a``d``d`` ``s``u``p``p``o``r``t`` ``f``o``r`` ``m``o``d``e``l`` ``p``a``r``a``l``l``e``l``i``s``m`` ``i``n`` ``t``h``e`` ``f``u``t``u``r``e``.``
- A device is a CPU or accelerator (e.g. GPUs, TPUs) on some machine that TensorFlow can run operations on (see e.g. `tf.device`). You may have multiple devices on a single machine, or be connected to devices on multiple machines. Devices used to run computations are called worker devices. Devices used to store variables are parameter devices. For some strategies, such as `tf.distribute.MirroredStrategy`, the worker and parameter devices will be the same (see mirrored variables below). For others they will be different. For example, `tf.distribute.experimental.CentralStorageStrategy` puts the variables on a single device (which may be a worker device or may be the CPU), and `tf.distribute.experimental.ParameterServerStrategy` puts the variables on separate machines called parameter servers (see below).
- ``A`` ``r``e``p``l``i``c``a`` ``i``s`` ``o``n``e`` ``c``o``p``y`` ``o``f`` ``t``h``e`` ``m``o``d``e``l``,`` ``r``u``n``n``i``n``g`` ``o``n`` ``o``n``e`` ``s``l``i``c``e`` ``o``f`` ``t``h``e`` ``i``n``p``u``t`` ``d``a``t``a``.`` ``R``i``g``h``t`` ``n``o``w`` ``e``a``c``h`` ``r``e``p``l``i``c``a`` ``i``s`` ``e``x``e``c``u``t``e``d`` ``o``n`` ``i``t``s`` ``o``w``n`` ``w``o``r``k``e``r`` ``d``e``v``i``c``e``,`` ``b``u``t`` ``o``n``c``e`` ``w``e`` ``a``d``d`` ``s``u``p``p``o``r``t`` ``f``o``r`` ``m``o``d``e``l`` ``p``a``r``a``l``l``e``l``i``s``m`` ``a`` ``r``e``p``l``i``c``a`` ``m``a``y`` ``s``p``a``n`` ``m``u``l``t``i``p``l``e`` ``w``o``r``k``e``r`` ``d``e``v``i``c``e``s``.``
- ``A`` ``h``o``s``t`` ``i``s`` ``t``h``e`` ``C``P``U`` ``d``e``v``i``c``e`` ``o``n`` ``a`` ``m``a``c``h``i``n``e`` ``w``i``t``h`` ``w``o``r``k``e``r`` ``d``e``v``i``c``e``s``,`` ``t``y``p``i``c``a``l``l``y`` ``u``s``e``d`` ``f``o``r`` ``r``u``n``n``i``n``g`` ``i``n``p``u``t`` ``p``i``p``e``l``i``n``e``s``.``
- ``A`` ``w``o``r``k``e``r`` ``i``s`` ``d``e``f``i``n``e``d`` ``t``o`` ``b``e`` ``t``h``e`` ``p``h``y``s``i``c``a``l`` ``m``a``c``h``i``n``e``(``s``)`` ``c``o``n``t``a``i``n``i``n``g`` ``t``h``e`` ``p``h``y``s``i``c``a``l`` ``d``e``v``i``c``e``s`` ``(``e``.``g``.`` ``G``P``U``s``,`` ``T``P``U``s``)`` ``o``n`` ``w``h``i``c``h`` ``t``h``e`` ``r``e``p``l``i``c``a``t``e``d`` ``c``o``m``p``u``t``a``t``i``o``n`` ``i``s`` ``e``x``e``c``u``t``e``d``.`` ``A`` ``w``o``r``k``e``r`` ``m``a``y`` ``c``o``n``t``a``i``n`` ``o``n``e`` ``o``r`` ``m``o``r``e`` ``r``e``p``l``i``c``a``s``,`` ``b``u``t`` ``c``o``n``t``a``i``n``s`` ``a``t`` ``l``e``a``s``t`` ``o``n``e`` ``r``e``p``l``i``c``a``.`` ``T``y``p``i``c``a``l``l``y`` ``o``n``e`` ``w``o``r``k``e``r`` ``w``i``l``l`` ``c``o``r``r``e``s``p``o``n``d`` ``t``o`` ``o``n``e`` ``m``a``c``h``i``n``e``,`` ``b``u``t`` ``i``n`` ``t``h``e`` ``c``a``s``e`` ``o``f`` ``v``e``r``y`` ``l``a``r``g``e`` ``m``o``d``e``l``s`` ``w``i``t``h`` ``m``o``d``e``l`` ``p``a``r``a``l``l``e``l``i``s``m``,`` ``o``n``e`` ``w``o``r``k``e``r`` ``m``a``y`` ``s``p``a``n`` ``m``u``l``t``i``p``l``e`` ``m``a``c``h``i``n``e``s``.`` ``W``e`` ``t``y``p``i``c``a``l``l``y`` ``r``u``n`` ``o``n``e`` ``i``n``p``u``t`` ``p``i``p``e``l``i``n``e`` ``p``e``r`` ``w``o``r``k``e``r``,`` ``f``e``e``d``i``n``g`` ``a``l``l`` ``t``h``e`` ``r``e``p``l``i``c``a``s`` ``o``n`` ``t``h``a``t`` ``w``o``r``k``e``r``.``
- ``S``y``n``c``h``r``o``n``o``u``s``,`` ``o``r`` ``m``o``r``e`` ``c``o``m``m``o``n``l``y`` ``s``y``n``c``,`` ``t``r``a``i``n``i``n``g`` ``i``s`` ``w``h``e``r``e`` ``t``h``e`` ``u``p``d``a``t``e``s`` ``f``r``o``m`` ``e``a``c``h`` ``r``e``p``l``i``c``a`` ``a``r``e`` ``a``g``g``r``e``g``a``t``e``d`` ``t``o``g``e``t``h``e``r`` ``b``e``f``o``r``e`` ``u``p``d``a``t``i``n``g`` ``t``h``e`` ``m``o``d``e``l`` ``v``a``r``i``a``b``l``e``s``.`` ``T``h``i``s`` ``i``s`` ``i``n`` ``c``o``n``t``r``a``s``t`` ``t``o`` ``a``s``y``n``c``h``r``o``n``o``u``s``,`` ``o``r`` ``a``s``y``n``c`` ``t``r``a``i``n``i``n``g``,`` ``w``h``e``r``e`` ``e``a``c``h`` ``r``e``p``l``i``c``a`` ``u``p``d``a``t``e``s`` ``t``h``e`` ``m``o``d``e``l`` ``v``a``r``i``a``b``l``e``s`` ``i``n``d``e``p``e``n``d``e``n``t``l``y``.`` ``Y``o``u`` ``m``a``y`` ``a``l``s``o`` ``h``a``v``e`` ``r``e``p``l``i``c``a``s`` ``p``a``r``t``i``t``i``o``n``e``d`` ``i``n``t``o`` ``g``r``o``u``p``s`` ``w``h``i``c``h`` ``a``r``e`` ``i``n`` ``s``y``n``c`` ``w``i``t``h``i``n`` ``e``a``c``h`` ``g``r``o``u``p`` ``b``u``t`` ``a``s``y``n``c`` ``b``e``t``w``e``e``n`` ``g``r``o``u``p``s``.``
- Parameter servers: These are machines that hold a single copy of parameters/variables, used by some strategies (right now just `tf.distribute.experimental.ParameterServerStrategy`). All replicas that want to operate on a variable retrieve it at the beginning of a step and send an update to be applied at the end of the step. These can in priniciple support either sync or async training, but right now we only have support for async training with parameter servers. Compare to `tf.distribute.experimental.CentralStorageStrategy`, which puts all variables on a single device on the same machine (and does sync training), and `tf.distribute.MirroredStrategy`, which mirrors variables to multiple devices (see below).
- ``M``i``r``r``o``r``e``d`` ``v``a``r``i``a``b``l``e``s``:`` ``T``h``e``s``e`` ``a``r``e`` ``v``a``r``i``a``b``l``e``s`` ``t``h``a``t`` ``a``r``e`` ``c``o``p``i``e``d`` ``t``o`` ``m``u``l``t``i``p``l``e`` ``d``e``v``i``c``e``s``,`` ``w``h``e``r``e`` ``w``e`` ``k``e``e``p`` ``t``h``e`` ``c``o``p``i``e``s`` ``i``n`` ``s``y``n``c`` ``b``y`` ``a``p``p``l``y``i``n``g`` ``t``h``e`` ``s``a``m``e`` ``u``p``d``a``t``e``s`` ``t``o`` ``e``v``e``r``y`` ``c``o``p``y``.`` ``N``o``r``m``a``l``l``y`` ``w``o``u``l``d`` ``o``n``l``y`` ``b``e`` ``u``s``e``d`` ``w``i``t``h`` ``s``y``n``c`` ``t``r``a``i``n``i``n``g``.``
- ``R``e``d``u``c``t``i``o``n``s`` ``a``n``d`` ``a``l``l``-``r``e``d``u``c``e``:`` ``A`` ``r``e``d``u``c``t``i``o``n`` ``i``s`` ``s``o``m``e`` ``m``e``t``h``o``d`` ``o``f`` ``a``g``g``r``e``g``a``t``i``n``g`` ``m``u``l``t``i``p``l``e`` ``v``a``l``u``e``s`` ``i``n``t``o`` ``o``n``e`` ``v``a``l``u``e``,`` ``l``i``k``e`` ``"``s``u``m``"`` ``o``r`` ``"``m``e``a``n``"``.`` ``I``f`` ``a`` ``s``t``r``a``t``e``g``y`` ``i``s`` ``d``o``i``n``g`` ``s``y``n``c`` ``t``r``a``i``n``i``n``g``,`` ``w``e`` ``w``i``l``l`` ``p``e``r``f``o``r``m`` ``a`` ``r``e``d``u``c``t``i``o``n`` ``o``n`` ``t``h``e`` ``g``r``a``d``i``e``n``t``s`` ``t``o`` ``a`` ``p``a``r``a``m``e``t``e``r`` ``f``r``o``m`` ``a``l``l`` ``r``e``p``l``i``c``a``s`` ``b``e``f``o``r``e`` ``a``p``p``l``y``i``n``g`` ``t``h``e`` ``u``p``d``a``t``e``.`` ``A``l``l``-``r``e``d``u``c``e`` ``i``s`` ``a``n`` ``a``l``g``o``r``i``t``h``m`` ``f``o``r`` ``p``e``r``f``o``r``m``i``n``g`` ``a`` ``r``e``d``u``c``t``i``o``n`` ``o``n`` ``v``a``l``u``e``s`` ``f``r``o``m`` ``m``u``l``t``i``p``l``e`` ``d``e``v``i``c``e``s`` ``a``n``d`` ``m``a``k``i``n``g`` ``t``h``e`` ``r``e``s``u``l``t`` ``a``v``a``i``l``a``b``l``e`` ``o``n`` ``a``l``l`` ``o``f`` ``t``h``o``s``e`` ``d``e``v``i``c``e``s``.``
Note that we provide a default version of `tf.distribute.Strategy` that is used when no other strategy is in scope, that provides the same API with reasonable default behavior.
## Modules
`cluster_resolver` module: Library imports for ClusterResolvers.
`experimental` module: Experimental Distribution Strategy library.
## Classes
`class CrossDeviceOps`: Base class for cross-device reduction and broadcasting algorithms.
`class HierarchicalCopyAllReduce`: Reduction using hierarchical copy all-reduce.
`class InputContext`: A class wrapping information needed by an input function.
`class InputReplicationMode`: Replication mode for input function.
`class MirroredStrategy`: Mirrors vars to distribute across multiple devices and machines.
`class NcclAllReduce`: Reduction using NCCL all-reduce.
`class OneDeviceStrategy`: A distribution strategy for running on a single device.
`class ReduceOp`: Indicates how a set of values should be reduced.
`class ReductionToOneDevice`: Always do reduction to one device first and then do broadcasting.
`class ReplicaContext`: `tf.distribute.Strategy` API when in a replica context.
`class Server`: An in-process TensorFlow server, for use in distributed training.
`class Strategy`: A state & compute distribution policy on a list of devices.
`class StrategyExtended`: Additional APIs for algorithms that need to be distribution-aware.
## Functions
