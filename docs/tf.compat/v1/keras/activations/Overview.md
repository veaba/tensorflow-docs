Built-in activation functions.

## 功能
[ `deserialize(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/deserialize)

[ `elu(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/elu): Exponential linear unit.

[ `exponential(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/exponential): Exponential activation function.

[ `get(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/get)

[ `hard_sigmoid(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/hard_sigmoid): Hard sigmoid activation function.

[ `linear(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/linear): Linear activation function.

[ `relu(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/relu): Rectified Linear Unit.

[ `selu(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/selu): Scaled Exponential Linear Unit (SELU).

[ `serialize(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/serialize)

[ `sigmoid(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/sigmoid): Sigmoid.

[ `softmax(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/softmax): The softmax activation function transforms the outputs so that all values are in

[ `softplus(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/softplus): Softplus activation function.

[ `softsign(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/softsign): Softsign activation function.

[ `tanh(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/activations/tanh): Hyperbolic Tangent (tanh) activation function.

