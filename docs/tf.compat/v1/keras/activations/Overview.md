
Built-in activation functions.
## Functions
[deserialize(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/deserialize)

[elu(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu): Exponential linear unit.

[exponential(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/exponential): Exponential activation function.

[get(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/get)

[hard_sigmoid(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/hard_sigmoid): Hard sigmoid activation function.

[linear(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear): Linear activation function.

[relu(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu): Rectified Linear Unit.

[selu(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu): Scaled Exponential Linear Unit (SELU).

[serialize(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/serialize)

[sigmoid(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid): Sigmoid.

[softmax(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax): The softmax activation function transforms the outputs so that all values are in

[softplus(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softplus): Softplus activation function.

[softsign(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softsign): Softsign activation function.

[tanh(...)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh): Hyperbolic Tangent (tanh) activation function.

