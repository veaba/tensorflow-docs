Built-in optimizer classes.

## 模块
[ `schedules` ](https://tensorflow.google.cn/api_docs/python/tf/compat/v1/keras/optimizers/schedules) module: Public API for tf.keras.optimizers.schedules namespace.

## Class 
[ `class Adadelta` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adadelta): Optimizer that implements the Adadelta algorithm.

[ `class Adagrad` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adagrad): Optimizer that implements the Adagrad algorithm.

[ `class Adam` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adam): Optimizer that implements the Adam algorithm.

[ `class Adamax` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Adamax): Optimizer that implements the Adamax algorithm.

[ `class Ftrl` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Ftrl): Optimizer that implements the FTRL algorithm.

[ `class Nadam` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Nadam): Optimizer that implements the NAdam algorithm.

[ `class Optimizer` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/Optimizer): Updated base class for optimizers.

[ `class RMSprop` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/RMSprop): Optimizer that implements the RMSprop algorithm.

[ `class SGD` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/SGD): Stochastic gradient descent and momentum optimizer.

## 功能
[ `deserialize(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/deserialize): Inverse of the  `serialize`  function.

[ `get(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/get): Retrieves a Keras Optimizer instance.

[ `serialize(...)` ](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/serialize)

