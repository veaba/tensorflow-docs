Rectified Linear Unit.
### Aliases:
- `tf.compat.v1.keras.activations.relu`
- `tf.compat.v2.keras.activations.relu`

```
 tf.keras.activations.relu(
    x,
    alpha=0.0,
    max_value=None,
    threshold=0
)
```
