(window.webpackJsonp=window.webpackJsonp||[]).push([[570],{758:function(e,s,t){"use strict";t.r(s);var o=t(0),a=Object(o.a)({},(function(){var e=this,s=e.$createElement,t=e._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.compat.v1.losses.softmax_cross_entropy(\n    onehot_labels,\n    logits,\n    weights=1.0,\n    label_smoothing=0,\n    scope=None,\n    loss_collection=tf.GraphKeys.LOSSES,\n    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n)\n")])])]),t("p",[e._v("weights acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If weights is a tensor of shape [batch_size], then the loss weights apply to each corresponding sample.")]),e._v(" "),t("p",[e._v("If label_smoothing is nonzero, smooth the labels towards 1/num_classes: new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes")]),e._v(" "),t("p",[e._v("Note that onehot_labels and logits must have the same shape, e.g. [batch_size, num_classes]. The shape of weights must be broadcastable to loss, whose shape is decided by the shape of logits. In case the shape of logits is [batch_size, num_classes], loss is a Tensor of shape [batch_size].")]),e._v(" "),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("onehot_labels")]),e._v(": One-hot-encoded labels.")]),e._v(" "),t("li",[t("code",[e._v("logits")]),e._v(": Logits outputs of the network.")]),e._v(" "),t("li",[t("code",[e._v("weights")]),e._v(": Optional "),t("code",[e._v("Tensor")]),e._v(" that is broadcastable to loss.")]),e._v(" "),t("li",[t("code",[e._v("label_smoothing")]),e._v(": If greater than 0 then smooth the labels.")]),e._v(" "),t("li",[t("code",[e._v("scope")]),e._v(": the "),t("code",[e._v("scope")]),e._v(" for the operations performed in computing the loss.")]),e._v(" "),t("li",[t("code",[e._v("loss_collection")]),e._v(": collection to which the loss will be added.")]),e._v(" "),t("li",[t("code",[e._v("reduction")]),e._v(": Type of "),t("code",[e._v("reduction")]),e._v(" to apply to loss.")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("Weighted loss Tensor of the same type as logits. If reduction is NONE, this has shape [batch_size]; otherwise, it is scalar.")]),e._v(" "),t("h4",{attrs:{id:"raises"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("ValueError")]),e._v(": If the shape of "),t("code",[e._v("logits")]),e._v(" doesn't match that of "),t("code",[e._v("onehot_labels")]),e._v(" or if the shape of "),t("code",[e._v("weights")]),e._v(" is invalid or if "),t("code",[e._v("weights")]),e._v(" is None. Also if "),t("code",[e._v("onehot_labels")]),e._v(" or "),t("code",[e._v("logits")]),e._v(" is None.")])]),e._v(" "),t("h4",{attrs:{id:"eager-compatibility"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#eager-compatibility","aria-hidden":"true"}},[e._v("#")]),e._v(" Eager Compatibility")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/Model",target:"_blank",rel:"noopener noreferrer"}},[e._v("tf.keras.Model"),t("OutboundLink")],1),e._v("The loss_collection argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a .")])])}),[],!1,null,null,null);s.default=a.exports}}]);