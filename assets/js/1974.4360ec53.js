(window.webpackJsonp=window.webpackJsonp||[]).push([[1974],{2165:function(e,o,_){"use strict";_.r(o);var v=_(0),t=Object(v.a)({},(function(){var e=this,o=e.$createElement,_=e._self._c||o;return _("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[_("p",[e._v("Replicates a model on different GPUs. (deprecated)")]),e._v(" "),_("h3",{attrs:{id:"aliases"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),_("ul",[_("li",[_("code",[e._v("tf.compat.v1.keras.utils.multi_gpu_model")])]),e._v(" "),_("li",[_("code",[e._v("tf.compat.v2.keras.utils.multi_gpu_model")])])]),e._v(" "),_("div",{staticClass:"language- extra-class"},[_("pre",{pre:!0,attrs:{class:"language-text"}},[_("code",[e._v(" tf.keras.utils.multi_gpu_model(\n    model,\n    gpus,\n    cpu_merge=True,\n    cpu_relocation=False\n)\n")])])]),_("p",[e._v("Specifically, this function implements single-machine multi-GPU data parallelism. It works in the following way:")]),e._v(" "),_("ul",[_("li",[_("code",[e._v("D")]),e._v("i"),_("code",[e._v("v")]),e._v("i"),_("code",[e._v("d")]),e._v("e"),_("code"),e._v("t"),_("code",[e._v("h")]),e._v("e"),_("code"),e._v("m"),_("code",[e._v("o")]),e._v("d"),_("code",[e._v("e")]),e._v("l"),_("code",[e._v("'")]),e._v("s"),_("code"),e._v("i"),_("code",[e._v("n")]),e._v("p"),_("code",[e._v("u")]),e._v("t"),_("code",[e._v("(")]),e._v("s"),_("code",[e._v(")")]),e._v(" "),_("code",[e._v("i")]),e._v("n"),_("code",[e._v("t")]),e._v("o"),_("code"),e._v("m"),_("code",[e._v("u")]),e._v("l"),_("code",[e._v("t")]),e._v("i"),_("code",[e._v("p")]),e._v("l"),_("code",[e._v("e")]),e._v(" "),_("code",[e._v("s")]),e._v("u"),_("code",[e._v("b")]),e._v("-"),_("code",[e._v("b")]),e._v("a"),_("code",[e._v("t")]),e._v("c"),_("code",[e._v("h")]),e._v("e"),_("code",[e._v("s")]),e._v(".``")]),e._v(" "),_("li",[_("code",[e._v("A")]),e._v("p"),_("code",[e._v("p")]),e._v("l"),_("code",[e._v("y")]),e._v(" "),_("code",[e._v("a")]),e._v(" "),_("code",[e._v("m")]),e._v("o"),_("code",[e._v("d")]),e._v("e"),_("code",[e._v("l")]),e._v(" "),_("code",[e._v("c")]),e._v("o"),_("code",[e._v("p")]),e._v("y"),_("code"),e._v("o"),_("code",[e._v("n")]),e._v(" "),_("code",[e._v("e")]),e._v("a"),_("code",[e._v("c")]),e._v("h"),_("code"),e._v("s"),_("code",[e._v("u")]),e._v("b"),_("code",[e._v("-")]),e._v("b"),_("code",[e._v("a")]),e._v("t"),_("code",[e._v("c")]),e._v("h"),_("code",[e._v(".")]),e._v(" "),_("code",[e._v("E")]),e._v("v"),_("code",[e._v("e")]),e._v("r"),_("code",[e._v("y")]),e._v(" "),_("code",[e._v("m")]),e._v("o"),_("code",[e._v("d")]),e._v("e"),_("code",[e._v("l")]),e._v(" "),_("code",[e._v("c")]),e._v("o"),_("code",[e._v("p")]),e._v("y"),_("code"),e._v("i"),_("code",[e._v("s")]),e._v(" "),_("code",[e._v("e")]),e._v("x"),_("code",[e._v("e")]),e._v("c"),_("code",[e._v("u")]),e._v("t"),_("code",[e._v("e")]),e._v("d"),_("code"),e._v("o"),_("code",[e._v("n")]),e._v(" "),_("code",[e._v("a")]),e._v(" "),_("code",[e._v("d")]),e._v("e"),_("code",[e._v("d")]),e._v("i"),_("code",[e._v("c")]),e._v("a"),_("code",[e._v("t")]),e._v("e"),_("code",[e._v("d")]),e._v(" "),_("code",[e._v("G")]),e._v("P"),_("code",[e._v("U")]),e._v(".``")]),e._v(" "),_("li",[_("code",[e._v("C")]),e._v("o"),_("code",[e._v("n")]),e._v("c"),_("code",[e._v("a")]),e._v("t"),_("code",[e._v("e")]),e._v("n"),_("code",[e._v("a")]),e._v("t"),_("code",[e._v("e")]),e._v(" "),_("code",[e._v("t")]),e._v("h"),_("code",[e._v("e")]),e._v(" "),_("code",[e._v("r")]),e._v("e"),_("code",[e._v("s")]),e._v("u"),_("code",[e._v("l")]),e._v("t"),_("code",[e._v("s")]),e._v(" "),_("code",[e._v("(")]),e._v("o"),_("code",[e._v("n")]),e._v(" "),_("code",[e._v("C")]),e._v("P"),_("code",[e._v("U")]),e._v(")"),_("code"),e._v("i"),_("code",[e._v("n")]),e._v("t"),_("code",[e._v("o")]),e._v(" "),_("code",[e._v("o")]),e._v("n"),_("code",[e._v("e")]),e._v(" "),_("code",[e._v("b")]),e._v("i"),_("code",[e._v("g")]),e._v(" "),_("code",[e._v("b")]),e._v("a"),_("code",[e._v("t")]),e._v("c"),_("code",[e._v("h")]),e._v(".``")])]),e._v(" "),_("p",[e._v("E.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples.")]),e._v(" "),_("p",[e._v("This induces quasi-linear speedup on up to 8 GPUs.")]),e._v(" "),_("p",[e._v("This function is only available with the TensorFlow backend for the time being.")]),e._v(" "),_("h4",{attrs:{id:"arguments"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),_("ul",[_("li",[_("code",[e._v("model")]),e._v(": A Keras "),_("code",[e._v("model")]),e._v(" instance. To avoid OOM errors, this "),_("code",[e._v("model")]),e._v(" could have been built on CPU, for instance (see usage example below).")]),e._v(" "),_("li",[_("code",[e._v("gpus")]),e._v(": Integer >= 2, number of on GPUs on which to create "),_("code",[e._v("model")]),e._v(" replicas.")]),e._v(" "),_("li",[_("code",[e._v("cpu_merge")]),e._v(": A boolean value to identify whether to force merging "),_("code",[e._v("model")]),e._v(" weights under the scope of the CPU or not.")]),e._v(" "),_("li",[_("code",[e._v("cpu_relocation")]),e._v(": A boolean value to identify whether to create the "),_("code",[e._v("model")]),e._v("'s weights under the scope of the CPU. If the "),_("code",[e._v("model")]),e._v(" is not defined under any preceding device scope, you can still rescue it by activating this option.")])]),e._v(" "),_("h4",{attrs:{id:"returns"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),_("p",[e._v("A Keras Model instance which can be used just like the initial model argument, but which distributes its workload on multiple GPUs.")]),e._v(" "),_("p",[e._v("Example 1: Training models with weights merge on CPU")]),e._v(" "),_("div",{staticClass:"language- extra-class"},[_("pre",{pre:!0,attrs:{class:"language-text"}},[_("code",[e._v("     import tensorflow as tf\n    from keras.applications import Xception\n    from keras.utils import multi_gpu_model\n    import numpy as np\n\n    num_samples = 1000\n    height = 224\n    width = 224\n    num_classes = 1000\n\n    # Instantiate the base model (or \"template\" model).\n    # We recommend doing this with under a CPU device scope,\n    # so that the model's weights are hosted on CPU memory.\n    # Otherwise they may end up hosted on a GPU, which would\n    # complicate weight sharing.\n    with tf.device('/cpu:0'):\n        model = Xception(weights=None,\n                         input_shape=(height, width, 3),\n                         classes=num_classes)\n\n    # Replicates the model on 8 GPUs.\n    # This assumes that your machine has 8 available GPUs.\n    parallel_model = multi_gpu_model(model, gpus=8)\n    parallel_model.compile(loss='categorical_crossentropy',\n                           optimizer='rmsprop')\n\n    # Generate dummy data.\n    x = np.random.random((num_samples, height, width, 3))\n    y = np.random.random((num_samples, num_classes))\n\n    # This `fit` call will be distributed on 8 GPUs.\n    # Since the batch size is 256, each GPU will process 32 samples.\n    parallel_model.fit(x, y, epochs=20, batch_size=256)\n\n    # Save model via the template model (which shares the same weights):\n    model.save('my_model.h5')\n")])])]),_("p",[e._v("Example 2: Training models with weights merge on CPU using cpu_relocation")]),e._v(" "),_("div",{staticClass:"language- extra-class"},[_("pre",{pre:!0,attrs:{class:"language-text"}},[_("code",[e._v('      ..\n     # Not needed to change the device scope for model definition:\n     model = Xception(weights=None, ..)\n\n     try:\n         model = multi_gpu_model(model, cpu_relocation=True)\n         print("Training using multiple GPUs..")\n     except:\n         print("Training using single GPU or CPU..")\n\n     model.compile(..)\n     ..\n')])])]),_("p",[e._v("Example 3: Training models with weights merge on GPU (recommended for NV-link)")]),e._v(" "),_("div",{staticClass:"language- extra-class"},[_("pre",{pre:!0,attrs:{class:"language-text"}},[_("code",[e._v('      ..\n     # Not needed to change the device scope for model definition:\n     model = Xception(weights=None, ..)\n\n     try:\n         model = multi_gpu_model(model, cpu_merge=False)\n         print("Training using multiple GPUs..")\n     except:\n         print("Training using single GPU or CPU..")\n     model.compile(..)\n     ..\n')])])]),_("h4",{attrs:{id:"raises"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),_("ul",[_("li",[_("code",[e._v("ValueError")]),e._v(": if the "),_("code",[e._v("gpus")]),e._v(" argument does not match available devices.")])])])}),[],!1,null,null,null);o.default=t.exports}}]);