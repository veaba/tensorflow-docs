(window.webpackJsonp=window.webpackJsonp||[]).push([[906],{1094:function(e,a,t){"use strict";t.r(a);var r=t(0),s=Object(r.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Recreates a Graph saved in a MetaGraphDef proto.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.compat.v1.train.import_meta_graph(\n    meta_graph_or_file,\n    clear_devices=False,\n    import_scope=None,\n    **kwargs\n)\n")])])]),t("p",[e._v("This function takes a MetaGraphDef protocol buffer as input. If the argument is a file containing a MetaGraphDef protocol buffer , it constructs a protocol buffer from the file content. The function then adds all the nodes from the graph_def field to the current graph, recreates all the collections, and returns a saver constructed from the saver_def field.")]),e._v(" "),t("p",[e._v("In combination with export_meta_graph(), this function can be used to")]),e._v(" "),t("ul",[t("li",[e._v("Serialize a graph along with other Python objects such as "),t("code",[e._v("QueueRunner")]),e._v(", "),t("code",[e._v("Variable")]),e._v(" into a "),t("code",[e._v("MetaGraphDef")]),e._v(".")]),e._v(" "),t("li",[e._v("Restart training from a saved graph and checkpoints.")]),e._v(" "),t("li",[e._v("Run inference from a saved graph and checkpoints.")])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" ...\n# Create a saver.\nsaver = tf.compat.v1.train.Saver(...variables...)\n# Remember the training_op we want to run by adding it to a collection.\ntf.compat.v1.add_to_collection('train_op', train_op)\nsess = tf.compat.v1.Session()\nfor step in xrange(1000000):\n    sess.run(train_op)\n    if step % 1000 == 0:\n        # Saves checkpoint, which by default also exports a meta_graph\n        # named 'my-model-global_step.meta'.\n        saver.save(sess, 'my-model', global_step=step)\n")])])]),t("p",[e._v("Later we can continue training from this saved meta_graph without building the model from scratch.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" with tf.Session() as sess:\n  new_saver =\n  tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n  new_saver.restore(sess, 'my-save-dir/my-model-10000')\n  # tf.get_collection() returns a list. In this example we only want\n  # the first one.\n  train_op = tf.get_collection('train_op')[0]\n  for step in xrange(1000000):\n    sess.run(train_op)\n")])])]),t("p",[e._v("NOTE: Restarting training from saved meta_graph only works if the device assignments have not changed.")]),e._v(" "),t("h4",{attrs:{id:"example"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#example","aria-hidden":"true"}},[e._v("#")]),e._v(" Example:")]),e._v(" "),t("p",[e._v("Variables, placeholders, and independent operations can also be stored, as shown in the following example.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(' # Saving contents and operations.\nv1 = tf.placeholder(tf.float32, name="v1")\nv2 = tf.placeholder(tf.float32, name="v2")\nv3 = tf.math.multiply(v1, v2)\nvx = tf.Variable(10.0, name="vx")\nv4 = tf.add(v3, vx, name="v4")\nsaver = tf.train.Saver([vx])\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(vx.assign(tf.add(vx, vx)))\nresult = sess.run(v4, feed_dict={v1:12.0, v2:3.3})\nprint(result)\nsaver.save(sess, "./model_ex1")\n')])])]),t("p",[e._v("Later this model can be restored and contents loaded.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(' # Restoring variables and running operations.\nsaver = tf.train.import_meta_graph("./model_ex1.meta")\nsess = tf.Session()\nsaver.restore(sess, "./model_ex1")\nresult = sess.run("v4:0", feed_dict={"v1:0": 12.0, "v2:0": 3.3})\nprint(result)\n')])])]),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("meta_graph_or_file")]),e._v(": "),t("code",[e._v("MetaGraphDef")]),e._v(" protocol buffer or filename (including the path) containing a "),t("code",[e._v("MetaGraphDef")]),e._v(".")]),e._v(" "),t("li",[t("code",[e._v("clear_devices")]),e._v(": Whether or not to clear the device field for an "),t("code",[e._v("Operation")]),e._v(" or "),t("code",[e._v("Tensor")]),e._v(" during import.")]),e._v(" "),t("li",[t("code",[e._v("import_scope")]),e._v(": Optional "),t("code",[e._v("string")]),e._v(". Name scope to add. Only used when initializing from protocol buffer.")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("A saver constructed from saver_def in MetaGraphDef or None.")]),e._v(" "),t("p",[e._v("A None value is returned if no variables exist in the MetaGraphDef (i.e., there are no variables to restore).")]),e._v(" "),t("h4",{attrs:{id:"raises"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("RuntimeError")]),e._v(": If called with eager execution enabled.")])]),e._v(" "),t("h4",{attrs:{id:"eager-compatibility"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#eager-compatibility","aria-hidden":"true"}},[e._v("#")]),e._v(" Eager Compatibility")]),e._v(" "),t("p",[e._v("Exporting/importing meta graphs is not supported. No graph exists when eager execution is enabled.")])])}),[],!1,null,null,null);a.default=s.exports}}]);