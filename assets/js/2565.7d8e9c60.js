(window.webpackJsonp=window.webpackJsonp||[]).push([[2565],{2756:function(e,a,t){"use strict";t.r(a);var s=t(0),n=Object(s.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Stacks a list of rank-"),t("code",[e._v("R")]),e._v(" tensors into one rank-("),t("code",[e._v("R")]),e._v("+1) tensor in parallel.")]),e._v(" "),t("h3",{attrs:{id:"aliases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("tf.compat.v1.parallel_stack")])]),e._v(" "),t("li",[t("code",[e._v("tf.compat.v2.parallel_stack")])])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.parallel_stack(\n    values,\n    name='parallel_stack'\n)\n")])])]),t("p",[e._v("Requires that the shape of inputs be known at graph construction time.\nPacks the list of tensors in "),t("code",[e._v("values")]),e._v(" into a tensor with rank one higher than each tensor in "),t("code",[e._v("values")]),e._v(", by packing them along the first dimension. Given a list of length "),t("code",[e._v("N")]),e._v(" of tensors of shape ("),t("code",[e._v("A, B, CA, B, C")]),e._v("); the "),t("code",[e._v("output")]),e._v(" tensor will have the shape ("),t("code",[e._v("N")]),e._v(", "),t("code",[e._v("A, B, CA, B, C")]),e._v(").")]),e._v(" "),t("h4",{attrs:{id:"for-example"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#for-example","aria-hidden":"true"}},[e._v("#")]),e._v(" For example:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" x = tf.constant([1, 4])\ny = tf.constant([2, 5])\nz = tf.constant([3, 6])\ntf.parallel_stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]]\n")])])]),t("p",[e._v("The difference between "),t("code",[e._v("stack")]),e._v(" and "),t("code",[e._v("parallel_stack")]),e._v(" is that "),t("code",[e._v("stack")]),e._v(" requires all the inputs be computed before the operation will begin but doesn't require that the input shapes be known during graph construction.\n"),t("code",[e._v("parallel_stack")]),e._v(" will copy pieces of the input into the output as they become available, in some situations this can provide a performance benefit.\nUnlike "),t("code",[e._v("stack")]),e._v(", "),t("code",[e._v("parallel_stack")]),e._v(" does NOT support backpropagation.\nThis is the opposite of unstack. The numpy equivalent is")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])\n")])])]),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("values")]),e._v(": A list of "),t("code",[e._v("Tensor")]),e._v(" objects with the same shape and type.")]),e._v(" "),t("li",[t("code",[e._v("name")]),e._v(": A "),t("code",[e._v("name")]),e._v(" for this operation (optional).")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("output")]),e._v(": A stacked "),t("code",[e._v("Tensor")]),e._v(" with the same type as "),t("code",[e._v("values")]),e._v(".")])])])}),[],!1,null,null,null);a.default=n.exports}}]);