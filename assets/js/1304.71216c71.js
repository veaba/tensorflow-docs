(window.webpackJsonp=window.webpackJsonp||[]).push([[1304],{1493:function(e,t,a){"use strict";a.r(t);var n=a(0),r=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("Train and evaluate the estimator.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("tf.compat.v1.estimator.train_and_evaluate")])]),e._v(" "),a("li",[a("code",[e._v("tf.compat.v2.estimator.train_and_evaluate")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" tf.estimator.train_and_evaluate(\n    estimator,\n    train_spec,\n    eval_spec\n)\n")])])]),a("h3",{attrs:{id:"used-in-the-guide"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-guide","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the guide:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("M")]),e._v("i"),a("code",[e._v("g")]),e._v("r"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("y")]),e._v("o"),a("code",[e._v("u")]),e._v("r"),a("code"),e._v("T"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("s")]),e._v("o"),a("code",[e._v("r")]),e._v("F"),a("code",[e._v("l")]),e._v("o"),a("code",[e._v("w")]),e._v(" "),a("code",[e._v("1")]),e._v(" "),a("code",[e._v("c")]),e._v("o"),a("code",[e._v("d")]),e._v("e"),a("code"),e._v("t"),a("code",[e._v("o")]),e._v(" "),a("code",[e._v("T")]),e._v("e"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("F")]),e._v("l"),a("code",[e._v("o")]),e._v("w"),a("code"),e._v("2``")])]),e._v(" "),a("h3",{attrs:{id:"used-in-the-tutorials"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-tutorials","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the tutorials:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("M")]),e._v("u"),a("code",[e._v("l")]),e._v("t"),a("code",[e._v("i")]),e._v("-"),a("code",[e._v("w")]),e._v("o"),a("code",[e._v("r")]),e._v("k"),a("code",[e._v("e")]),e._v("r"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(" "),a("code",[e._v("w")]),e._v("i"),a("code",[e._v("t")]),e._v("h"),a("code"),e._v("E"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("i")]),e._v("m"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("o")]),e._v("r``")])]),e._v(" "),a("p",[e._v("This utility function trains, evaluates, and (optionally) exports the model by using the given estimator. All training related specification is held in train_spec, including training input_fn and training max steps, etc. All evaluation and export related specification is held in eval_spec, including evaluation input_fn, steps, etc.\n"),a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute",target:"_blank",rel:"noopener noreferrer"}},[e._v("DistributionStrategies"),a("OutboundLink")],1),e._v("This utility function provides consistent behavior for both local (non-distributed) and distributed configurations. The default distribution configuration is parameter server-based between-graph replication. For other types of distribution configurations such as all-reduce training, please use .")]),e._v(" "),a("p",[e._v("Overfitting: In order to avoid overfitting, it is recommended to set up the training input_fn to shuffle the training data properly.\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/Estimator#train",target:"_blank",rel:"noopener noreferrer"}},[e._v("Estimator.train"),a("OutboundLink")],1),e._v("Stop condition: In order to support both distributed and non-distributed configuration reliably, the only supported stop condition for model training is train_spec.max_steps. If train_spec.max_steps is None, the model is trained forever. Use with care if model stop condition is different. For example, assume that the model is expected to be trained with one epoch of training data, and the training input_fn is configured to throw OutOfRangeError after going through one epoch, which stops the . For a three-training-worker distributed configuration, each training worker is likely to go through the whole epoch independently. So, the model will be trained with three epochs of training data instead of one epoch.")]),e._v(" "),a("p",[e._v("Example of local (non-distributed) training:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" # Set up feature columns.\ncategorial_feature_a = categorial_column_with_hash_bucket(...)\ncategorial_feature_a_emb = embedding_column(\n    categorical_column=categorial_feature_a, ...)\n...  # other feature columns\n\nestimator = DNNClassifier(\n    feature_columns=[categorial_feature_a_emb, ...],\n    hidden_units=[1024, 512, 256])\n\n# Or set up the model directory\n#   estimator = DNNClassifier(\n#       config=tf.estimator.RunConfig(\n#           model_dir='/my_model', save_summary_steps=100),\n#       feature_columns=[categorial_feature_a_emb, ...],\n#       hidden_units=[1024, 512, 256])\n\n# Input pipeline for train and evaluate.\ndef train_input_fn(): # returns x, y\n  # please shuffle the data.\n  pass\ndef eval_input_fn(): # returns x, y\n  pass\n\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)\neval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n")])])]),a("p",[e._v("Note that in current implementation estimator.evaluate will be called multiple times. This means that evaluation graph (including eval_input_fn) will be re-created for each evaluate call. estimator.train will be called only once.")]),e._v(" "),a("p",[e._v("Example of distributed training:\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig#model_dir",target:"_blank",rel:"noopener noreferrer"}},[e._v("RunConfig.model_dir"),a("OutboundLink")],1),e._v("Regarding the example of distributed training, the code above can be used without a change (Please do make sure that the  for all workers is set to the same directory, i.e., a shared file system all workers can read and write). The only extra work to do is setting the environment variable TF_CONFIG properly for each worker correspondingly.")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.tensorflow.org/deploy/distributed",target:"_blank",rel:"noopener noreferrer"}},[e._v("Distributed TensorFlow"),a("OutboundLink")],1),e._v("Also see .")]),e._v(" "),a("p",[e._v("Setting environment variable depends on the platform. For example, on Linux, it can be done as follows ($ is the shell prompt):")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" $ TF_CONFIG='<replace_with_real_content>' python train_model.py\n")])])]),a("p",[e._v("For the content in TF_CONFIG, assume that the training cluster spec looks like:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' cluster = {"chief": ["host0:2222"],\n           "worker": ["host1:2222", "host2:2222", "host3:2222"],\n           "ps": ["host4:2222", "host5:2222"]}\n')])])]),a("p",[e._v("Example of TF_CONFIG for chief training worker (must have one and only one):")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' # This should be a JSON string, which is set as environment variable. Usually\n# the cluster manager handles that.\nTF_CONFIG=\'{\n    "cluster": {\n        "chief": ["host0:2222"],\n        "worker": ["host1:2222", "host2:2222", "host3:2222"],\n        "ps": ["host4:2222", "host5:2222"]\n    },\n    "task": {"type": "chief", "index": 0}\n}\'\n')])])]),a("p",[e._v("Note that the chief worker also does the model training job, similar to other non-chief training workers (see next paragraph). In addition to the model training, it manages some extra work, e.g., checkpoint saving and restoring, writing summaries, etc.")]),e._v(" "),a("p",[e._v("Example of TF_CONFIG for non-chief training worker (optional, could be multiple):")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' # This should be a JSON string, which is set as environment variable. Usually\n# the cluster manager handles that.\nTF_CONFIG=\'{\n    "cluster": {\n        "chief": ["host0:2222"],\n        "worker": ["host1:2222", "host2:2222", "host3:2222"],\n        "ps": ["host4:2222", "host5:2222"]\n    },\n    "task": {"type": "worker", "index": 0}\n}\'\n')])])]),a("p",[e._v("where the task.index should be set as 0, 1, 2, in this example, respectively for non-chief training workers.")]),e._v(" "),a("p",[e._v("Example of TF_CONFIG for parameter server, aka ps (could be multiple):")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' # This should be a JSON string, which is set as environment variable. Usually\n# the cluster manager handles that.\nTF_CONFIG=\'{\n    "cluster": {\n        "chief": ["host0:2222"],\n        "worker": ["host1:2222", "host2:2222", "host3:2222"],\n        "ps": ["host4:2222", "host5:2222"]\n    },\n    "task": {"type": "ps", "index": 0}\n}\'\n')])])]),a("p",[e._v("where the task.index should be set as 0 and 1, in this example, respectively for parameter servers.")]),e._v(" "),a("p",[e._v("Example of TF_CONFIG for evaluator task. Evaluator is a special task that is not part of the training cluster. There could be only one. It is used for model evaluation.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' # This should be a JSON string, which is set as environment variable. Usually\n# the cluster manager handles that.\nTF_CONFIG=\'{\n    "cluster": {\n        "chief": ["host0:2222"],\n        "worker": ["host1:2222", "host2:2222", "host3:2222"],\n        "ps": ["host4:2222", "host5:2222"]\n    },\n    "task": {"type": "evaluator", "index": 0}\n}\'\n')])])]),a("p",[e._v("When distribute or experimental_distribute.train_distribute and experimental_distribute.remote_cluster is set, this method will start a client running on the current host which connects to the remote_cluster for training and evaluation.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("estimator")]),e._v(": An "),a("code",[e._v("Estimator")]),e._v(" instance to train and evaluate.")]),e._v(" "),a("li",[a("code",[e._v("train_spec")]),e._v(": A "),a("code",[e._v("TrainSpec")]),e._v(" instance to specify the training specification.")]),e._v(" "),a("li",[a("code",[e._v("eval_spec")]),e._v(": A "),a("code",[e._v("EvalSpec")]),e._v(" instance to specify the evaluation and export specification.")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A tuple of the result of the evaluate call to the Estimator and the export results using the specified ExportStrategy. Currently, the return value is undefined for distributed training mode.")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": if environment variable "),a("code",[e._v("TF_CONFIG")]),e._v(" is incorrectly set.")])])])}),[],!1,null,null,null);t.default=r.exports}}]);