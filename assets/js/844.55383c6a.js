(window.webpackJsonp=window.webpackJsonp||[]).push([[844],{1032:function(t,e,a){"use strict";a.r(e);var i=a(0),r=Object(i.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"class-adagradparameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-adagradparameters","aria-hidden":"true"}},[t._v("#")]),t._v(" Class AdagradParameters")]),t._v(" "),a("p",[t._v("Optimization parameters for Adagrad with TPU embeddings.")]),t._v(" "),a("p",[t._v("Pass this to tf.estimator.tpu.experimental.EmbeddingConfigSpec via the optimization_parameters argument to set the optimizer and its parameters. See the documentation for tf.estimator.tpu.experimental.EmbeddingConfigSpec for more details.")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v(" estimator = tf.estimator.tpu.TPUEstimator(\n    ...\n    embedding_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(\n        ...\n        optimization_parameters=tf.tpu.experimental.AdagradParameters(0.1),\n        ...))\n")])])]),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[t._v("#")]),t._v(" "),a("strong",[t._v("init")])]),t._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tpu/tpu_embedding.py#L280-L303",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source"),a("OutboundLink")],1)]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v(" __init__(\n    learning_rate,\n    initial_accumulator=0.1,\n    use_gradient_accumulation=True,\n    clip_weight_min=None,\n    clip_weight_max=None\n)\n")])])]),a("p",[t._v("Optimization parameters for Adagrad.")]),t._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[t._v("#")]),t._v(" Args:")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("learning_rate")]),t._v(": used for updating embedding table.")]),t._v(" "),a("li",[a("code",[t._v("initial_accumulator")]),t._v(": initial accumulator for Adagrad.")]),t._v(" "),a("li",[a("code",[t._v("use_gradient_accumulation")]),t._v(": setting this to "),a("code",[t._v("False")]),t._v(" makes embedding gradients calculation less accurate but faster. Please see "),a("code",[t._v("optimization_parameters.proto")]),t._v(" for details. for details.")]),t._v(" "),a("li",[a("code",[t._v("clip_weight_min")]),t._v(": the minimum value to clip by; None means -infinity.")]),t._v(" "),a("li",[a("code",[t._v("clip_weight_max")]),t._v(": the maximum value to clip by; None means +infinity.")])])])}),[],!1,null,null,null);e.default=r.exports}}]);