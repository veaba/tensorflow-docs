(window.webpackJsonp=window.webpackJsonp||[]).push([[2552],{2743:function(e,t,a){"use strict";a.r(t);var s=a(0),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("map on the list of tensors unpacked from "),a("code",[e._v("elems")]),e._v(" on dimension 0.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("tf.compat.v1.map_fn")])]),e._v(" "),a("li",[a("code",[e._v("tf.compat.v2.map_fn")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" tf.map_fn(\n    fn,\n    elems,\n    dtype=None,\n    parallel_iterations=None,\n    back_prop=True,\n    swap_memory=False,\n    infer_shape=True,\n    name=None\n)\n")])])]),a("p",[e._v("The simplest version of "),a("code",[e._v("map_fn")]),e._v(" repeatedly applies the callable "),a("code",[e._v("fn")]),e._v(" to a sequence of elements from first to last. The elements are made of the tensors unpacked from "),a("code",[e._v("elems")]),e._v(". "),a("code",[e._v("dtype")]),e._v(" is the data type of the return value of "),a("code",[e._v("fn")]),e._v(". Users must provide "),a("code",[e._v("dtype")]),e._v(" if it is different from the data type of "),a("code",[e._v("elems")]),e._v(".\nSuppose that "),a("code",[e._v("elems")]),e._v(" is unpacked into "),a("code",[e._v("values")]),e._v(", a list of tensors. The shape of the result tensor is ["),a("code",[e._v("values")]),e._v(".shape[0]] + fn("),a("code",[e._v("values")]),e._v("[0]).shape.\nThis method also allows multi-arity "),a("code",[e._v("elems")]),e._v(" and output of "),a("code",[e._v("fn")]),e._v(". If "),a("code",[e._v("elems")]),e._v(" is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of "),a("code",[e._v("fn")]),e._v(" may match the structure of "),a("code",[e._v("elems")]),e._v(". That is, if "),a("code",[e._v("elems")]),e._v(" is ("),a("code",[e._v("t1, [t2, t3, [t4, t5]]t1, [t2, t3, [t4, t5]]")]),e._v("), then an appropriate signature for "),a("code",[e._v("fn")]),e._v(" is: "),a("code",[e._v("fn")]),e._v(" = lambda ("),a("code",[e._v("t1, [t2, t3, [t4, t5]]t1, [t2, t3, [t4, t5]]")]),e._v("):.\nFurthermore, "),a("code",[e._v("fn")]),e._v(" may emit a different structure than its input. For example, "),a("code",[e._v("fn")]),e._v(" may look like: "),a("code",[e._v("fn")]),e._v(" = lambda t1: return (t1 + 1, t1 - 1). In this case, the "),a("code",[e._v("dtype")]),e._v(" parameter is not optional: "),a("code",[e._v("dtype")]),e._v(" must be a type or (possibly nested) tuple of types matching the output of "),a("code",[e._v("fn")]),e._v(".\nTo apply a functional operation to the nonzero elements of a SparseTensor one of the following methods is recommended. First, if the function is expressible as TensorFlow ops, use")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("   result = SparseTensor(input.indices, fn(input.values), input.dense_shape)\n")])])]),a("p",[e._v("If, however, the function is not expressible as a TensorFlow op, then use")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" result = SparseTensor(\n  input.indices, map_fn(fn, input.values), input.dense_shape)\n")])])]),a("p",[e._v("instead.\nWhen executing eagerly, map_fn does not execute in parallel even if "),a("code",[e._v("parallel_iterations")]),e._v(" is set to a value > 1. You can still get the performance benefits of running a function in parallel by using the "),a("code",[e._v("tf.contrib.eager.defun")]),e._v(" decorator,")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" # Assume the function being used in map_fn is fn.\n# To ensure map_fn calls fn in parallel, use the defun decorator.\n@tf.contrib.eager.defun\ndef func(tensor):\n  return tf.map_fn(fn, tensor)\n")])])]),a("p",[e._v("Note that if you use the defun decorator, any non-TensorFlow Python code that you may have written in your function won't get executed. See "),a("code",[e._v("tf.contrib.eager.defun")]),e._v(" for more details. The recommendation would be to debug without defun but switch to defun to get performance benefits of running map_fn in parallel.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("fn")]),e._v(": The callable to be performed. It accepts one argument, which will have the same (possibly nested) structure as "),a("code",[e._v("elems")]),e._v(". Its output must have the same structure as "),a("code",[e._v("dtype")]),e._v(" if one is provided, otherwise it must have the same structure as "),a("code",[e._v("elems")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("elems")]),e._v(": A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be applied to "),a("code",[e._v("fn")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("dtype")]),e._v(": (optional) The output type(s) of "),a("code",[e._v("fn")]),e._v(". If "),a("code",[e._v("fn")]),e._v(" returns a structure of Tensors differing from the structure of "),a("code",[e._v("elems")]),e._v(", then "),a("code",[e._v("dtype")]),e._v(" is not optional and must have the same structure as the output of "),a("code",[e._v("fn")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("parallel_iterations")]),e._v(": (optional) The number of iterations allowed to run in parallel. When graph building, the default value is 10. While executing eagerly, the default value is set to 1.")]),e._v(" "),a("li",[a("code",[e._v("back_prop")]),e._v(": (optional) True enables support for back propagation.")]),e._v(" "),a("li",[a("code",[e._v("swap_memory")]),e._v(": (optional) True enables GPU-CPU memory swapping.")]),e._v(" "),a("li",[a("code",[e._v("infer_shape")]),e._v(": (optional) False disables tests for consistent output shapes.")]),e._v(" "),a("li",[a("code",[e._v("name")]),e._v(": (optional) Name prefix for the returned tensors.")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying "),a("code",[e._v("fn")]),e._v(" to tensors unpacked from "),a("code",[e._v("elems")]),e._v(" along the first dimension, from first to last.")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("TypeError")]),e._v(": if "),a("code",[e._v("fn")]),e._v(" is not callable or the structure of the output of "),a("code",[e._v("fn")]),e._v(" and "),a("code",[e._v("dtype")]),e._v(" do not match, or if elems is a SparseTensor.")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": if the lengths of the output of "),a("code",[e._v("fn")]),e._v(" and "),a("code",[e._v("dtype")]),e._v(" do not match.")])]),e._v(" "),a("h4",{attrs:{id:"examples"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#examples","aria-hidden":"true"}},[e._v("#")]),e._v(" Examples:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" elems = np.array([1, 2, 3, 4, 5, 6])\nsquares = map_fn(lambda x: x * x, elems)\n# squares == [1, 4, 9, 16, 25, 36]\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\nalternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n# alternate == [-1, 2, -3]\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" elems = np.array([1, 2, 3])\nalternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\n# alternates[0] == [1, 2, 3]\n# alternates[1] == [-1, -2, -3]\n")])])])])}),[],!1,null,null,null);t.default=n.exports}}]);