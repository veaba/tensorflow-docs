(window.webpackJsonp=window.webpackJsonp||[]).push([[912],{1100:function(e,a,t){"use strict";t.r(a);var s=t(0),n=Object(s.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Conditionally creates batches of tensors based on keep_input. (deprecated)")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.compat.v1.train.maybe_batch(\n    tensors,\n    keep_input,\n    batch_size,\n    num_threads=1,\n    capacity=32,\n    enqueue_many=False,\n    shapes=None,\n    dynamic_pad=False,\n    allow_smaller_final_batch=False,\n    shared_name=None,\n    name=None\n)\n")])])]),t("p",[e._v("See docstring in batch for more details.")]),e._v(" "),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("tensors")]),e._v(": The list or dictionary of "),t("code",[e._v("tensors")]),e._v(" to enqueue.")]),e._v(" "),t("li",[t("code",[e._v("keep_input")]),e._v(": A "),t("code",[e._v("bool")]),e._v(" Tensor. This tensor controls whether the input is added to the queue or not. If it is a scalar and evaluates "),t("code",[e._v("True")]),e._v(", then "),t("code",[e._v("tensors")]),e._v(" are all added to the queue. If it is a vector and "),t("code",[e._v("enqueue_many")]),e._v(" is "),t("code",[e._v("True")]),e._v(", then each example is added to the queue only if the corresponding value in "),t("code",[e._v("keep_input")]),e._v(" is "),t("code",[e._v("True")]),e._v(". This tensor essentially acts as a filtering mechanism.")]),e._v(" "),t("li",[t("code",[e._v("batch_size")]),e._v(": The new batch size pulled from the queue.")]),e._v(" "),t("li",[t("code",[e._v("num_threads")]),e._v(": The number of threads enqueuing "),t("code",[e._v("tensors")]),e._v(". The batching will be nondeterministic if "),t("code",[e._v("num_threads")]),e._v(" > 1.")]),e._v(" "),t("li",[t("code",[e._v("capacity")]),e._v(": An integer. The maximum number of elements in the queue.")]),e._v(" "),t("li",[t("code",[e._v("enqueue_many")]),e._v(": Whether each tensor in "),t("code",[e._v("tensors")]),e._v(" is a single example.")]),e._v(" "),t("li",[t("code",[e._v("shapes")]),e._v(": (Optional) The "),t("code",[e._v("shapes")]),e._v(" for each example. Defaults to the inferred "),t("code",[e._v("shapes")]),e._v(" for "),t("code",[e._v("tensors")]),e._v(".")]),e._v(" "),t("li",[t("code",[e._v("dynamic_pad")]),e._v(": Boolean. Allow variable dimensions in input "),t("code",[e._v("shapes")]),e._v(". The given dimensions are padded upon dequeue so that "),t("code",[e._v("tensors")]),e._v(" within a batch have the same "),t("code",[e._v("shapes")]),e._v(".")]),e._v(" "),t("li",[t("code",[e._v("allow_smaller_final_batch")]),e._v(": (Optional) Boolean. If "),t("code",[e._v("True")]),e._v(", allow the final batch to be smaller if there are insufficient items left in the queue.")]),e._v(" "),t("li",[t("code",[e._v("shared_name")]),e._v(": (Optional). If set, this queue will be shared under the given name across multiple sessions.")]),e._v(" "),t("li",[t("code",[e._v("name")]),e._v(": (Optional) A "),t("code",[e._v("name")]),e._v(" for the operations.")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("A list or dictionary of tensors with the same types as tensors.")]),e._v(" "),t("h4",{attrs:{id:"raises"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("ValueError")]),e._v(": If the "),t("code",[e._v("shapes")]),e._v(" are not specified, and cannot be inferred from the elements of "),t("code",[e._v("tensors")]),e._v(".")])])])}),[],!1,null,null,null);a.default=n.exports}}]);