(window.webpackJsonp=window.webpackJsonp||[]).push([[1081],{1270:function(i,e,t){"use strict";t.r(e);var o=t(0),n=Object(o.a)({},(function(){var i=this,e=i.$createElement,t=i._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":i.$parent.slotKey}},[t("p",[i._v("Set experimental optimizer options.")]),i._v(" "),t("h3",{attrs:{id:"aliases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[i._v("#")]),i._v(" Aliases:")]),i._v(" "),t("ul",[t("li",[t("code",[i._v("tf.compat.v1.config.optimizer.set_experimental_options")])]),i._v(" "),t("li",[t("code",[i._v("tf.compat.v2.config.optimizer.set_experimental_options")])])]),i._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[i._v(" tf.config.optimizer.set_experimental_options(options)\n")])])]),t("p",[i._v("Note that optimizations are only applied in graph mode, (within tf.function). In addition, as these are experimental options, the list is subject to change.")]),i._v(" "),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[i._v("#")]),i._v(" Args:")]),i._v(" "),t("ul",[t("li",[t("code",[i._v("options")]),i._v(": Dictionary of experimental optimizer "),t("code",[i._v("options")]),i._v(" to configure. Valid keys:\nlayout_optimizer: Optimize tensor layouts e.g. This will try to use NCHW layout on GPU which is faster.\nconstant_folding: Fold constants Statically infer the value of tensors when possible, and materialize the result using constants.\nshape_optimization: Simplify computations made on shapes.\nremapping: Remap subgraphs onto more efficient implementations.\narithmetic_optimization: Simplify arithmetic ops with common sub-expression elimination and arithmetic simplification.\ndependency_optimization: Control dependency optimizations. Remove redundant control dependencies, which may enable other optimization. This optimizer is also essential for pruning Identity and NoOp nodes.\nloop_optimization: Loop optimizations.\nfunction_optimization: Function optimizations and inlining.\ndebug_stripper: Strips debug-related nodes from the graph.\ndisable_model_pruning: Disable removal of unnecessary ops from the graph\nscoped_allocator_optimization: Try to allocate some independent Op outputs contiguously in order to merge or eliminate downstream Ops.\npin_to_host_optimization: Force small ops onto the CPU.\nimplementation_selector: Enable the swap of kernel implementations based on the device placement.\nauto_mixed_precision: Change certain float32 ops to float16 on Volta GPUs and above. Without the use of loss scaling, this can cause numerical underflow (see "),t("code",[i._v("keras.mixed_precision.experimental.LossScaleOptimizer")]),i._v(").\ndisable_meta_optimizer: Disable the entire meta optimizer.\nmin_graph_nodes: The minimum number of nodes in a graph to optimizer. For smaller graphs, optimization is skipped.")]),i._v(" "),t("li",[i._v("layout_optimizer: Optimize tensor layouts e.g. This will try to use NCHW layout on GPU which is faster.")]),i._v(" "),t("li",[i._v("constant_folding: Fold constants Statically infer the value of tensors when possible, and materialize the result using constants.")]),i._v(" "),t("li",[i._v("shape_optimization: Simplify computations made on shapes.")]),i._v(" "),t("li",[i._v("remapping: Remap subgraphs onto more efficient implementations.")]),i._v(" "),t("li",[i._v("arithmetic_optimization: Simplify arithmetic ops with common sub-expression elimination and arithmetic simplification.")]),i._v(" "),t("li",[i._v("dependency_optimization: Control dependency optimizations. Remove redundant control dependencies, which may enable other optimization. This optimizer is also essential for pruning Identity and NoOp nodes.")]),i._v(" "),t("li",[i._v("loop_optimization: Loop optimizations.")]),i._v(" "),t("li",[i._v("function_optimization: Function optimizations and inlining.")]),i._v(" "),t("li",[i._v("debug_stripper: Strips debug-related nodes from the graph.")]),i._v(" "),t("li",[i._v("disable_model_pruning: Disable removal of unnecessary ops from the graph")]),i._v(" "),t("li",[i._v("scoped_allocator_optimization: Try to allocate some independent Op outputs contiguously in order to merge or eliminate downstream Ops.")]),i._v(" "),t("li",[i._v("pin_to_host_optimization: Force small ops onto the CPU.")]),i._v(" "),t("li",[i._v("implementation_selector: Enable the swap of kernel implementations based on the device placement.")]),i._v(" "),t("li",[i._v("auto_mixed_precision: Change certain float32 ops to float16 on Volta GPUs and above. Without the use of loss scaling, this can cause numerical underflow (see "),t("code",[i._v("keras.mixed_precision.experimental.LossScaleOptimizer")]),i._v(").")]),i._v(" "),t("li",[i._v("disable_meta_optimizer: Disable the entire meta optimizer.")]),i._v(" "),t("li",[i._v("min_graph_nodes: The minimum number of nodes in a graph to optimizer. For smaller graphs, optimization is skipped.")])])])}),[],!1,null,null,null);e.default=n.exports}}]);