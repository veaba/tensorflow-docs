(window.webpackJsonp=window.webpackJsonp||[]).push([[1746],{1937:function(t,e,a){"use strict";a.r(e);var r=a(0),s=Object(r.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"class-cropping3d"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-cropping3d","aria-hidden":"true"}},[t._v("#")]),t._v(" Class Cropping3D")]),t._v(" "),a("p",[t._v("Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer",target:"_blank",rel:"noopener noreferrer"}},[t._v("Layer"),a("OutboundLink")],1),t._v("Inherits From:")]),t._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[t._v("#")]),t._v(" Aliases:")]),t._v(" "),a("ul",[a("li",[t._v("Class "),a("code",[t._v("tf.compat.v1.keras.layers.Cropping3D")])]),t._v(" "),a("li",[t._v("Class "),a("code",[t._v("tf.compat.v2.keras.layers.Cropping3D")])])]),t._v(" "),a("h4",{attrs:{id:"arguments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[t._v("#")]),t._v(" Arguments:")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("cropping")]),t._v(": Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\nIf int: the same symmetric "),a("code",[t._v("cropping")]),t._v(" is applied to depth, height, and width.\nIf tuple of 3 ints: interpreted as two different symmetric "),a("code",[t._v("cropping")]),t._v(" values for depth, height, and width: ("),a("code",[t._v("symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop")]),t._v(").\nIf tuple of 3 tuples of 2 ints: interpreted as ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop))")]),t._v(" "),a("li",[t._v("If int: the same symmetric "),a("code",[t._v("cropping")]),t._v(" is applied to depth, height, and width.")]),t._v(" "),a("li",[t._v("If tuple of 3 ints: interpreted as two different symmetric "),a("code",[t._v("cropping")]),t._v(" values for depth, height, and width: ("),a("code",[t._v("symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop")]),t._v(").")]),t._v(" "),a("li",[t._v("If tuple of 3 tuples of 2 ints: interpreted as ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop))")]),t._v(" "),a("li",[a("code",[t._v("data_format")]),t._v(": A string, one of "),a("code",[t._v("channels_last")]),t._v(" (default) or "),a("code",[t._v("channels_first")]),t._v(". The ordering of the dimensions in the inputs. "),a("code",[t._v("channels_last")]),t._v(" corresponds to inputs with shape ("),a("code",[t._v("batch, spatial_dim1, spatial_dim2, spatial_dim3, channels")]),t._v(") while "),a("code",[t._v("channels_first")]),t._v(" corresponds to inputs with shape ("),a("code",[t._v("batch, channels, spatial_dim1, spatial_dim2, spatial_dim3")]),t._v("). It defaults to the "),a("code",[t._v("image_data_format")]),t._v(" value found in your Keras config file at "),a("code",[t._v("~/.keras/keras.json")]),t._v('. If you never set it, then it will be "'),a("code",[t._v("channels_last")]),t._v('".')])]),t._v(" "),a("h4",{attrs:{id:"input-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-shape","aria-hidden":"true"}},[t._v("#")]),t._v(" Input shape:")]),t._v(" "),a("p",[t._v('5D tensor with shape: - If data_format is "channels_last": (batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth) - If data_format is "channels_first": (batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)')]),t._v(" "),a("h4",{attrs:{id:"output-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#output-shape","aria-hidden":"true"}},[t._v("#")]),t._v(" Output shape:")]),t._v(" "),a("p",[t._v('5D tensor with shape: - If data_format is "channels_last": (batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth) - If data_format is "channels_first": (batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)')]),t._v(" "),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[t._v("#")]),t._v(" "),a("strong",[t._v("init")])]),t._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L2553-L2583",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source"),a("OutboundLink")],1)]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v(" __init__(\n    cropping=((1, 1), (1, 1), (1, 1)),\n    data_format=None,\n    **kwargs\n)\n")])])])])}),[],!1,null,null,null);e.default=s.exports}}]);