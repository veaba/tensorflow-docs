(window.webpackJsonp=window.webpackJsonp||[]).push([[854],{1042:function(e,t,o){"use strict";o.r(t);var s=o(0),a=Object(s.a)({},(function(){var e=this,t=e.$createElement,o=e._self._c||t;return o("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[o("p",[e._v("Shards computation for parallel execution.")]),e._v(" "),o("div",{staticClass:"language- extra-class"},[o("pre",{pre:!0,attrs:{class:"language-text"}},[o("code",[e._v(" tf.compat.v1.tpu.shard(\n    computation,\n    inputs=None,\n    num_shards=1,\n    input_shard_axes=None,\n    outputs_from_all_shards=True,\n    output_shard_axes=None,\n    infeed_queue=None,\n    device_assignment=None,\n    name=None\n)\n")])])]),o("p",[e._v("inputs must be a list of Tensors or None (equivalent to an empty list), each of which has a corresponding split axis (from input_shard_axes). Each input is split into num_shards pieces along the corresponding axis, and computation is applied to each shard in parallel.")]),e._v(" "),o("p",[e._v("Tensors are broadcast to all shards if they are lexically captured by computation. e.g.,")]),e._v(" "),o("p",[e._v("x = tf.constant(7) def computation(): return x + 3 ... = shard(computation, ...)")]),e._v(" "),o("p",[e._v("TODO(phawkins): consider adding support for broadcasting Tensors passed as inputs.")]),e._v(" "),o("p",[e._v("If outputs_from_all_shards is true, the outputs from all shards of computation are concatenated back together along their output_shards_axes. Otherwise, each output is taken from an arbitrary shard.")]),e._v(" "),o("p",[e._v("Inputs and outputs of the computation must be at least rank-1 Tensors.")]),e._v(" "),o("h4",{attrs:{id:"args"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),o("ul",[o("li",[o("code",[e._v("computation")]),e._v(": A Python function that builds a "),o("code",[e._v("computation")]),e._v(" to apply to each shard of the input.")]),e._v(" "),o("li",[o("code",[e._v("inputs")]),e._v(": A list of input tensors or None (equivalent to an empty list). Each input tensor has a corresponding shard axes, given by "),o("code",[e._v("input_shard_axes")]),e._v(", which must have size divisible by "),o("code",[e._v("num_shards")]),e._v(".")]),e._v(" "),o("li",[o("code",[e._v("num_shards")]),e._v(": The number of shards.")]),e._v(" "),o("li",[o("code",[e._v("input_shard_axes")]),e._v(": A list of dimensions along which to shard "),o("code",[e._v("inputs")]),e._v(", or "),o("code",[e._v("None")]),e._v(". "),o("code",[e._v("None")]),e._v(' means "shard all '),o("code",[e._v("inputs")]),e._v(' along dimension 0". If not '),o("code",[e._v("None")]),e._v(", there must be one dimension per input.")]),e._v(" "),o("li",[o("code",[e._v("outputs_from_all_shards")]),e._v(": Boolean or list of boolean. For each output, if "),o("code",[e._v("True")]),e._v(", outputs from all shards are concatenated along the corresponding "),o("code",[e._v("output_shard_axes")]),e._v(" entry. Otherwise, each output is taken from an arbitrary shard. If the argument is a boolean, the argument's value is used for each output.")]),e._v(" "),o("li",[o("code",[e._v("output_shard_axes")]),e._v(": A list of dimensions along which to concatenate the outputs of "),o("code",[e._v("computation")]),e._v(", or "),o("code",[e._v("None")]),e._v(". "),o("code",[e._v("None")]),e._v(' means "concatenate all outputs along dimension 0". If not '),o("code",[e._v("None")]),e._v(", there must be one dimension per output. Ignored if "),o("code",[e._v("outputs_from_all_shards")]),e._v(" is False.")]),e._v(" "),o("li",[o("code",[e._v("infeed_queue")]),e._v(": If not "),o("code",[e._v("None")]),e._v(", the "),o("code",[e._v("InfeedQueue")]),e._v(" to use to augment the "),o("code",[e._v("inputs")]),e._v(" of "),o("code",[e._v("computation")]),e._v(".")]),e._v(" "),o("li",[o("code",[e._v("device_assignment")]),e._v(": If not "),o("code",[e._v("None")]),e._v(", a "),o("code",[e._v("DeviceAssignment")]),e._v(" describing the mapping between logical cores in the "),o("code",[e._v("computation")]),e._v(" with physical cores in the TPU topology. Uses a default device assignment if "),o("code",[e._v("None")]),e._v(". The "),o("code",[e._v("DeviceAssignment")]),e._v(" may be omitted if each shard of the "),o("code",[e._v("computation")]),e._v(" uses only one core, and there is either only one shard, or the number of shards is equal to the number of cores in the TPU system.")]),e._v(" "),o("li",[o("code",[e._v("name")]),e._v(": (Deprecated) Does nothing.")])]),e._v(" "),o("h4",{attrs:{id:"returns"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),o("p",[e._v("A list of output tensors.")]),e._v(" "),o("h4",{attrs:{id:"raises"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),o("ul",[o("li",[o("code",[e._v("ValueError")]),e._v(": If num_shards <= 0")]),e._v(" "),o("li",[o("code",[e._v("ValueError")]),e._v(": If len(input_shard_axes) != len(inputs)")]),e._v(" "),o("li",[o("code",[e._v("ValueError")]),e._v(": If len(output_shard_axes) != len(outputs from "),o("code",[e._v("computation")]),e._v(")")])])])}),[],!1,null,null,null);t.default=a.exports}}]);