(window.webpackJsonp=window.webpackJsonp||[]).push([[267],{454:function(e,t,a){"use strict";a.r(t);var i=a(0),r=Object(i.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-embeddingconfigspec"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-embeddingconfigspec","aria-hidden":"true"}},[e._v("#")]),e._v(" Class EmbeddingConfigSpec")]),e._v(" "),a("p",[e._v("Class to keep track of the specification for TPU embeddings.")]),e._v(" "),a("p",[e._v("Pass this class to tf.estimator.tpu.TPUEstimator via the embedding_config_spec parameter. At minimum you need to specify feature_columns and optimization_parameters. The feature columns passed should be created with some combination of tf.tpu.experimental.embedding_column and tf.tpu.experimental.shared_embedding_columns.\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/tpu/experimental",target:"_blank",rel:"noopener noreferrer"}},[e._v("tf.tpu.experimental"),a("OutboundLink")],1),e._v("TPU embeddings do not support arbitrary Tensorflow optimizers and the main optimizer you use for your model will be ignored for the embedding table variables. Instead TPU embeddigns support a fixed set of predefined optimizers that you can select from and set the parameters of. These include adagrad, adam and stochastic gradient descent. Each supported optimizer has a Parameters class in the  namespace.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' column_a = tf.feature_column.categorical_column_with_identity(...)\ncolumn_b = tf.feature_column.categorical_column_with_identity(...)\ncolumn_c = tf.feature_column.categorical_column_with_identity(...)\ntpu_shared_columns = tf.tpu.experimental.shared_embedding_columns(\n    [column_a, column_b], 10)\ntpu_non_shared_column = tf.tpu.experimental.embedding_column(\n    column_c, 10)\ntpu_columns = [tpu_non_shared_column] + tpu_shared_columns\n...\ndef model_fn(features):\n  dense_features = tf.keras.layers.DenseFeature(tpu_columns)\n  embedded_feature = dense_features(features)\n  ...\n\nestimator = tf.estimator.tpu.TPUEstimator(\n    model_fn=model_fn,\n    ...\n    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(\n        column=tpu_columns,\n        optimization_parameters=(\n            tf.estimator.tpu.experimental.AdagradParameters(0.1))))\n\n<h2 id="__new__"><code>__new__</code></h2>\n\n<a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/tpu/_tpu_estimator_embedding.py">View source</a>\n\n``` python\n@staticmethod\n__new__(\n    cls,\n    feature_columns=None,\n    optimization_parameters=None,\n    clipping_limit=None,\n    pipeline_execution_with_tensor_core=False,\n    experimental_gradient_multiplier_fn=None,\n    feature_to_config_dict=None,\n    table_to_config_dict=None,\n    partition_strategy=\'div\'\n)\n')])])]),a("p",[e._v("Creates an EmbeddingConfigSpec instance.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("feature_columns")]),e._v(": All embedding "),a("code",[e._v("FeatureColumn")]),e._v("s used by model.")]),e._v(" "),a("li",[a("code",[e._v("optimization_parameters")]),e._v(": An instance of "),a("code",[e._v("AdagradParameters")]),e._v(", "),a("code",[e._v("AdamParameters")]),e._v(" or "),a("code",[e._v("StochasticGradientDescentParameters")]),e._v(". This optimizer will be applied to all embedding variables specified by "),a("code",[e._v("feature_columns")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("clipping_limit")]),e._v(": (Optional) Clipping limit (absolute value).")]),e._v(" "),a("li",[a("code",[e._v("pipeline_execution_with_tensor_core")]),e._v(": setting this to "),a("code",[e._v("True")]),e._v(" makes training faster, but trained model will be different if step N and step N+1 involve the same set of embedding IDs. Please see "),a("code",[e._v("tpu_embedding_configuration.proto")]),e._v(" for details.")]),e._v(" "),a("li",[a("code",[e._v("experimental_gradient_multiplier_fn")]),e._v(": (Optional) A Fn taking global step as input returning the current multiplier for all embedding gradients.")]),e._v(" "),a("li",[a("code",[e._v("feature_to_config_dict")]),e._v(": A dictionary mapping features names to instances of the class "),a("code",[e._v("FeatureConfig")]),e._v(". Either features_columns or the pair of "),a("code",[e._v("feature_to_config_dict")]),e._v(" and "),a("code",[e._v("table_to_config_dict")]),e._v(" must be specified.")]),e._v(" "),a("li",[a("code",[e._v("table_to_config_dict")]),e._v(": A dictionary mapping features names to instances of the class "),a("code",[e._v("TableConfig")]),e._v(". Either features_columns or the pair of "),a("code",[e._v("feature_to_config_dict")]),e._v(" and "),a("code",[e._v("table_to_config_dict")]),e._v(" must be specified.")]),e._v(" "),a("li",[a("code",[e._v("partition_strategy")]),e._v(": A string, determining how tensors are sharded to the tpu hosts. See "),a("code",[e._v("tf.nn.safe_embedding_lookup_sparse")]),e._v(" for more details. Allowed value are "),a("code",[e._v('"div"')]),e._v(" and "),a("code",[e._v('"mod"\'. If')]),e._v('"mod"` is used, evaluation and exporting the model to CPU will not work as expected.')])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("An EmbeddingConfigSpec instance.")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": If the feature_columns are not specified.")]),e._v(" "),a("li",[a("code",[e._v("TypeError")]),e._v(": If the feature columns are not of ths correct type (one of _SUPPORTED_FEATURE_COLUMNS, _TPU_EMBEDDING_COLUMN_CLASSES OR _EMBEDDING_COLUMN_CLASSES).")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": If "),a("code",[e._v("optimization_parameters")]),e._v(" is not one of the required types.")])]),e._v(" "),a("h2",{attrs:{id:"properties"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#properties","aria-hidden":"true"}},[e._v("#")]),e._v(" Properties")]),e._v(" "),a("h3",{attrs:{id:"feature-columns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#feature-columns","aria-hidden":"true"}},[e._v("#")]),e._v(" feature_columns")]),e._v(" "),a("h3",{attrs:{id:"optimization-parameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#optimization-parameters","aria-hidden":"true"}},[e._v("#")]),e._v(" optimization_parameters")]),e._v(" "),a("h3",{attrs:{id:"clipping-limit"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#clipping-limit","aria-hidden":"true"}},[e._v("#")]),e._v(" clipping_limit")]),e._v(" "),a("h3",{attrs:{id:"pipeline-execution-with-tensor-core"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pipeline-execution-with-tensor-core","aria-hidden":"true"}},[e._v("#")]),e._v(" pipeline_execution_with_tensor_core")]),e._v(" "),a("h3",{attrs:{id:"experimental-gradient-multiplier-fn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#experimental-gradient-multiplier-fn","aria-hidden":"true"}},[e._v("#")]),e._v(" experimental_gradient_multiplier_fn")]),e._v(" "),a("h3",{attrs:{id:"feature-to-config-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#feature-to-config-dict","aria-hidden":"true"}},[e._v("#")]),e._v(" feature_to_config_dict")]),e._v(" "),a("h3",{attrs:{id:"table-to-config-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#table-to-config-dict","aria-hidden":"true"}},[e._v("#")]),e._v(" table_to_config_dict")]),e._v(" "),a("h3",{attrs:{id:"partition-strategy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#partition-strategy","aria-hidden":"true"}},[e._v("#")]),e._v(" partition_strategy")])])}),[],!1,null,null,null);t.default=r.exports}}]);