(window.webpackJsonp=window.webpackJsonp||[]).push([[1516],{1707:function(e,a,t){"use strict";t.r(a);var s=t(0),o=Object(s.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Batchwise dot product.")]),e._v(" "),t("h3",{attrs:{id:"aliases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("tf.compat.v1.keras.backend.batch_dot")])]),e._v(" "),t("li",[t("code",[e._v("tf.compat.v2.keras.backend.batch_dot")])])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.keras.backend.batch_dot(\n    x,\n    y,\n    axes=None\n)\n")])])]),t("p",[e._v("batch_dot is used to compute dot product of x and y when x and y are data in batch, i.e. in a shape of (batch_size, ðŸ˜ƒ. batch_dot results in a tensor or variable with less dimensions than the input. If the number of dimensions is reduced to 1, we use expand_dims to make sure that ndim is at least 2.")]),e._v(" "),t("h4",{attrs:{id:"arguments"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("x")]),e._v(": Keras tensor or variable with "),t("code",[e._v("ndim >= 2")]),e._v(".")]),e._v(" "),t("li",[t("code",[e._v("y")]),e._v(": Keras tensor or variable with "),t("code",[e._v("ndim >= 2")]),e._v(".")]),e._v(" "),t("li",[t("code",[e._v("axes")]),e._v(": Tuple or list of integers with target dimensions, or single integer. The sizes of "),t("code",[e._v("x")]),e._v(".shape["),t("code",[e._v("axes")]),e._v("[0]] and "),t("code",[e._v("y")]),e._v(".shape["),t("code",[e._v("axes")]),e._v("[1]] should be equal.")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1).")]),e._v(" "),t("h4",{attrs:{id:"examples"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#examples","aria-hidden":"true"}},[e._v("#")]),e._v(" Examples:")]),e._v(" "),t("p",[e._v("Assume x = [[1, 2], [3, 4]] and y = [[5, 6], [7, 8]] batch_dot(x, y, axes=1) = [[17], [53]] which is the main diagonal of x.dot(y.T), although we never have to calculate the off-diagonal elements.")]),e._v(" "),t("p",[e._v("Shape inference: Let x's shape be (100, 20) and y's shape be (100, 30, 20). If axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x's shape and y's shape: * x.shape[0] : 100 : append to output shape * x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. (dot_axes[0] = 1) * y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y * y.shape[1] : 30 : append to output shape * y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. (dot_axes[1] = 2) output_shape = (100, 30)")])])}),[],!1,null,null,null);a.default=o.exports}}]);