(window.webpackJsonp=window.webpackJsonp||[]).push([[485],{674:function(e,_,v){"use strict";v.r(_);var o=v(0),a=Object(o.a)({},(function(){var e=this,_=e.$createElement,v=e._self._c||_;return v("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[v("h2",{attrs:{id:"class-batchnormalization"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#class-batchnormalization","aria-hidden":"true"}},[e._v("#")]),e._v(" Class BatchNormalization")]),e._v(" "),v("p",[e._v("Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n"),v("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/BatchNormalization",target:"_blank",rel:"noopener noreferrer"}},[e._v("BatchNormalization"),v("OutboundLink")],1),e._v("Inherits From: , Layer")]),e._v(" "),v("p",[e._v('"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"')]),e._v(" "),v("p",[e._v("Sergey Ioffe, Christian Szegedy")]),e._v(" "),v("p",[e._v("Keras APIs handle BatchNormalization updates to the moving_mean and moving_variance as part of their fit() and evaluate() loops. However, if a custom training loop is used with an instance of Model, these updates need to be explicitly included. Here's a simple example of how it can be done:")]),e._v(" "),v("div",{staticClass:"language- extra-class"},[v("pre",{pre:!0,attrs:{class:"language-text"}},[v("code",[e._v("   # model is an instance of Model that contains BatchNormalization layer.\n  update_ops = model.get_updates_for(None) + model.get_updates_for(features)\n  train_op = optimizer.minimize(loss)\n  train_op = tf.group([train_op, update_ops])\n")])])]),v("h4",{attrs:{id:"arguments"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),v("ul",[v("li",[v("code",[e._v("axis")]),e._v(": An "),v("code",[e._v("int")]),e._v(" or list of "),v("code",[e._v("int")]),e._v(", the "),v("code",[e._v("axis")]),e._v(" or axes that should be normalized, typically the features "),v("code",[e._v("axis")]),e._v("/axes. For instance, after a "),v("code",[e._v("Conv2D")]),e._v(" layer with "),v("code",[e._v('data_format="channels_first"')]),e._v(", set "),v("code",[e._v("axis")]),e._v("=1. If a list of axes is provided, each "),v("code",[e._v("axis")]),e._v(" in "),v("code",[e._v("axis")]),e._v(" will be normalized simultaneously. Default is "),v("code",[e._v("-1")]),e._v(" which uses the last "),v("code",[e._v("axis")]),e._v(". Note: when using multi-"),v("code",[e._v("axis")]),e._v(" batch norm, the "),v("code",[e._v("beta")]),e._v(", "),v("code",[e._v("gamma")]),e._v(", "),v("code",[e._v("moving_mean")]),e._v(", and "),v("code",[e._v("moving_variance")]),e._v(" variables are the same rank as the input Tensor, with dimension size 1 in all reduced (non-"),v("code",[e._v("axis")]),e._v(") dimensions).")]),e._v(" "),v("li",[v("code",[e._v("momentum")]),e._v(": Momentum for the moving average.")]),e._v(" "),v("li",[v("code",[e._v("epsilon")]),e._v(": Small float added to variance to avoid dividing by zero.")]),e._v(" "),v("li",[v("code",[e._v("center")]),e._v(": If True, add offset of "),v("code",[e._v("beta")]),e._v(" to normalized tensor. If False, "),v("code",[e._v("beta")]),e._v(" is ignored.")]),e._v(" "),v("li",[v("code",[e._v("scale")]),e._v(": If True, multiply by "),v("code",[e._v("gamma")]),e._v(". If False, "),v("code",[e._v("gamma")]),e._v(" is not used. When the next layer is linear (also e.g. "),v("code",[e._v("nn.relu")]),e._v("), this can be disabled since the scaling can be done by the next layer.")]),e._v(" "),v("li",[v("code",[e._v("beta")]),e._v("_initializer: Initializer for the "),v("code",[e._v("beta")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("gamma")]),e._v("_initializer: Initializer for the "),v("code",[e._v("gamma")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("moving_mean")]),e._v("_initializer: Initializer for the moving mean.")]),e._v(" "),v("li",[v("code",[e._v("moving_variance")]),e._v("_initializer: Initializer for the moving variance.")]),e._v(" "),v("li",[v("code",[e._v("beta")]),e._v("_regularizer: Optional regularizer for the "),v("code",[e._v("beta")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("gamma")]),e._v("_regularizer: Optional regularizer for the "),v("code",[e._v("gamma")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("beta")]),e._v("_constra"),v("code",[e._v("int")]),e._v(": An optional projection function to be applied to the "),v("code",[e._v("beta")]),e._v(" weight after being updated by an "),v("code",[e._v("Optimizer")]),e._v(" (e.g. used to implement norm constra"),v("code",[e._v("int")]),e._v("s or value constra"),v("code",[e._v("int")]),e._v("s for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constra"),v("code",[e._v("int")]),e._v("s are not safe to use when doing asynchronous distributed training.")]),e._v(" "),v("li",[v("code",[e._v("gamma")]),e._v("_constra"),v("code",[e._v("int")]),e._v(": An optional projection function to be applied to the "),v("code",[e._v("gamma")]),e._v(" weight after being updated by an "),v("code",[e._v("Optimizer")]),e._v(".")]),e._v(" "),v("li",[v("code",[e._v("renorm")]),e._v(": Whether to use Batch Renormalization (https://arxiv.org/abs/1702.03275). This adds extra variables during training. The inference is the same for either value of this parameter.")]),e._v(" "),v("li",[v("code",[e._v("renorm")]),e._v("_clipping: A "),v("code",[e._v("d")]),e._v("ictiona"),v("code",[e._v("r")]),e._v("y that may map keys '"),v("code",[e._v("r")]),e._v("max', '"),v("code",[e._v("r")]),e._v("min', '"),v("code",[e._v("d")]),e._v("max' to scala"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("Tensors")]),e._v(" use"),v("code",[e._v("d")]),e._v(" to clip the "),v("code",[e._v("renorm")]),e._v(" co"),v("code",[e._v("r``r")]),e._v("ection. The co"),v("code",[e._v("r``r")]),e._v("ection ("),v("code",[e._v("r, d")]),e._v(") is use"),v("code",[e._v("d")]),e._v(" as co"),v("code",[e._v("r``r")]),e._v("ecte"),v("code",[e._v("d")]),e._v("_value = no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v("_value * "),v("code",[e._v("r")]),e._v(" + "),v("code",[e._v("d")]),e._v(", with "),v("code",[e._v("r")]),e._v(" clippe"),v("code",[e._v("d")]),e._v(" to ["),v("code",[e._v("r")]),e._v("min, "),v("code",[e._v("r")]),e._v("max], an"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("d")]),e._v(" to [-"),v("code",[e._v("d")]),e._v("max, "),v("code",[e._v("d")]),e._v("max]. Missing "),v("code",[e._v("r")]),e._v("max, "),v("code",[e._v("r")]),e._v("min, "),v("code",[e._v("d")]),e._v("max a"),v("code",[e._v("r")]),e._v("e set to inf, 0, inf, "),v("code",[e._v("r")]),e._v("espectively.")]),e._v(" "),v("li",[v("code",[e._v("renorm")]),e._v("_"),v("code",[e._v("momentum")]),e._v(": Momentum use"),v("code",[e._v("d")]),e._v(" to up"),v("code",[e._v("d")]),e._v("ate the moving means an"),v("code",[e._v("d")]),e._v(" stan"),v("code",[e._v("d")]),e._v("a"),v("code",[e._v("r``d")]),e._v(" "),v("code",[e._v("d")]),e._v("eviations with "),v("code",[e._v("renorm")]),e._v(". Unlike "),v("code",[e._v("momentum")]),e._v(", this affects t"),v("code",[e._v("r")]),e._v("aining an"),v("code",[e._v("d")]),e._v(" shoul"),v("code",[e._v("d")]),e._v(" be neithe"),v("code",[e._v("r")]),e._v(" too small (which woul"),v("code",[e._v("d")]),e._v(" a"),v("code",[e._v("d``d")]),e._v(" noise) no"),v("code",[e._v("r")]),e._v(" too la"),v("code",[e._v("r")]),e._v("ge (which woul"),v("code",[e._v("d")]),e._v(" give stale estimates). Note that "),v("code",[e._v("momentum")]),e._v(" is still applie"),v("code",[e._v("d")]),e._v(" to get the means an"),v("code",[e._v("d")]),e._v(" va"),v("code",[e._v("r")]),e._v("iances fo"),v("code",[e._v("r")]),e._v(" infe"),v("code",[e._v("r")]),e._v("ence.")]),e._v(" "),v("li",[v("code",[e._v("fused")]),e._v(": if "),v("code",[e._v("None")]),e._v(" o"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("True")]),e._v(", use a faste"),v("code",[e._v("r")]),e._v(", "),v("code",[e._v("fused")]),e._v(" implementation if possible. If "),v("code",[e._v("False")]),e._v(", use the system "),v("code",[e._v("r")]),e._v("ecommen"),v("code",[e._v("d")]),e._v("e"),v("code",[e._v("d")]),e._v(" implementation.")]),e._v(" "),v("li",[v("code",[e._v("trainable")]),e._v(": Boolean, if "),v("code",[e._v("True")]),e._v(" also a"),v("code",[e._v("d``d")]),e._v(" va"),v("code",[e._v("r")]),e._v("iables to the g"),v("code",[e._v("r")]),e._v("aph collection "),v("code",[e._v("GraphKeys.TRAINABLE_VARIABLES")]),e._v(" (see tf.Va"),v("code",[e._v("r")]),e._v("iable).")]),e._v(" "),v("li",[v("code",[e._v("virtual_batch_size")]),e._v(": An "),v("code",[e._v("int")]),e._v(". By "),v("code",[e._v("d")]),e._v("efault, "),v("code",[e._v("virtual_batch_size")]),e._v(" is "),v("code",[e._v("None")]),e._v(", which means batch no"),v("code",[e._v("r")]),e._v("malization is pe"),v("code",[e._v("r")]),e._v("fo"),v("code",[e._v("r")]),e._v("me"),v("code",[e._v("d")]),e._v(" ac"),v("code",[e._v("r")]),e._v("oss the whole batch. When "),v("code",[e._v("virtual_batch_size")]),e._v(" is not "),v("code",[e._v("None")]),e._v(", instea"),v("code",[e._v("d")]),e._v(" pe"),v("code",[e._v("r")]),e._v("fo"),v("code",[e._v("r")]),e._v('m "Ghost Batch No'),v("code",[e._v("r")]),e._v('malization", which c'),v("code",[e._v("r")]),e._v("eates vi"),v("code",[e._v("r")]),e._v("tual sub-batches which a"),v("code",[e._v("r")]),e._v("e each no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v(" sepa"),v("code",[e._v("r")]),e._v("ately (with sha"),v("code",[e._v("r")]),e._v("e"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("gamma")]),e._v(", "),v("code",[e._v("beta")]),e._v(", an"),v("code",[e._v("d")]),e._v(" moving statistics). Must "),v("code",[e._v("d")]),e._v("ivi"),v("code",[e._v("d")]),e._v("e the actual batch size "),v("code",[e._v("d")]),e._v("u"),v("code",[e._v("r")]),e._v("ing execution.")]),e._v(" "),v("li",[v("code",[e._v("adjustment")]),e._v(": A function taking the "),v("code",[e._v("Tensor")]),e._v(" containing the ("),v("code",[e._v("d")]),e._v("ynamic) shape of the input tenso"),v("code",[e._v("r")]),e._v(" an"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("r")]),e._v("etu"),v("code",[e._v("r")]),e._v("ning a pai"),v("code",[e._v("r")]),e._v(" ("),v("code",[e._v("scale")]),e._v(", bias) to apply to the no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v(" values (befo"),v("code",[e._v("r")]),e._v("e "),v("code",[e._v("gamma")]),e._v(" an"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("beta")]),e._v("), only "),v("code",[e._v("d")]),e._v("u"),v("code",[e._v("r")]),e._v("ing t"),v("code",[e._v("r")]),e._v("aining. Fo"),v("code",[e._v("r")]),e._v(" example, if "),v("code",[e._v("axis")]),e._v("=="),v("code",[e._v("-1")]),e._v(", "),v("code",[e._v("adjustment")]),e._v(" = lamb"),v("code",[e._v("d")]),e._v("a shape: ( tf."),v("code",[e._v("r")]),e._v("an"),v("code",[e._v("d")]),e._v("om.unifo"),v("code",[e._v("r")]),e._v("m(shape["),v("code",[e._v("-1")]),e._v(":], 0.93, 1.07), tf."),v("code",[e._v("r")]),e._v("an"),v("code",[e._v("d")]),e._v("om.unifo"),v("code",[e._v("r")]),e._v("m(shape["),v("code",[e._v("-1")]),e._v(":], -0.1, 0.1)) will "),v("code",[e._v("scale")]),e._v(" the no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v(" value by up to 7% up o"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("d")]),e._v("own, then shift the "),v("code",[e._v("r")]),e._v("esult by up to 0.1 (with in"),v("code",[e._v("d")]),e._v("epen"),v("code",[e._v("d")]),e._v("ent scaling an"),v("code",[e._v("d")]),e._v(" bias fo"),v("code",[e._v("r")]),e._v(" each featu"),v("code",[e._v("r")]),e._v("e but sha"),v("code",[e._v("r")]),e._v("e"),v("code",[e._v("d")]),e._v(" ac"),v("code",[e._v("r")]),e._v("oss all examples), an"),v("code",[e._v("d")]),e._v(" finally apply "),v("code",[e._v("gamma")]),e._v(" an"),v("code",[e._v("d")]),e._v("/o"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("beta")]),e._v(". If "),v("code",[e._v("None")]),e._v(", no "),v("code",[e._v("adjustment")]),e._v(" is applie"),v("code",[e._v("d")]),e._v(". Cannot be specifie"),v("code",[e._v("d")]),e._v(" if "),v("code",[e._v("virtual_batch_size")]),e._v(" is specifie"),v("code",[e._v("d")]),e._v(".")]),e._v(" "),v("li",[v("code",[e._v("name")]),e._v(": A st"),v("code",[e._v("r")]),e._v("ing, the "),v("code",[e._v("name")]),e._v(" of the laye"),v("code",[e._v("r")]),e._v(".")])]),e._v(" "),v("h2",{attrs:{id:"init"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),v("strong",[e._v("init")])]),e._v(" "),v("p",[v("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/layers/normalization.py#L119-L164",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),v("OutboundLink")],1)]),e._v(" "),v("div",{staticClass:"language- extra-class"},[v("pre",{pre:!0,attrs:{class:"language-text"}},[v("code",[e._v(" __init__(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=tf.zeros_initializer(),\n    gamma_initializer=tf.ones_initializer(),\n    moving_mean_initializer=tf.zeros_initializer(),\n    moving_variance_initializer=tf.ones_initializer(),\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    renorm=False,\n    renorm_clipping=None,\n    renorm_momentum=0.99,\n    fused=None,\n    trainable=True,\n    virtual_batch_size=None,\n    adjustment=None,\n    name=None,\n    **kwargs\n)\n")])])]),v("h2",{attrs:{id:"properties"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#properties","aria-hidden":"true"}},[e._v("#")]),e._v(" Properties")]),e._v(" "),v("h3",{attrs:{id:"graph"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#graph","aria-hidden":"true"}},[e._v("#")]),e._v(" graph")]),e._v(" "),v("p",[e._v("DEPRECATED FUNCTION")]),e._v(" "),v("h3",{attrs:{id:"scope-name"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#scope-name","aria-hidden":"true"}},[e._v("#")]),e._v(" scope_name")])])}),[],!1,null,null,null);_.default=a.exports}}]);