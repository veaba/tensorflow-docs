(window.webpackJsonp=window.webpackJsonp||[]).push([[1740],{1931:function(e,o,t){"use strict";t.r(o);var v=t(0),_=Object(v.a)({},(function(){var e=this,o=e.$createElement,t=e._self._c||o;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"class-conv2dtranspose"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#class-conv2dtranspose","aria-hidden":"true"}},[e._v("#")]),e._v(" Class Conv2DTranspose")]),e._v(" "),t("p",[e._v("Transposed convolution layer (sometimes called Deconvolution).\n"),t("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D",target:"_blank",rel:"noopener noreferrer"}},[e._v("Conv2D"),t("OutboundLink")],1),e._v("Inherits From:")]),e._v(" "),t("h3",{attrs:{id:"aliases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),t("ul",[t("li",[e._v("Class "),t("code",[e._v("tf.compat.v1.keras.layers.Conv2DTranspose")])]),e._v(" "),t("li",[e._v("Class "),t("code",[e._v("tf.compat.v1.keras.layers.Convolution2DTranspose")])]),e._v(" "),t("li",[e._v("Class "),t("code",[e._v("tf.compat.v2.keras.layers.Conv2DTranspose")])]),e._v(" "),t("li",[e._v("Class "),t("code",[e._v("tf.compat.v2.keras.layers.Convolution2DTranspose")])]),e._v(" "),t("li",[e._v("Class "),t("code",[e._v("tf.keras.layers.Convolution2DTranspose")])])]),e._v(" "),t("h3",{attrs:{id:"used-in-the-guide"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-guide","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the guide:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("T")]),e._v("h"),t("code",[e._v("e")]),e._v(" "),t("code",[e._v("K")]),e._v("e"),t("code",[e._v("r")]),e._v("a"),t("code",[e._v("s")]),e._v(" "),t("code",[e._v("f")]),e._v("u"),t("code",[e._v("n")]),e._v("c"),t("code",[e._v("t")]),e._v("i"),t("code",[e._v("o")]),e._v("n"),t("code",[e._v("a")]),e._v("l"),t("code"),e._v("A"),t("code",[e._v("P")]),e._v("I"),t("code"),e._v("i"),t("code",[e._v("n")]),e._v(" "),t("code",[e._v("T")]),e._v("e"),t("code",[e._v("n")]),e._v("s"),t("code",[e._v("o")]),e._v("r"),t("code",[e._v("F")]),e._v("l"),t("code",[e._v("o")]),e._v("w``")])]),e._v(" "),t("h3",{attrs:{id:"used-in-the-tutorials"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-tutorials","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the tutorials:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("C")]),e._v("o"),t("code",[e._v("n")]),e._v("v"),t("code",[e._v("o")]),e._v("l"),t("code",[e._v("u")]),e._v("t"),t("code",[e._v("i")]),e._v("o"),t("code",[e._v("n")]),e._v("a"),t("code",[e._v("l")]),e._v(" "),t("code",[e._v("V")]),e._v("a"),t("code",[e._v("r")]),e._v("i"),t("code",[e._v("a")]),e._v("t"),t("code",[e._v("i")]),e._v("o"),t("code",[e._v("n")]),e._v("a"),t("code",[e._v("l")]),e._v(" "),t("code",[e._v("A")]),e._v("u"),t("code",[e._v("t")]),e._v("o"),t("code",[e._v("e")]),e._v("n"),t("code",[e._v("c")]),e._v("o"),t("code",[e._v("d")]),e._v("e"),t("code",[e._v("r")])]),e._v(" "),t("li",[t("code",[e._v("D")]),e._v("e"),t("code",[e._v("e")]),e._v("p"),t("code"),e._v("C"),t("code",[e._v("o")]),e._v("n"),t("code",[e._v("v")]),e._v("o"),t("code",[e._v("l")]),e._v("u"),t("code",[e._v("t")]),e._v("i"),t("code",[e._v("o")]),e._v("n"),t("code",[e._v("a")]),e._v("l"),t("code"),e._v("G"),t("code",[e._v("e")]),e._v("n"),t("code",[e._v("e")]),e._v("r"),t("code",[e._v("a")]),e._v("t"),t("code",[e._v("i")]),e._v("v"),t("code",[e._v("e")]),e._v(" "),t("code",[e._v("A")]),e._v("d"),t("code",[e._v("v")]),e._v("e"),t("code",[e._v("r")]),e._v("s"),t("code",[e._v("a")]),e._v("r"),t("code",[e._v("i")]),e._v("a"),t("code",[e._v("l")]),e._v(" "),t("code",[e._v("N")]),e._v("e"),t("code",[e._v("t")]),e._v("w"),t("code",[e._v("o")]),e._v("r"),t("code",[e._v("k")])]),e._v(" "),t("li",[t("code",[e._v("I")]),e._v("m"),t("code",[e._v("a")]),e._v("g"),t("code",[e._v("e")]),e._v(" "),t("code",[e._v("s")]),e._v("e"),t("code",[e._v("g")]),e._v("m"),t("code",[e._v("e")]),e._v("n"),t("code",[e._v("t")]),e._v("a"),t("code",[e._v("t")]),e._v("i"),t("code",[e._v("o")]),e._v("n``")]),e._v(" "),t("li",[t("code",[e._v("P")]),e._v("i"),t("code",[e._v("x")]),e._v("2"),t("code",[e._v("P")]),e._v("i"),t("code",[e._v("x")])])]),e._v(" "),t("p",[e._v("The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.")]),e._v(" "),t("p",[e._v('When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format="channels_last".')]),e._v(" "),t("h4",{attrs:{id:"arguments"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("filters")]),e._v(": Integer, the dimensionality of the output space (i.e. the number of output "),t("code",[e._v("filters")]),e._v(" in the convolution).")]),e._v(" "),t("li",[t("code",[e._v("kernel_size")]),e._v(": An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.")]),e._v(" "),t("li",[t("code",[e._v("strides")]),e._v(": An integer or tuple/list of 2 integers, specifying the "),t("code",[e._v("strides")]),e._v(" of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any "),t("code",[e._v("dilation_rate")]),e._v(" value != 1.")]),e._v(" "),t("li",[t("code",[e._v("padding")]),e._v(": one of "),t("code",[e._v('"valid"')]),e._v(" or "),t("code",[e._v('"same"')]),e._v(" (case-insensitive).")]),e._v(" "),t("li",[t("code",[e._v("output_padding")]),e._v(": An integer or tuple/list of 2 integers, specifying the amount of "),t("code",[e._v("padding")]),e._v(" along the height and width of the output tensor. Can be a single integer to specify the same value for all spatial dimensions. The amount of output "),t("code",[e._v("padding")]),e._v(" along a given dimension must be lower than the stride along that same dimension. If set to "),t("code",[e._v("None")]),e._v(" (default), the output shape is inferred.")]),e._v(" "),t("li",[t("code",[e._v("data_format")]),e._v(": A string, one of "),t("code",[e._v("channels_last")]),e._v(" (default) or "),t("code",[e._v("channels_first")]),e._v(". The ordering of the dimensions in the inputs. "),t("code",[e._v("channels_last")]),e._v(" corresponds to inputs with shape ("),t("code",[e._v("batch, height, width, channels")]),e._v(") while "),t("code",[e._v("channels_first")]),e._v(" corresponds to inputs with shape ("),t("code",[e._v("batch, channels, height, width")]),e._v("). It defaults to the "),t("code",[e._v("image_data_format")]),e._v(" value found in your Keras config file at "),t("code",[e._v("~/.keras/keras.json")]),e._v('. If you never set it, then it will be "'),t("code",[e._v("channels_last")]),e._v('".')]),e._v(" "),t("li",[t("code",[e._v("dilation_rate")]),e._v(": an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any "),t("code",[e._v("dilation_rate")]),e._v(" value != 1 is incompatible with specifying any stride value != 1.")]),e._v(" "),t("li",[t("code",[e._v("activation")]),e._v(": Activation function to use. If you don't specify anything, no "),t("code",[e._v("activation")]),e._v(' is applied (ie. "linear" '),t("code",[e._v("activation")]),e._v(": a(x) = x).")]),e._v(" "),t("li",[t("code",[e._v("use_bias")]),e._v(": Boolean, whether the layer uses a bias vector.")]),e._v(" "),t("li",[t("code",[e._v("kernel_initializer")]),e._v(": Initializer for the "),t("code",[e._v("kernel")]),e._v(" weights matrix.")]),e._v(" "),t("li",[t("code",[e._v("bias_initializer")]),e._v(": Initializer for the bias vector.")]),e._v(" "),t("li",[t("code",[e._v("kernel")]),e._v("_regularizer: Regularizer function applied to the "),t("code",[e._v("kernel")]),e._v(" weights matrix.")]),e._v(" "),t("li",[t("code",[e._v("bias_regularizer")]),e._v(": Regularizer function applied to the bias vector.")]),e._v(" "),t("li",[t("code",[e._v("activity_regularizer")]),e._v(': Regularizer function applied to the output of the layer (its "'),t("code",[e._v("activation")]),e._v('")..')]),e._v(" "),t("li",[t("code",[e._v("kernel")]),e._v("_constraint: Constraint function applied to the "),t("code",[e._v("kernel")]),e._v(" matrix.")]),e._v(" "),t("li",[t("code",[e._v("bias_constraint")]),e._v(": Constraint function applied to the bias vector.")])]),e._v(" "),t("h4",{attrs:{id:"input-shape"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#input-shape","aria-hidden":"true"}},[e._v("#")]),e._v(" Input shape:")]),e._v(" "),t("p",[e._v("4D tensor with shape: (batch, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (batch, rows, cols, channels) if data_format='channels_last'.")]),e._v(" "),t("h4",{attrs:{id:"output-shape"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#output-shape","aria-hidden":"true"}},[e._v("#")]),e._v(" Output shape:")]),e._v(" "),t("p",[e._v("4D tensor with shape: (batch, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (batch, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.")]),e._v(" "),t("h4",{attrs:{id:"references"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#references","aria-hidden":"true"}},[e._v("#")]),e._v(" References:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("A")]),e._v(" "),t("code",[e._v("g")]),e._v("u"),t("code",[e._v("i")]),e._v("d"),t("code",[e._v("e")]),e._v(" "),t("code",[e._v("t")]),e._v("o"),t("code"),e._v("c"),t("code",[e._v("o")]),e._v("n"),t("code",[e._v("v")]),e._v("o"),t("code",[e._v("l")]),e._v("u"),t("code",[e._v("t")]),e._v("i"),t("code",[e._v("o")]),e._v("n"),t("code"),e._v("a"),t("code",[e._v("r")]),e._v("i"),t("code",[e._v("t")]),e._v("h"),t("code",[e._v("m")]),e._v("e"),t("code",[e._v("t")]),e._v("i"),t("code",[e._v("c")]),e._v(" "),t("code",[e._v("f")]),e._v("o"),t("code",[e._v("r")]),e._v(" "),t("code",[e._v("d")]),e._v("e"),t("code",[e._v("e")]),e._v("p"),t("code"),e._v("l"),t("code",[e._v("e")]),e._v("a"),t("code",[e._v("r")]),e._v("n"),t("code",[e._v("i")]),e._v("n"),t("code",[e._v("g")])]),e._v(" "),t("li",[t("code",[e._v("D")]),e._v("e"),t("code",[e._v("c")]),e._v("o"),t("code",[e._v("n")]),e._v("v"),t("code",[e._v("o")]),e._v("l"),t("code",[e._v("u")]),e._v("t"),t("code",[e._v("i")]),e._v("o"),t("code",[e._v("n")]),e._v("a"),t("code",[e._v("l")]),e._v(" "),t("code",[e._v("N")]),e._v("e"),t("code",[e._v("t")]),e._v("w"),t("code",[e._v("o")]),e._v("r"),t("code",[e._v("k")]),e._v("s``")])]),e._v(" "),t("h2",{attrs:{id:"init"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),t("strong",[e._v("init")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L709-L753",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),t("OutboundLink")],1)]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" __init__(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    dilation_rate=(1, 1),\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n")])])])])}),[],!1,null,null,null);o.default=_.exports}}]);