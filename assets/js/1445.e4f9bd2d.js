(window.webpackJsonp=window.webpackJsonp||[]).push([[1445],{1636:function(e,a,t){"use strict";t.r(a);var n=t(0),r=Object(n.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Scaled Exponential Linear Unit (SELU).")]),e._v(" "),t("h3",{attrs:{id:"aliases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("tf.compat.v1.keras.activations.selu")])]),e._v(" "),t("li",[t("code",[e._v("tf.compat.v2.keras.activations.selu")])])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.keras.activations.selu(x)\n")])])]),t("p",[e._v("The Scaled Exponential Linear Unit (SELU) activation function is: scale * x if x > 0 and scale * alpha * (exp(x) - 1) if x < 0 where alpha and scale are pre-defined constants (alpha = 1.67326324 and scale = 1.05070098). The SELU activation function multiplies scale > 1 with the "),t("a",{attrs:{href:"https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu",target:"_blank",rel:"noopener noreferrer"}},[e._v("elu"),t("OutboundLink")],1),e._v(" (Exponential Linear Unit (ELU)) to ensure a slope larger than one for positive net inputs.\n"),t("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal",target:"_blank",rel:"noopener noreferrer"}},[e._v("lecun_normal initialization"),t("OutboundLink")],1),e._v('The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see ) and the number of inputs is "large enough" (see references for more information).')]),e._v(" "),t("p",[e._v("(Courtesy: Blog on Towards DataScience at https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9)")]),e._v(" "),t("h4",{attrs:{id:"example-usage"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#example-usage","aria-hidden":"true"}},[e._v("#")]),e._v(" Example Usage:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" n_classes = 10 #10_class problem\nmodel = models.Sequential()\nmodel.add(Dense(64, kernel_initializer='lecun_normal', activation='selu',\ninput_shape=(28, 28, 1))))\nmodel.add(Dense(32, kernel_initializer='lecun_normal', activation='selu'))\nmodel.add(Dense(16, kernel_initializer='lecun_normal', activation='selu'))\nmodel.add(Dense(n_classes, activation='softmax'))\n")])])]),t("h4",{attrs:{id:"arguments"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("x")]),e._v(": A tensor or variable to compute the activation function for.")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("The scaled exponential unit activation: scale * elu(x, alpha).")]),e._v(" "),t("h1",{attrs:{id:"note"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#note","aria-hidden":"true"}},[e._v("#")]),e._v(" Note")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(' - To be used together with the initialization "[lecun_normal]\n(https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal)".\n- To be used together with the dropout variant "[AlphaDropout]\n(https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout)".\n')])])]),t("h4",{attrs:{id:"references"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#references","aria-hidden":"true"}},[e._v("#")]),e._v(" References:")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/1706.02515",target:"_blank",rel:"noopener noreferrer"}},[e._v("Self-Normalizing Neural Networks (Klambauer et al, 2017)"),t("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=r.exports}}]);