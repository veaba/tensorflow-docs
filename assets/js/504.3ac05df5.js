(window.webpackJsonp=window.webpackJsonp||[]).push([[504],{692:function(e,_,v){"use strict";v.r(_);var o=v(0),a=Object(o.a)({},(function(){var e=this,_=e.$createElement,v=e._self._c||_;return v("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[v("p",[e._v("Functional interface for the batch normalization layer. (deprecated)")]),e._v(" "),v("div",{staticClass:"language- extra-class"},[v("pre",{pre:!0,attrs:{class:"language-text"}},[v("code",[e._v(" tf.compat.v1.layers.batch_normalization(\n    inputs,\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=tf.zeros_initializer(),\n    gamma_initializer=tf.ones_initializer(),\n    moving_mean_initializer=tf.zeros_initializer(),\n    moving_variance_initializer=tf.ones_initializer(),\n    beta_regularizer=None,\n    gamma_regularizer=None,\n    beta_constraint=None,\n    gamma_constraint=None,\n    training=False,\n    trainable=True,\n    name=None,\n    reuse=None,\n    renorm=False,\n    renorm_clipping=None,\n    renorm_momentum=0.99,\n    fused=None,\n    virtual_batch_size=None,\n    adjustment=None\n)\n")])])]),v("p",[e._v("Reference: http://arxiv.org/abs/1502.03167")]),e._v(" "),v("p",[e._v('"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"')]),e._v(" "),v("p",[e._v("Sergey Ioffe, Christian Szegedy")]),e._v(" "),v("div",{staticClass:"language- extra-class"},[v("pre",{pre:!0,attrs:{class:"language-text"}},[v("code",[e._v("   x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)\n\n  # ...\n\n  update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n  train_op = optimizer.minimize(loss)\n  train_op = tf.group([train_op, update_ops])\n")])])]),v("h4",{attrs:{id:"arguments"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),v("ul",[v("li",[v("code",[e._v("inputs")]),e._v(": Tensor input.")]),e._v(" "),v("li",[v("code",[e._v("axis")]),e._v(": An "),v("code",[e._v("int")]),e._v(", the "),v("code",[e._v("axis")]),e._v(" that should be normalized (typically the features "),v("code",[e._v("axis")]),e._v("). For instance, after a "),v("code",[e._v("Convolution2D")]),e._v(" layer with "),v("code",[e._v('data_format="channels_first"')]),e._v(", set "),v("code",[e._v("axis")]),e._v("=1 in "),v("code",[e._v("BatchNormalization")]),e._v(".")]),e._v(" "),v("li",[v("code",[e._v("momentum")]),e._v(": Momentum for the moving average.")]),e._v(" "),v("li",[v("code",[e._v("epsilon")]),e._v(": Small float added to variance to avoid dividing by zero.")]),e._v(" "),v("li",[v("code",[e._v("center")]),e._v(": If True, add offset of "),v("code",[e._v("beta")]),e._v(" to normalized tensor. If False, "),v("code",[e._v("beta")]),e._v(" is ignored.")]),e._v(" "),v("li",[v("code",[e._v("scale")]),e._v(": If True, multiply by "),v("code",[e._v("gamma")]),e._v(". If False, "),v("code",[e._v("gamma")]),e._v(" is not used. When the next layer is linear (also e.g. "),v("code",[e._v("nn.relu")]),e._v("), this can be disabled since the scaling can be done by the next layer.")]),e._v(" "),v("li",[v("code",[e._v("beta")]),e._v("_initializer: Initializer for the "),v("code",[e._v("beta")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("gamma")]),e._v("_initializer: Initializer for the "),v("code",[e._v("gamma")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("moving_mean_initializer")]),e._v(": Initializer for the moving mean.")]),e._v(" "),v("li",[v("code",[e._v("moving_variance_initializer")]),e._v(": Initializer for the moving variance.")]),e._v(" "),v("li",[v("code",[e._v("beta")]),e._v("_regularizer: Optional regularizer for the "),v("code",[e._v("beta")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("gamma")]),e._v("_regularizer: Optional regularizer for the "),v("code",[e._v("gamma")]),e._v(" weight.")]),e._v(" "),v("li",[v("code",[e._v("beta")]),e._v("_constra"),v("code",[e._v("int")]),e._v(": An optional projection function to be applied to the "),v("code",[e._v("beta")]),e._v(" weight after being updated by an "),v("code",[e._v("Optimizer")]),e._v(" (e.g. used to implement norm constra"),v("code",[e._v("int")]),e._v("s or value constra"),v("code",[e._v("int")]),e._v("s for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constra"),v("code",[e._v("int")]),e._v("s are not safe to use when doing asynchronous distributed training.")]),e._v(" "),v("li",[v("code",[e._v("gamma")]),e._v("_constra"),v("code",[e._v("int")]),e._v(": An optional projection function to be applied to the "),v("code",[e._v("gamma")]),e._v(" weight after being updated by an "),v("code",[e._v("Optimizer")]),e._v(".")]),e._v(" "),v("li",[v("code",[e._v("training")]),e._v(": Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in "),v("code",[e._v("training")]),e._v(" mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, or else your "),v("code",[e._v("training")]),e._v("/inference will not work properly.")]),e._v(" "),v("li",[v("code",[e._v("trainable")]),e._v(": Boolean, if "),v("code",[e._v("True")]),e._v(" also add variables to the graph collection "),v("code",[e._v("GraphKeys.TRAINABLE_VARIABLES")]),e._v(" (see tf.Variable).")]),e._v(" "),v("li",[v("code",[e._v("name")]),e._v(": String, the "),v("code",[e._v("name")]),e._v(" of the layer.")]),e._v(" "),v("li",[v("code",[e._v("reuse")]),e._v(": Boolean, whether to "),v("code",[e._v("reuse")]),e._v(" the weights of a previous layer by the same "),v("code",[e._v("name")]),e._v(".")]),e._v(" "),v("li",[v("code",[e._v("renorm")]),e._v(": Whether to use Batch Renormalization (https://arxiv.org/abs/1702.03275). This adds extra variables during "),v("code",[e._v("training")]),e._v(". The inference is the same for either value of this parameter.")]),e._v(" "),v("li",[v("code",[e._v("renorm")]),e._v("_clipping: A "),v("code",[e._v("d")]),e._v("ictiona"),v("code",[e._v("r")]),e._v("y that may map keys '"),v("code",[e._v("r")]),e._v("max', '"),v("code",[e._v("r")]),e._v("min', '"),v("code",[e._v("d")]),e._v("max' to scala"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("Tensors")]),e._v(" use"),v("code",[e._v("d")]),e._v(" to clip the "),v("code",[e._v("renorm")]),e._v(" co"),v("code",[e._v("r``r")]),e._v("ection. The co"),v("code",[e._v("r``r")]),e._v("ection ("),v("code",[e._v("r, d")]),e._v(") is use"),v("code",[e._v("d")]),e._v(" as co"),v("code",[e._v("r``r")]),e._v("ecte"),v("code",[e._v("d")]),e._v("_value = no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v("_value * "),v("code",[e._v("r")]),e._v(" + "),v("code",[e._v("d")]),e._v(", with "),v("code",[e._v("r")]),e._v(" clippe"),v("code",[e._v("d")]),e._v(" to ["),v("code",[e._v("r")]),e._v("min, "),v("code",[e._v("r")]),e._v("max], an"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("d")]),e._v(" to [-"),v("code",[e._v("d")]),e._v("max, "),v("code",[e._v("d")]),e._v("max]. Missing "),v("code",[e._v("r")]),e._v("max, "),v("code",[e._v("r")]),e._v("min, "),v("code",[e._v("d")]),e._v("max a"),v("code",[e._v("r")]),e._v("e set to inf, 0, inf, "),v("code",[e._v("r")]),e._v("espectively.")]),e._v(" "),v("li",[v("code",[e._v("renorm")]),e._v("_"),v("code",[e._v("momentum")]),e._v(": Momentum use"),v("code",[e._v("d")]),e._v(" to up"),v("code",[e._v("d")]),e._v("ate the moving means an"),v("code",[e._v("d")]),e._v(" stan"),v("code",[e._v("d")]),e._v("a"),v("code",[e._v("r``d")]),e._v(" "),v("code",[e._v("d")]),e._v("eviations with "),v("code",[e._v("renorm")]),e._v(". Unlike "),v("code",[e._v("momentum")]),e._v(", this affects "),v("code",[e._v("training")]),e._v(" an"),v("code",[e._v("d")]),e._v(" shoul"),v("code",[e._v("d")]),e._v(" be neithe"),v("code",[e._v("r")]),e._v(" too small (which woul"),v("code",[e._v("d")]),e._v(" a"),v("code",[e._v("d``d")]),e._v(" noise) no"),v("code",[e._v("r")]),e._v(" too la"),v("code",[e._v("r")]),e._v("ge (which woul"),v("code",[e._v("d")]),e._v(" give stale estimates). Note that "),v("code",[e._v("momentum")]),e._v(" is still applie"),v("code",[e._v("d")]),e._v(" to get the means an"),v("code",[e._v("d")]),e._v(" va"),v("code",[e._v("r")]),e._v("iances fo"),v("code",[e._v("r")]),e._v(" infe"),v("code",[e._v("r")]),e._v("ence.")]),e._v(" "),v("li",[v("code",[e._v("fused")]),e._v(": if "),v("code",[e._v("None")]),e._v(" o"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("True")]),e._v(", use a faste"),v("code",[e._v("r")]),e._v(", "),v("code",[e._v("fused")]),e._v(" implementation if possible. If "),v("code",[e._v("False")]),e._v(", use the system "),v("code",[e._v("r")]),e._v("ecommen"),v("code",[e._v("d")]),e._v("e"),v("code",[e._v("d")]),e._v(" implementation.")]),e._v(" "),v("li",[v("code",[e._v("virtual_batch_size")]),e._v(": An "),v("code",[e._v("int")]),e._v(". By "),v("code",[e._v("d")]),e._v("efault, "),v("code",[e._v("virtual_batch_size")]),e._v(" is "),v("code",[e._v("None")]),e._v(", which means batch no"),v("code",[e._v("r")]),e._v("malization is pe"),v("code",[e._v("r")]),e._v("fo"),v("code",[e._v("r")]),e._v("me"),v("code",[e._v("d")]),e._v(" ac"),v("code",[e._v("r")]),e._v("oss the whole batch. When "),v("code",[e._v("virtual_batch_size")]),e._v(" is not "),v("code",[e._v("None")]),e._v(", instea"),v("code",[e._v("d")]),e._v(" pe"),v("code",[e._v("r")]),e._v("fo"),v("code",[e._v("r")]),e._v('m "Ghost Batch No'),v("code",[e._v("r")]),e._v('malization", which c'),v("code",[e._v("r")]),e._v("eates vi"),v("code",[e._v("r")]),e._v("tual sub-batches which a"),v("code",[e._v("r")]),e._v("e each no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v(" sepa"),v("code",[e._v("r")]),e._v("ately (with sha"),v("code",[e._v("r")]),e._v("e"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("gamma")]),e._v(", "),v("code",[e._v("beta")]),e._v(", an"),v("code",[e._v("d")]),e._v(" moving statistics). Must "),v("code",[e._v("d")]),e._v("ivi"),v("code",[e._v("d")]),e._v("e the actual batch size "),v("code",[e._v("d")]),e._v("u"),v("code",[e._v("r")]),e._v("ing execution.")]),e._v(" "),v("li",[v("code",[e._v("adjustment")]),e._v(": A function taking the "),v("code",[e._v("Tensor")]),e._v(" containing the ("),v("code",[e._v("d")]),e._v("ynamic) shape of the input tenso"),v("code",[e._v("r")]),e._v(" an"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("r")]),e._v("etu"),v("code",[e._v("r")]),e._v("ning a pai"),v("code",[e._v("r")]),e._v(" ("),v("code",[e._v("scale")]),e._v(", bias) to apply to the no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v(" values (befo"),v("code",[e._v("r")]),e._v("e "),v("code",[e._v("gamma")]),e._v(" an"),v("code",[e._v("d")]),e._v(" "),v("code",[e._v("beta")]),e._v("), only "),v("code",[e._v("d")]),e._v("u"),v("code",[e._v("r")]),e._v("ing "),v("code",[e._v("training")]),e._v(". Fo"),v("code",[e._v("r")]),e._v(" example, if "),v("code",[e._v("axis")]),e._v("==-1, "),v("code",[e._v("adjustment")]),e._v(" = lamb"),v("code",[e._v("d")]),e._v("a shape: ( tf."),v("code",[e._v("r")]),e._v("an"),v("code",[e._v("d")]),e._v("om.unifo"),v("code",[e._v("r")]),e._v("m(shape[-1:], 0.93, 1.07), tf."),v("code",[e._v("r")]),e._v("an"),v("code",[e._v("d")]),e._v("om.unifo"),v("code",[e._v("r")]),e._v("m(shape[-1:], -0.1, 0.1)) will "),v("code",[e._v("scale")]),e._v(" the no"),v("code",[e._v("r")]),e._v("malize"),v("code",[e._v("d")]),e._v(" value by up to 7% up o"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("d")]),e._v("own, then shift the "),v("code",[e._v("r")]),e._v("esult by up to 0.1 (with in"),v("code",[e._v("d")]),e._v("epen"),v("code",[e._v("d")]),e._v("ent scaling an"),v("code",[e._v("d")]),e._v(" bias fo"),v("code",[e._v("r")]),e._v(" each featu"),v("code",[e._v("r")]),e._v("e but sha"),v("code",[e._v("r")]),e._v("e"),v("code",[e._v("d")]),e._v(" ac"),v("code",[e._v("r")]),e._v("oss all examples), an"),v("code",[e._v("d")]),e._v(" finally apply "),v("code",[e._v("gamma")]),e._v(" an"),v("code",[e._v("d")]),e._v("/o"),v("code",[e._v("r")]),e._v(" "),v("code",[e._v("beta")]),e._v(". If "),v("code",[e._v("None")]),e._v(", no "),v("code",[e._v("adjustment")]),e._v(" is applie"),v("code",[e._v("d")]),e._v(". Cannot be specifie"),v("code",[e._v("d")]),e._v(" if "),v("code",[e._v("virtual_batch_size")]),e._v(" is specifie"),v("code",[e._v("d")]),e._v(".")])]),e._v(" "),v("h4",{attrs:{id:"returns"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),v("p",[e._v("Output tensor.")]),e._v(" "),v("h4",{attrs:{id:"raises"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),v("ul",[v("li",[v("code",[e._v("ValueError")]),e._v(": if eager execution is enabled.")])])])}),[],!1,null,null,null);_.default=a.exports}}]);