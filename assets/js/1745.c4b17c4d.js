(window.webpackJsonp=window.webpackJsonp||[]).push([[1745],{1936:function(t,e,a){"use strict";a.r(e);var s=a(0),r=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"class-cropping2d"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-cropping2d","aria-hidden":"true"}},[t._v("#")]),t._v(" Class Cropping2D")]),t._v(" "),a("p",[t._v("Cropping layer for 2D input (e.g. picture).\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer",target:"_blank",rel:"noopener noreferrer"}},[t._v("Layer"),a("OutboundLink")],1),t._v("Inherits From:")]),t._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[t._v("#")]),t._v(" Aliases:")]),t._v(" "),a("ul",[a("li",[t._v("Class "),a("code",[t._v("tf.compat.v1.keras.layers.Cropping2D")])]),t._v(" "),a("li",[t._v("Class "),a("code",[t._v("tf.compat.v2.keras.layers.Cropping2D")])])]),t._v(" "),a("p",[t._v("It crops along spatial dimensions, i.e. height and width.")]),t._v(" "),a("h4",{attrs:{id:"arguments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[t._v("#")]),t._v(" Arguments:")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("cropping")]),t._v(": Int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\nIf int: the same symmetric "),a("code",[t._v("cropping")]),t._v(" is applied to height and width.\nIf tuple of 2 ints: interpreted as two different symmetric "),a("code",[t._v("cropping")]),t._v(" values for height and width: ("),a("code",[t._v("symmetric_height_crop, symmetric_width_crop")]),t._v(").\nIf tuple of 2 tuples of 2 ints: interpreted as ((top_crop, bottom_crop), (left_crop, right_crop))")]),t._v(" "),a("li",[t._v("If int: the same symmetric "),a("code",[t._v("cropping")]),t._v(" is applied to height and width.")]),t._v(" "),a("li",[t._v("If tuple of 2 ints: interpreted as two different symmetric "),a("code",[t._v("cropping")]),t._v(" values for height and width: ("),a("code",[t._v("symmetric_height_crop, symmetric_width_crop")]),t._v(").")]),t._v(" "),a("li",[t._v("If tuple of 2 tuples of 2 ints: interpreted as ((top_crop, bottom_crop), (left_crop, right_crop))")]),t._v(" "),a("li",[a("code",[t._v("data_format")]),t._v(": A string, one of "),a("code",[t._v("channels_last")]),t._v(" (default) or "),a("code",[t._v("channels_first")]),t._v(". The ordering of the dimensions in the inputs. "),a("code",[t._v("channels_last")]),t._v(" corresponds to inputs with shape ("),a("code",[t._v("batch, height, width, channels")]),t._v(") while "),a("code",[t._v("channels_first")]),t._v(" corresponds to inputs with shape ("),a("code",[t._v("batch, channels, height, width")]),t._v("). It defaults to the "),a("code",[t._v("image_data_format")]),t._v(" value found in your Keras config file at "),a("code",[t._v("~/.keras/keras.json")]),t._v('. If you never set it, then it will be "'),a("code",[t._v("channels_last")]),t._v('".')])]),t._v(" "),a("h4",{attrs:{id:"input-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-shape","aria-hidden":"true"}},[t._v("#")]),t._v(" Input shape:")]),t._v(" "),a("p",[t._v('4D tensor with shape: - If data_format is "channels_last": (batch, rows, cols, channels) - If data_format is "channels_first": (batch, channels, rows, cols)')]),t._v(" "),a("h4",{attrs:{id:"output-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#output-shape","aria-hidden":"true"}},[t._v("#")]),t._v(" Output shape:")]),t._v(" "),a("p",[t._v('4D tensor with shape: - If data_format is "channels_last": (batch, cropped_rows, cropped_cols, channels) - If data_format is "channels_first": (batch, channels, cropped_rows, cropped_cols)')]),t._v(" "),a("h4",{attrs:{id:"examples"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#examples","aria-hidden":"true"}},[t._v("#")]),t._v(" Examples:")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v(" # Crop the input 2D images or feature maps\nmodel = Sequential()\nmodel.add(Cropping2D(cropping=((2, 2), (4, 4)),\n                     input_shape=(28, 28, 3)))\n# now model.output_shape == (None, 24, 20, 3)\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Cropping2D(cropping=((2, 2), (2, 2))))\n# now model.output_shape == (None, 20, 16. 64)\n")])])]),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[t._v("#")]),t._v(" "),a("strong",[t._v("init")])]),t._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L2433-L2454",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source"),a("OutboundLink")],1)]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v(" __init__(\n    cropping=((0, 0), (0, 0)),\n    data_format=None,\n    **kwargs\n)\n")])])])])}),[],!1,null,null,null);e.default=r.exports}}]);