(window.webpackJsonp=window.webpackJsonp||[]).push([[1742],{1933:function(e,t,a){"use strict";a.r(t);var o=a(0),i=Object(o.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-conv3dtranspose"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-conv3dtranspose","aria-hidden":"true"}},[e._v("#")]),e._v(" Class Conv3DTranspose")]),e._v(" "),a("p",[e._v("Transposed convolution layer (sometimes called Deconvolution).\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D",target:"_blank",rel:"noopener noreferrer"}},[e._v("Conv3D"),a("OutboundLink")],1),e._v("Inherits From:")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[e._v("Class "),a("code",[e._v("tf.compat.v1.keras.layers.Conv3DTranspose")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.compat.v1.keras.layers.Convolution3DTranspose")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.compat.v2.keras.layers.Conv3DTranspose")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.compat.v2.keras.layers.Convolution3DTranspose")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.keras.layers.Convolution3DTranspose")])])]),e._v(" "),a("p",[e._v("The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.")]),e._v(" "),a("p",[e._v('When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format="channels_last".')]),e._v(" "),a("h4",{attrs:{id:"arguments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("filters")]),e._v(": Integer, the dimensionality of the output space (i.e. the number of output "),a("code",[e._v("filters")]),e._v(" in the convolution).")]),e._v(" "),a("li",[a("code",[e._v("kernel_size")]),e._v(": An integer or tuple/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions.")]),e._v(" "),a("li",[a("code",[e._v("strides")]),e._v(": An integer or tuple/list of 3 integers, specifying the "),a("code",[e._v("strides")]),e._v(" of the convolution along the depth, height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any "),a("code",[e._v("dilation_rate")]),e._v(" value != 1.")]),e._v(" "),a("li",[a("code",[e._v("padding")]),e._v(": one of "),a("code",[e._v('"valid"')]),e._v(" or "),a("code",[e._v('"same"')]),e._v(" (case-insensitive).")]),e._v(" "),a("li",[a("code",[e._v("output_padding")]),e._v(": An integer or tuple/list of 3 integers, specifying the amount of "),a("code",[e._v("padding")]),e._v(" along the depth, height, and width. Can be a single integer to specify the same value for all spatial dimensions. The amount of output "),a("code",[e._v("padding")]),e._v(" along a given dimension must be lower than the stride along that same dimension. If set to "),a("code",[e._v("None")]),e._v(" (default), the output shape is inferred.")]),e._v(" "),a("li",[a("code",[e._v("data_format")]),e._v(": A string, one of "),a("code",[e._v("channels_last")]),e._v(" (default) or "),a("code",[e._v("channels_first")]),e._v(". The ordering of the dimensions in the inputs. "),a("code",[e._v("channels_last")]),e._v(" corresponds to inputs with shape ("),a("code",[e._v("batch, depth, height, width, channels")]),e._v(") while "),a("code",[e._v("channels_first")]),e._v(" corresponds to inputs with shape ("),a("code",[e._v("batch, channels, depth, height, width")]),e._v("). It defaults to the "),a("code",[e._v("image_data_format")]),e._v(" value found in your Keras config file at "),a("code",[e._v("~/.keras/keras.json")]),e._v('. If you never set it, then it will be "'),a("code",[e._v("channels_last")]),e._v('".')]),e._v(" "),a("li",[a("code",[e._v("dilation_rate")]),e._v(": an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any "),a("code",[e._v("dilation_rate")]),e._v(" value != 1 is incompatible with specifying any stride value != 1.")]),e._v(" "),a("li",[a("code",[e._v("activation")]),e._v(": Activation function to use. If you don't specify anything, no "),a("code",[e._v("activation")]),e._v(' is applied (ie. "linear" '),a("code",[e._v("activation")]),e._v(": a(x) = x).")]),e._v(" "),a("li",[a("code",[e._v("use_bias")]),e._v(": Boolean, whether the layer uses a bias vector.")]),e._v(" "),a("li",[a("code",[e._v("kernel_initializer")]),e._v(": Initializer for the "),a("code",[e._v("kernel")]),e._v(" weights matrix.")]),e._v(" "),a("li",[a("code",[e._v("bias_initializer")]),e._v(": Initializer for the bias vector.")]),e._v(" "),a("li",[a("code",[e._v("kernel")]),e._v("_regularizer: Regularizer function applied to the "),a("code",[e._v("kernel")]),e._v(" weights matrix.")]),e._v(" "),a("li",[a("code",[e._v("bias_regularizer")]),e._v(": Regularizer function applied to the bias vector.")]),e._v(" "),a("li",[a("code",[e._v("activity_regularizer")]),e._v(': Regularizer function applied to the output of the layer (its "'),a("code",[e._v("activation")]),e._v('").')]),e._v(" "),a("li",[a("code",[e._v("kernel")]),e._v("_constraint: Constraint function applied to the "),a("code",[e._v("kernel")]),e._v(" matrix.")]),e._v(" "),a("li",[a("code",[e._v("bias_constraint")]),e._v(": Constraint function applied to the bias vector.")])]),e._v(" "),a("h4",{attrs:{id:"input-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-shape","aria-hidden":"true"}},[e._v("#")]),e._v(" Input shape:")]),e._v(" "),a("p",[e._v("5D tensor with shape: (batch, channels, depth, rows, cols) if data_format='channels_first' or 5D tensor with shape: (batch, depth, rows, cols, channels) if data_format='channels_last'.")]),e._v(" "),a("h4",{attrs:{id:"output-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#output-shape","aria-hidden":"true"}},[e._v("#")]),e._v(" Output shape:")]),e._v(" "),a("p",[e._v("5D tensor with shape: (batch, filters, new_depth, new_rows, new_cols) if data_format='channels_first' or 5D tensor with shape: (batch, new_depth, new_rows, new_cols, filters) if data_format='channels_last'. depth and rows and cols values might have changed due to padding.")]),e._v(" "),a("h4",{attrs:{id:"references"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#references","aria-hidden":"true"}},[e._v("#")]),e._v(" References:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("A")]),e._v(" "),a("code",[e._v("g")]),e._v("u"),a("code",[e._v("i")]),e._v("d"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("t")]),e._v("o"),a("code"),e._v("c"),a("code",[e._v("o")]),e._v("n"),a("code",[e._v("v")]),e._v("o"),a("code",[e._v("l")]),e._v("u"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("o")]),e._v("n"),a("code"),e._v("a"),a("code",[e._v("r")]),e._v("i"),a("code",[e._v("t")]),e._v("h"),a("code",[e._v("m")]),e._v("e"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("c")]),e._v(" "),a("code",[e._v("f")]),e._v("o"),a("code",[e._v("r")]),e._v(" "),a("code",[e._v("d")]),e._v("e"),a("code",[e._v("e")]),e._v("p"),a("code"),e._v("l"),a("code",[e._v("e")]),e._v("a"),a("code",[e._v("r")]),e._v("n"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")])]),e._v(" "),a("li",[a("code",[e._v("D")]),e._v("e"),a("code",[e._v("c")]),e._v("o"),a("code",[e._v("n")]),e._v("v"),a("code",[e._v("o")]),e._v("l"),a("code",[e._v("u")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v("a"),a("code",[e._v("l")]),e._v(" "),a("code",[e._v("N")]),e._v("e"),a("code",[e._v("t")]),e._v("w"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("k")]),e._v("s``")])]),e._v(" "),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("init")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L984-L1026",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __init__(\n    filters,\n    kernel_size,\n    strides=(1, 1, 1),\n    padding='valid',\n    output_padding=None,\n    data_format=None,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n")])])])])}),[],!1,null,null,null);t.default=i.exports}}]);