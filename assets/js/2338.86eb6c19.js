(window.webpackJsonp=window.webpackJsonp||[]).push([[2338],{2526:function(e,a,n){"use strict";n.r(a);var t=n(0),s=Object(t.a)({},(function(){var e=this,a=e.$createElement,n=e._self._c||a;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("Concatenates a list of SparseTensor along the specified dimension. (deprecated arguments)")]),e._v(" "),n("h3",{attrs:{id:"aliases"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),n("ul",[n("li",[n("code",[e._v("tf.compat.v2.sparse.concat")])])]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(" tf.sparse.concat(\n    axis,\n    sp_inputs,\n    expand_nonconcat_dims=False,\n    name=None\n)\n")])])]),n("h3",{attrs:{id:"used-in-the-guide"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-guide","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the guide:")]),e._v(" "),n("ul",[n("li",[n("code",[e._v("R")]),e._v("a"),n("code",[e._v("g")]),e._v("g"),n("code",[e._v("e")]),e._v("d"),n("code"),e._v("t"),n("code",[e._v("e")]),e._v("n"),n("code",[e._v("s")]),e._v("o"),n("code",[e._v("r")]),e._v("s``")])]),e._v(" "),n("p",[e._v("Concatenation is with respect to the dense versions of each sparse input. It is assumed that each inputs is a SparseTensor whose elements are ordered along increasing dimension number.")]),e._v(" "),n("p",[e._v("If expand_nonconcat_dim is False, all inputs' shapes must match, except for the concat dimension. If expand_nonconcat_dim is True, then inputs' shapes are allowed to vary among all inputs.")]),e._v(" "),n("p",[e._v("The indices, values, and shapes lists must have the same length.")]),e._v(" "),n("p",[e._v("If expand_nonconcat_dim is False, then the output shape is identical to the inputs', except along the concat dimension, where it is the sum of the inputs' sizes along that dimension.")]),e._v(" "),n("p",[e._v("If expand_nonconcat_dim is True, then the output shape along the non-concat dimensions will be expand to be the largest among all inputs, and it is the sum of the inputs sizes along the concat dimension.")]),e._v(" "),n("p",[e._v("The output elements will be resorted to preserve the sort order along increasing dimension number.")]),e._v(" "),n("p",[e._v("This op runs in O(M log M) time, where M is the total number of non-empty values across all inputs. This is due to the need for an internal sort in order to concatenate efficiently across an arbitrary dimension.")]),e._v(" "),n("p",[e._v("For example, if axis = 1 and the inputs are")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(' sp_inputs[0]: shape = [2, 3]\n[0, 2]: "a"\n[1, 0]: "b"\n[1, 1]: "c"\n\nsp_inputs[1]: shape = [2, 4]\n[0, 1]: "d"\n[0, 2]: "e"\n')])])]),n("p",[e._v("then the output will be")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(' shape = [2, 7]\n[0, 2]: "a"\n[0, 4]: "d"\n[0, 5]: "e"\n[1, 0]: "b"\n[1, 1]: "c"\n')])])]),n("p",[e._v("Graphically this is equivalent to doing")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(" [    a] concat [  d e  ] = [    a   d e  ]\n[b c  ]        [       ]   [b c          ]\n")])])]),n("p",[e._v("Another example, if 'axis = 1' and the inputs are")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(' sp_inputs[0]: shape = [3, 3]\n[0, 2]: "a"\n[1, 0]: "b"\n[2, 1]: "c"\n\nsp_inputs[1]: shape = [2, 4]\n[0, 1]: "d"\n[0, 2]: "e"\n')])])]),n("p",[e._v("if expand_nonconcat_dim = False, this will result in an error. But if expand_nonconcat_dim = True, this will result in:")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(' shape = [3, 7]\n[0, 2]: "a"\n[0, 4]: "d"\n[0, 5]: "e"\n[1, 0]: "b"\n[2, 1]: "c"\n')])])]),n("p",[e._v("Graphically this is equivalent to doing")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v(" [    a] concat [  d e  ] = [    a   d e  ]\n[b    ]        [       ]   [b            ]\n[  c  ]                    [  c          ]\n")])])]),n("h4",{attrs:{id:"args"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),n("ul",[n("li",[n("code",[e._v("axis")]),e._v(": Dimension to concatenate along. Must be in range [-rank, rank), where rank is the number of dimensions in each input "),n("code",[e._v("SparseTensor")]),e._v(".")]),e._v(" "),n("li",[n("code",[e._v("sp_inputs")]),e._v(": List of "),n("code",[e._v("SparseTensor")]),e._v(" to concatenate.")]),e._v(" "),n("li",[n("code",[e._v("name")]),e._v(": A "),n("code",[e._v("name")]),e._v(" prefix for the returned tensors (optional).")]),e._v(" "),n("li",[n("code",[e._v("expand_nonconcat_dim")]),e._v(": Whether to allow the expansion in the non-concat dimensions. Defaulted to False.")]),e._v(" "),n("li",[n("code",[e._v("concat_dim")]),e._v(": The old (deprecated) "),n("code",[e._v("name")]),e._v(" for "),n("code",[e._v("axis")]),e._v(".")]),e._v(" "),n("li",[n("code",[e._v("expand_nonconcat_dim")]),e._v("s: alias for "),n("code",[e._v("expand_nonconcat_dim")])])]),e._v(" "),n("h4",{attrs:{id:"returns"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),n("p",[e._v("A SparseTensor with the concatenated output.")]),e._v(" "),n("h4",{attrs:{id:"raises"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),n("ul",[n("li",[n("code",[e._v("TypeError")]),e._v(": If "),n("code",[e._v("sp_inputs")]),e._v(" is not a list of "),n("code",[e._v("SparseTensor")]),e._v(".")])])])}),[],!1,null,null,null);a.default=s.exports}}]);