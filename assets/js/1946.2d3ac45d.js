(window.webpackJsonp=window.webpackJsonp||[]).push([[1946],{2137:function(e,t,a){"use strict";a.r(t);var s=a(0),r=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-tokenizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-tokenizer","aria-hidden":"true"}},[e._v("#")]),e._v(" Class Tokenizer")]),e._v(" "),a("p",[e._v("Text tokenization utility class.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[e._v("Class "),a("code",[e._v("tf.compat.v1.keras.preprocessing.text.Tokenizer")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.compat.v2.keras.preprocessing.text.Tokenizer")])])]),e._v(" "),a("h3",{attrs:{id:"used-in-the-tutorials"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-tutorials","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the tutorials:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("I")]),e._v("m"),a("code",[e._v("a")]),e._v("g"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("c")]),e._v("a"),a("code",[e._v("p")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v("i"),a("code",[e._v("n")]),e._v("g"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("v")]),e._v("i"),a("code",[e._v("s")]),e._v("u"),a("code",[e._v("a")]),e._v("l"),a("code"),e._v("a"),a("code",[e._v("t")]),e._v("t"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("o")]),e._v("n``")]),e._v(" "),a("li",[a("code",[e._v("N")]),e._v("e"),a("code",[e._v("u")]),e._v("r"),a("code",[e._v("a")]),e._v("l"),a("code"),e._v("m"),a("code",[e._v("a")]),e._v("c"),a("code",[e._v("h")]),e._v("i"),a("code",[e._v("n")]),e._v("e"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("l")]),e._v("a"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("o")]),e._v("n"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("t")]),e._v("e"),a("code",[e._v("n")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")])])]),e._v(" "),a("p",[e._v("This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...")]),e._v(" "),a("h1",{attrs:{id:"arguments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" num_words: the maximum number of words to keep, based\n    on word frequency. Only the most common `num_words-1` words will\n    be kept.\nfilters: a string where each element is a character that will be\n    filtered from the texts. The default is all punctuation, plus\n    tabs and line breaks, minus the `'` character.\nlower: boolean. Whether to convert the texts to lowercase.\nsplit: str. Separator for word splitting.\nchar_level: if True, every character will be treated as a token.\noov_token: if given, it will be added to word_index and used to\n    replace out-of-vocabulary words during text_to_sequence calls\n")])])]),a("p",[e._v("By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized.")]),e._v(" "),a("p",[e._v("0 is a reserved index that won't be assigned to any word.")]),e._v(" "),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("init")])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __init__(\n    num_words=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=' ',\n    char_level=False,\n    oov_token=None,\n    document_count=0,\n    **kwargs\n)\n")])])]),a("p",[e._v("Initialize self. See help(type(self)) for accurate signature.")]),e._v(" "),a("h2",{attrs:{id:"methods"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#methods","aria-hidden":"true"}},[e._v("#")]),e._v(" Methods")]),e._v(" "),a("h3",{attrs:{id:"fit-on-sequences"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fit-on-sequences","aria-hidden":"true"}},[e._v("#")]),e._v(" fit_on_sequences")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" fit_on_sequences(sequences)\n")])])]),a("p",[e._v("Updates internal vocabulary based on a list of sequences.")]),e._v(" "),a("p",[e._v("Required before using sequences_to_matrix (if fit_on_texts was never called).")]),e._v(" "),a("h1",{attrs:{id:"arguments-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' sequences: A list of sequence.\n    A "sequence" is a list of integer word indices.\n')])])]),a("h3",{attrs:{id:"fit-on-texts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fit-on-texts","aria-hidden":"true"}},[e._v("#")]),e._v(" fit_on_texts")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" fit_on_texts(texts)\n")])])]),a("p",[e._v("Updates internal vocabulary based on a list of texts.")]),e._v(" "),a("p",[e._v("In the case where texts contains lists, we assume each entry of the lists to be a token.")]),e._v(" "),a("p",[e._v("Required before using texts_to_sequences or texts_to_matrix.")]),e._v(" "),a("h1",{attrs:{id:"arguments-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" texts: can be a list of strings,\n    a generator of strings (for memory-efficiency),\n    or a list of list of strings.\n")])])]),a("h3",{attrs:{id:"get-config"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get-config","aria-hidden":"true"}},[e._v("#")]),e._v(" get_config")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" get_config()\n")])])]),a("p",[e._v("Returns the tokenizer configuration as Python dictionary. The word count dictionaries used by the tokenizer get serialized into plain JSON, so that the configuration can be read by other projects.")]),e._v(" "),a("h1",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" A Python dictionary with the tokenizer configuration.\n")])])]),a("h3",{attrs:{id:"sequences-to-matrix"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sequences-to-matrix","aria-hidden":"true"}},[e._v("#")]),e._v(" sequences_to_matrix")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" sequences_to_matrix(\n    sequences,\n    mode='binary'\n)\n")])])]),a("p",[e._v("Converts a list of sequences into a Numpy matrix.")]),e._v(" "),a("h1",{attrs:{id:"arguments-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' sequences: list of sequences\n    (a sequence is a list of integer word indices).\nmode: one of "binary", "count", "tfidf", "freq"\n')])])]),a("h1",{attrs:{id:"returns-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" A Numpy matrix.\n")])])]),a("h1",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" ValueError: In case of invalid `mode` argument,\n    or if the Tokenizer requires to be fit to sample data.\n")])])]),a("h3",{attrs:{id:"sequences-to-texts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sequences-to-texts","aria-hidden":"true"}},[e._v("#")]),e._v(" sequences_to_texts")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" sequences_to_texts(sequences)\n")])])]),a("p",[e._v("Transforms each sequence into a list of text.")]),e._v(" "),a("p",[e._v("Only top num_words-1 most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.")]),e._v(" "),a("h1",{attrs:{id:"arguments-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" sequences: A list of sequences (list of integers).\n")])])]),a("h1",{attrs:{id:"returns-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" A list of texts (strings)\n")])])]),a("h3",{attrs:{id:"sequences-to-texts-generator"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sequences-to-texts-generator","aria-hidden":"true"}},[e._v("#")]),e._v(" sequences_to_texts_generator")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" sequences_to_texts_generator(sequences)\n")])])]),a("p",[e._v("Transforms each sequence in sequences to a list of texts(strings).")]),e._v(" "),a("p",[e._v("Each sequence has to a list of integers. In other words, sequences should be a list of sequences")]),e._v(" "),a("p",[e._v("Only top num_words-1 most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.")]),e._v(" "),a("h1",{attrs:{id:"arguments-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-6","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" sequences: A list of sequences.\n")])])]),a("h1",{attrs:{id:"yields"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#yields","aria-hidden":"true"}},[e._v("#")]),e._v(" Yields")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" Yields individual texts.\n")])])]),a("h3",{attrs:{id:"texts-to-matrix"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#texts-to-matrix","aria-hidden":"true"}},[e._v("#")]),e._v(" texts_to_matrix")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" texts_to_matrix(\n    texts,\n    mode='binary'\n)\n")])])]),a("p",[e._v("Convert a list of texts to a Numpy matrix.")]),e._v(" "),a("h1",{attrs:{id:"arguments-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-7","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' texts: list of strings.\nmode: one of "binary", "count", "tfidf", "freq".\n')])])]),a("h1",{attrs:{id:"returns-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" A Numpy matrix.\n")])])]),a("h3",{attrs:{id:"texts-to-sequences"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#texts-to-sequences","aria-hidden":"true"}},[e._v("#")]),e._v(" texts_to_sequences")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" texts_to_sequences(texts)\n")])])]),a("p",[e._v("Transforms each text in texts to a sequence of integers.")]),e._v(" "),a("p",[e._v("Only top num_words-1 most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.")]),e._v(" "),a("h1",{attrs:{id:"arguments-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-8","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" texts: A list of texts (strings).\n")])])]),a("h1",{attrs:{id:"returns-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" A list of sequences.\n")])])]),a("h3",{attrs:{id:"texts-to-sequences-generator"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#texts-to-sequences-generator","aria-hidden":"true"}},[e._v("#")]),e._v(" texts_to_sequences_generator")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" texts_to_sequences_generator(texts)\n")])])]),a("p",[e._v("Transforms each text in texts to a sequence of integers.")]),e._v(" "),a("p",[e._v("Each item in texts can also be a list, in which case we assume each item of that list to be a token.")]),e._v(" "),a("p",[e._v("Only top num_words-1 most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.")]),e._v(" "),a("h1",{attrs:{id:"arguments-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-9","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" texts: A list of texts (strings).\n")])])]),a("h1",{attrs:{id:"yields-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#yields-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Yields")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" Yields individual sequences.\n")])])]),a("h3",{attrs:{id:"to-json"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-json","aria-hidden":"true"}},[e._v("#")]),e._v(" to_json")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" to_json(**kwargs)\n")])])]),a("p",[e._v("Returns a JSON string containing the tokenizer configuration. To load a tokenizer from a JSON string, use keras.preprocessing.text.tokenizer_from_json(json_string).")]),e._v(" "),a("h1",{attrs:{id:"arguments-10"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments-10","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" **kwargs: Additional keyword arguments\n    to be passed to `json.dumps()`.\n")])])]),a("h1",{attrs:{id:"returns-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-6","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" A JSON string containing the tokenizer configuration.\n")])])])])}),[],!1,null,null,null);t.default=r.exports}}]);