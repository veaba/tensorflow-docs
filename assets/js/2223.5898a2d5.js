(window.webpackJsonp=window.webpackJsonp||[]).push([[2223],{2411:function(e,a,t){"use strict";t.r(a);var n=t(0),o=Object(n.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Defined in generated file: python/ops/gen_nn_ops.py")]),e._v(" "),t("p",[e._v("Local Response Normalization.")]),e._v(" "),t("h3",{attrs:{id:"aliases"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("tf.compat.v1.nn.local_response_normalization")])]),e._v(" "),t("li",[t("code",[e._v("tf.compat.v1.nn.lrn")])]),e._v(" "),t("li",[t("code",[e._v("tf.compat.v2.nn.local_response_normalization")])]),e._v(" "),t("li",[t("code",[e._v("tf.compat.v2.nn.lrn")])]),e._v(" "),t("li",[t("code",[e._v("tf.nn.lrn")])])]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.nn.local_response_normalization(\n    input,\n    depth_radius=5,\n    bias=1,\n    alpha=1,\n    beta=0.5,\n    name=None\n)\n")])])]),t("p",[e._v("The 4-D input tensor is treated as a 3-D array of 1-D vectors (along the last dimension), and each vector is normalized independently. Within a given vector, each component is divided by the weighted, squared sum of inputs within depth_radius. In detail,")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" sqr_sum[a, b, c, d] =\n    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\noutput = input / (bias + alpha * sqr_sum) ** beta\n")])])]),t("p",[t("a",{attrs:{href:"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks",target:"_blank",rel:"noopener noreferrer"}},[e._v("Krizhevsky et al., ImageNet classification with deep convolutional neural networks (NIPS 2012)"),t("OutboundLink")],1),e._v("For details, see .")]),e._v(" "),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("input")]),e._v(": A "),t("code",[e._v("Tensor")]),e._v(". Must be one of the following types: "),t("code",[e._v("half")]),e._v(", "),t("code",[e._v("bfloat16")]),e._v(", "),t("code",[e._v("float32")]),e._v(". 4-D.")]),e._v(" "),t("li",[t("code",[e._v("depth_radius")]),e._v(": An optional "),t("code",[e._v("int")]),e._v(". Defaults to "),t("code",[e._v("5")]),e._v(". 0-D. Half-width of the 1-D normalization window.")]),e._v(" "),t("li",[t("code",[e._v("bias")]),e._v(": An optional "),t("code",[e._v("float")]),e._v(". Defaults to "),t("code",[e._v("1")]),e._v(". An offset (usually positive to avoid dividing by 0).")]),e._v(" "),t("li",[t("code",[e._v("alpha")]),e._v(": An optional "),t("code",[e._v("float")]),e._v(". Defaults to "),t("code",[e._v("1")]),e._v(". A scale factor, usually positive.")]),e._v(" "),t("li",[t("code",[e._v("beta")]),e._v(": An optional "),t("code",[e._v("float")]),e._v(". Defaults to "),t("code",[e._v("0.5")]),e._v(". An exponent.")]),e._v(" "),t("li",[t("code",[e._v("name")]),e._v(": A "),t("code",[e._v("name")]),e._v(" for the operation (optional).")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("A Tensor. Has the same type as input.")])])}),[],!1,null,null,null);a.default=o.exports}}]);