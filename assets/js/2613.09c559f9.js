(window.webpackJsonp=window.webpackJsonp||[]).push([[2613],{2804:function(e,t,a){"use strict";a.r(t);var s=a(0),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("Parallel map on the list of tensors unpacked from "),a("code",[e._v("elems")]),e._v(" on dimension 0.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("tf.compat.v1.vectorized_map")])]),e._v(" "),a("li",[a("code",[e._v("tf.compat.v2.vectorized_map")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" tf.vectorized_map(\n    fn,\n    elems\n)\n")])])]),a("p",[e._v("This method works similar to tf.map_"),a("code",[e._v("fn")]),e._v(" but is optimized to run much faster, possibly with a much larger memory footprint. The speedups are obtained by vectorization (see https://arxiv.org/pdf/1903.04243.pdf). The idea behind vectorization is to semantically launch all the invocations of "),a("code",[e._v("fn")]),e._v(" in parallel and fuse corresponding operations across all these invocations. This fusion is done statically at graph generation time and the generated code is often similar in performance to a manually fused version.\n"),a("a",{attrs:{href:"https://tensorflow.google.cn/api_docs/python/tf/vectorized_map",target:"_blank",rel:"noopener noreferrer"}},[e._v("tf.vectorized_map"),a("OutboundLink")],1),e._v("Because  fully parallelizes the batch, this method will generally be significantly faster than using tf.map_fn, especially in eager mode. However this is an experimental feature and currently has a lot of limitations: - There should be no data dependency between the different semantic invocations of fn, i.e. it should be safe to map the elements of the inputs in any order. - Stateful kernels may mostly not be supported since these often imply a data dependency. We do support a limited set of such stateful kernels though (like RandomFoo, Variable operations like reads, etc). - fn has limited support for control flow operations. tf.cond in particular is not supported. - fn should return nested structure of Tensors or Operations. However if an Operation is returned, it should have zero outputs. - The shape and dtype of any intermediate or output tensors in the computation of fn should not depend on the input to fn.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("fn")]),e._v(": The callable to be performed. It accepts one argument, which will have the same (possibly nested) structure as "),a("code",[e._v("elems")]),e._v(", and returns a possibly nested structure of Tensors and Operations, which may be different than the structure of "),a("code",[e._v("elems")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("elems")]),e._v(": A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be mapped over by "),a("code",[e._v("fn")]),e._v(".")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying fn to tensors unpacked from elems along the first dimension, from first to last.")]),e._v(" "),a("h4",{attrs:{id:"examples"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#examples","aria-hidden":"true"}},[e._v("#")]),e._v(" Examples:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" def outer_product(a):\n  return tf.tensordot(a, a, 0)\n\nbatch_size = 100\na = tf.ones((batch_size, 32, 32))\nc = tf.vectorized_map(outer_product, a)\nassert c.shape == (batch_size, 32, 32, 32, 32)\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" # Computing per-example gradients\n\nbatch_size = 10\nnum_features = 32\nlayer = tf.keras.layers.Dense(1)\n\ndef model_fn(arg):\n  with tf.GradientTape() as g:\n    inp, label = arg\n    inp = tf.expand_dims(inp, 0)\n    label = tf.expand_dims(label, 0)\n    prediction = layer(inp)\n    loss = tf.nn.l2_loss(label - prediction)\n  return g.gradient(loss, (layer.kernel, layer.bias))\n\ninputs = tf.random_uniform([batch_size, num_features])\nlabels = tf.random_uniform([batch_size, 1])\nper_example_gradients = tf.vectorized_map(model_fn, (inputs, labels))\nassert per_example_gradients[0].shape == (batch_size, num_features, 1)\nassert per_example_gradients[1].shape == (batch_size, 1)\n")])])])])}),[],!1,null,null,null);t.default=n.exports}}]);