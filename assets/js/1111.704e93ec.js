(window.webpackJsonp=window.webpackJsonp||[]).push([[1111],{1300:function(e,t,a){"use strict";a.r(t);var n=a(0),s=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("A transformation that buckets elements in a Dataset by length.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("tf.compat.v1.data.experimental.bucket_by_sequence_length")])]),e._v(" "),a("li",[a("code",[e._v("tf.compat.v2.data.experimental.bucket_by_sequence_length")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" tf.data.experimental.bucket_by_sequence_length(\n    element_length_func,\n    bucket_boundaries,\n    bucket_batch_sizes,\n    padded_shapes=None,\n    padding_values=None,\n    pad_to_bucket_boundary=False,\n    no_padding=False,\n    drop_remainder=False\n)\n")])])]),a("p",[e._v("Elements of the Dataset are grouped together by length and then are padded and batched.")]),e._v(" "),a("p",[e._v("This is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("element_length_func")]),e._v(": function from element in "),a("code",[e._v("Dataset")]),e._v(" to "),a("code",[e._v("tf.int32")]),e._v(", determines the length of the element, which will determine the bucket it goes into.")]),e._v(" "),a("li",[a("code",[e._v("bucket_boundaries")]),e._v(": "),a("code",[e._v("list<int>")]),e._v(", upper length boundaries of the buckets.")]),e._v(" "),a("li",[a("code",[e._v("bucket_batch_sizes")]),e._v(": "),a("code",[e._v("list<int>")]),e._v(", batch size per bucket. Length should be len("),a("code",[e._v("bucket_boundaries")]),e._v(") + 1.")]),e._v(" "),a("li",[a("code",[e._v("padded_shapes")]),e._v(": Nested structure of "),a("code",[e._v("tf.TensorShape")]),e._v(" to pass to "),a("code",[e._v("tf.data.Dataset.padded_batch")]),e._v(". If not provided, will use "),a("code",[e._v("dataset.output_shapes")]),e._v(", which will result in variable length dimensions being padded out to the maximum length in each batch.")]),e._v(" "),a("li",[a("code",[e._v("padding_values")]),e._v(": Values to pad with, passed to "),a("code",[e._v("tf.data.Dataset.padded_batch")]),e._v(". Defaults to padding with 0.")]),e._v(" "),a("li",[a("code",[e._v("pad_to_bucket_boundary")]),e._v(": bool, if "),a("code",[e._v("False")]),e._v(", will pad dimensions with unknown size to maximum length in batch. If "),a("code",[e._v("True")]),e._v(", will pad dimensions with unknown size to bucket boundary minus 1 (i.e., the maximum length in each bucket), and caller must ensure that the source "),a("code",[e._v("Dataset")]),e._v(" does not contain any elements with length longer than max("),a("code",[e._v("bucket_boundaries")]),e._v(").")]),e._v(" "),a("li",[a("code",[e._v("no_padding")]),e._v(": "),a("code",[e._v("bool")]),e._v(", indicates whether to pad the batch features (features need to be either of type "),a("code",[e._v("tf.SparseTensor")]),e._v(" or of same shape).")]),e._v(" "),a("li",[a("code",[e._v("drop_remainder")]),e._v(": (Optional.) A "),a("code",[e._v("tf.bool")]),e._v(" scalar "),a("code",[e._v("tf.Tensor")]),e._v(", representing whether the last batch should be dropped in the case it has fewer than "),a("code",[e._v("batch_size")]),e._v(" elements; the default behavior is not to drop the smaller batch.")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply",target:"_blank",rel:"noopener noreferrer"}},[e._v("tf.data.Dataset.apply"),a("OutboundLink")],1),e._v("A Dataset transformation function, which can be passed to .")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": if len(bucket_batch_sizes) != len(bucket_boundaries) + 1.")])])])}),[],!1,null,null,null);t.default=s.exports}}]);