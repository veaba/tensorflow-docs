(window.webpackJsonp=window.webpackJsonp||[]).push([[274],{462:function(e,a,t){"use strict";t.r(a);var s=t(0),r=Object(s.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("A CategoricalColumn with a vocabulary file.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" tf.compat.v1.feature_column.categorical_column_with_vocabulary_file(\n    key,\n    vocabulary_file,\n    vocabulary_size=None,\n    num_oov_buckets=0,\n    default_value=None,\n    dtype=tf.dtypes.string\n)\n")])])]),t("p",[e._v("Use this when your inputs are in string or integer format, and you have a vocabulary file that maps each value to an integer ID. By default, out-of-vocabulary values are ignored. Use either (but not both) of num_oov_buckets and default_value to specify how to include out-of-vocabulary values.")]),e._v(" "),t("p",[e._v("For input dictionary features, features[key] is either Tensor or SparseTensor. If Tensor, missing values can be represented by -1 for int and '' for string, which will be dropped by this feature column.")]),e._v(" "),t("p",[e._v("Example with num_oov_buckets: File '/us/states.txt' contains 50 lines, each with a 2-character U.S. state abbreviation. All inputs with values in that file are assigned an ID 0-49, corresponding to its line number. All other values are hashed and assigned an ID 50-54.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" states = categorical_column_with_vocabulary_file(\n    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,\n    num_oov_buckets=5)\ncolumns = [states, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\nlinear_prediction = linear_model(features, columns)\n")])])]),t("p",[e._v("Example with default_value: File '/us/states.txt' contains 51 lines - the first line is 'XX', and the other 50 each have a 2-character U.S. state abbreviation. Both a literal 'XX' in input, and other values missing from the file, will be assigned ID 0. All others are assigned the corresponding line number 1-50.")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" states = categorical_column_with_vocabulary_file(\n    key='states', vocabulary_file='/us/states.txt', vocabulary_size=51,\n    default_value=0)\ncolumns = [states, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\nlinear_prediction, _, _ = linear_model(features, columns)\n")])])]),t("p",[e._v("And to make an embedding with either:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v(" columns = [embedding_column(states, 3),...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\ndense_tensor = input_layer(features, columns)\n")])])]),t("h4",{attrs:{id:"args"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("key")]),e._v(": A unique string identifying the input feature. It is used as the column name and the dictionary "),t("code",[e._v("key")]),e._v(" for feature parsing configs, feature "),t("code",[e._v("Tensor")]),e._v(" objects, and feature columns.")]),e._v(" "),t("li",[t("code",[e._v("vocabulary_file")]),e._v(": The vocabulary file name.")]),e._v(" "),t("li",[t("code",[e._v("vocabulary_size")]),e._v(": Number of the elements in the vocabulary. This must be no greater than length of "),t("code",[e._v("vocabulary_file")]),e._v(", if less than length, later values are ignored. If None, it is set to the length of "),t("code",[e._v("vocabulary_file")]),e._v(".")])]),e._v(" "),t("h4",{attrs:{id:"returns"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),t("p",[e._v("A CategoricalColumn with a vocabulary file.")]),e._v(" "),t("h4",{attrs:{id:"raises"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),t("ul",[t("li",[t("code",[e._v("ValueError")]),e._v(": "),t("code",[e._v("vocabulary_file")]),e._v(" is missing or cannot be opened.")]),e._v(" "),t("li",[t("code",[e._v("ValueError")]),e._v(": "),t("code",[e._v("vocabulary_size")]),e._v(" is missing or < 1.")]),e._v(" "),t("li",[t("code",[e._v("ValueError")]),e._v(": "),t("code",[e._v("num_oov_buckets")]),e._v(" is a negative integer.")]),e._v(" "),t("li",[t("code",[e._v("ValueError")]),e._v(": "),t("code",[e._v("num_oov_buckets")]),e._v(" and "),t("code",[e._v("default_value")]),e._v(" are both specified.")]),e._v(" "),t("li",[t("code",[e._v("ValueError")]),e._v(": "),t("code",[e._v("dtype")]),e._v(" is neither string nor integer.")])])])}),[],!1,null,null,null);a.default=r.exports}}]);