(window.webpackJsonp=window.webpackJsonp||[]).push([[876],{1064:function(e,t,a){"use strict";a.r(t);var r=a(0),s=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-saver"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-saver","aria-hidden":"true"}},[e._v("#")]),e._v(" Class Saver")]),e._v(" "),a("p",[e._v("Saves and restores variables.\n"),a("a",{attrs:{href:"https://tensorflow.org/guide/variables",target:"_blank",rel:"noopener noreferrer"}},[e._v("Variables"),a("OutboundLink")],1),e._v("See  for an overview of variables, saving and restoring.")]),e._v(" "),a("p",[e._v("The Saver class adds ops to save and restore variables to and from checkpoints. It also provides convenience methods to run these ops.")]),e._v(" "),a("p",[e._v("Checkpoints are binary files in a proprietary format which map variable names to tensor values. The best way to examine the contents of a checkpoint is to load it using a Saver.")]),e._v(" "),a("p",[e._v("Savers can automatically number checkpoint filenames with a provided counter. This lets you keep multiple checkpoints at different steps while training a model. For example you can number the checkpoint filenames with the training step number. To avoid filling up disks, savers manage checkpoint files automatically. For example, they can keep only the N most recent files, or one checkpoint for every N hours of training.")]),e._v(" "),a("p",[e._v("You number checkpoint filenames by passing a value to the optional global_step argument to save():")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n...\nsaver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n")])])]),a("p",[e._v("Additionally, optional arguments to the Saver() constructor let you control the proliferation of checkpoint files on disk:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("max_to_keep")]),e._v(" indicates the maximum number of recent "),a("code",[e._v("checkpoint")]),e._v(" files to keep. As new files are created, older files are deleted. If None or 0, no "),a("code",[e._v("checkpoint")]),e._v("s are deleted from the filesystem but only the last one is kept in the "),a("code",[e._v("checkpoint")]),e._v(" file. Defaults to 5 (that is, the 5 most recent "),a("code",[e._v("checkpoint")]),e._v(" files are kept.)")]),e._v(" "),a("li",[a("code",[e._v("keep_checkpoint_every_n_hours")]),e._v(": In addition to keeping the most recent "),a("code",[e._v("max_to_keep")]),e._v(" "),a("code",[e._v("checkpoint")]),e._v(" files, you might want to keep one "),a("code",[e._v("checkpoint")]),e._v(" file for every N hours of training. This can be useful if you want to later analyze how a model progressed during a long training session. For example, passing "),a("code",[e._v("keep_checkpoint_every_n_hours")]),e._v("=2 ensures that you keep one "),a("code",[e._v("checkpoint")]),e._v(" file for every 2 hours of training. The default value of 10,000 hours effectively disables the feature.")])]),e._v(" "),a("p",[e._v("Note that you still have to call the save() method to save the model. Passing these arguments to the constructor will not save variables automatically for you.")]),e._v(" "),a("p",[e._v("A training program that saves regularly looks like:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" ...\n# Create a saver.\nsaver = tf.compat.v1.train.Saver(...variables...)\n# Launch the graph and train, saving the model every 1,000 steps.\nsess = tf.compat.v1.Session()\nfor step in xrange(1000000):\n    sess.run(..training_op..)\n    if step % 1000 == 0:\n        # Append the step number to the checkpoint name:\n        saver.save(sess, 'my-model', global_step=step)\n")])])]),a("p",[e._v("In addition to checkpoint files, savers keep a protocol buffer on disk with the list of recent checkpoints. This is used to manage numbered checkpoint files and by latest_checkpoint(), which makes it easy to discover the path to the most recent checkpoint. That protocol buffer is stored in a file named 'checkpoint' next to the checkpoint files.")]),e._v(" "),a("p",[e._v("If you create several savers, you can specify a different filename for the protocol buffer file in the call to save().")]),e._v(" "),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("init")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L681-L835",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __init__(\n    var_list=None,\n    reshape=False,\n    sharded=False,\n    max_to_keep=5,\n    keep_checkpoint_every_n_hours=10000.0,\n    name=None,\n    restore_sequentially=False,\n    saver_def=None,\n    builder=None,\n    defer_build=False,\n    allow_empty=False,\n    write_version=tf.train.SaverDef.V2,\n    pad_step_number=False,\n    save_relative_paths=False,\n    filename=None\n)\n")])])]),a("p",[e._v("Creates a Saver.")]),e._v(" "),a("p",[e._v("The constructor adds ops to save and restore variables.")]),e._v(" "),a("p",[e._v("var_list specifies the variables that will be saved and restored. It can be passed as a dict or a list:")]),e._v(" "),a("ul",[a("li",[e._v("A "),a("code",[e._v("dict")]),e._v(" of names to variables: The keys are the names that will be used to save or restore the variables in the checkpoint files.")]),e._v(" "),a("li",[e._v("A list of variables: The variables will be keyed with their op name in the checkpoint files.")])]),e._v(" "),a("h4",{attrs:{id:"for-example"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#for-example","aria-hidden":"true"}},[e._v("#")]),e._v(" For example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" v1 = tf.Variable(..., name='v1')\nv2 = tf.Variable(..., name='v2')\n\n# Pass the variables as a dict:\nsaver = tf.compat.v1.train.Saver({'v1': v1, 'v2': v2})\n\n# Or pass them as a list.\nsaver = tf.compat.v1.train.Saver([v1, v2])\n# Passing a list is equivalent to passing a dict with the variable op names\n# as keys:\nsaver = tf.compat.v1.train.Saver({v.op.name: v for v in [v1, v2]})\n")])])]),a("p",[e._v("The optional reshape argument, if True, allows restoring a variable from a save file where the variable had a different shape, but the same number of elements and type. This is useful if you have reshaped a variable and want to reload it from an older checkpoint.")]),e._v(" "),a("p",[e._v("The optional sharded argument, if True, instructs the saver to shard checkpoints per device.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("var_list")]),e._v(": A list of "),a("code",[e._v("Variable")]),e._v("/"),a("code",[e._v("SaveableObject")]),e._v(", or a dictionary mapping names to "),a("code",[e._v("SaveableObject")]),e._v("s. If "),a("code",[e._v("None")]),e._v(", defaults to the list of all saveable objects.")]),e._v(" "),a("li",[a("code",[e._v("reshape")]),e._v(": If "),a("code",[e._v("True")]),e._v(", allows restoring parameters from a checkpoint where the variables have a different shape.")]),e._v(" "),a("li",[a("code",[e._v("sharded")]),e._v(": If "),a("code",[e._v("True")]),e._v(", shard the checkpoints, one per device.")]),e._v(" "),a("li",[a("code",[e._v("max_to_keep")]),e._v(": Maximum number of recent checkpoints to keep. Defaults to 5.")]),e._v(" "),a("li",[a("code",[e._v("keep_checkpoint_every_n_hours")]),e._v(": How often to keep checkpoints. Defaults to 10,000 hours.")]),e._v(" "),a("li",[a("code",[e._v("name")]),e._v(": String. Optional "),a("code",[e._v("name")]),e._v(" to use as a prefix when adding operations.")]),e._v(" "),a("li",[a("code",[e._v("restore_sequentially")]),e._v(": A "),a("code",[e._v("Bool")]),e._v(", which if true, causes restore of different variables to happen sequentially within each device. This can lower memory usage when restoring very large models.")]),e._v(" "),a("li",[a("code",[e._v("saver_def")]),e._v(": Optional "),a("code",[e._v("SaverDef")]),e._v(" proto to use instead of running the builder. This is only useful for specialty code that wants to recreate a "),a("code",[e._v("Saver")]),e._v(" object for a previously built "),a("code",[e._v("Graph")]),e._v(" that had a "),a("code",[e._v("Saver")]),e._v(". The "),a("code",[e._v("saver_def")]),e._v(" proto should be the one returned by the "),a("code",[e._v("as_saver_def")]),e._v("() call of the "),a("code",[e._v("Saver")]),e._v(" that was created for that "),a("code",[e._v("Graph")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("builder")]),e._v(": Optional "),a("code",[e._v("Saver")]),e._v("Builder to use if a "),a("code",[e._v("saver_def")]),e._v(" was not provided. Defaults to "),a("code",[e._v("BulkSaverBuilder")]),e._v("().")]),e._v(" "),a("li",[a("code",[e._v("defer_build")]),e._v(": If "),a("code",[e._v("True")]),e._v(", defer adding the save and restore ops to the "),a("code",[e._v("build")]),e._v("() call. In that case "),a("code",[e._v("build")]),e._v("() should be called before finalizing the graph or using the saver.")]),e._v(" "),a("li",[a("code",[e._v("allow_empty")]),e._v(": If "),a("code",[e._v("False")]),e._v(" (default) raise an error if there are no variables in the graph. Otherwise, construct the saver anyway and make it a no-op.")]),e._v(" "),a("li",[a("code",[e._v("write_version")]),e._v(": controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the "),a("code",[e._v("Saver")]),e._v(" is able to restore from both V2 and V1 checkpoints.")]),e._v(" "),a("li",[a("code",[e._v("pad_step_number")]),e._v(": if "),a("code",[e._v("True")]),e._v(", pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default.")]),e._v(" "),a("li",[a("code",[e._v("save_relative_paths")]),e._v(": If "),a("code",[e._v("True")]),e._v(", will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory.")]),e._v(" "),a("li",[a("code",[e._v("filename")]),e._v(": If known at graph construction time, "),a("code",[e._v("filename")]),e._v(" used for variable loading/saving.")])]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("TypeError")]),e._v(": If "),a("code",[e._v("var_list")]),e._v(" is invalid.")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": If any of the keys or values in "),a("code",[e._v("var_list")]),e._v(" are not unique.")]),e._v(" "),a("li",[a("code",[e._v("RuntimeError")]),e._v(": If eager execution is enabled and"),a("code",[e._v("var_list")]),e._v(" does not specify a list of variables to save.")])]),e._v(" "),a("h4",{attrs:{id:"eager-compatibility"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eager-compatibility","aria-hidden":"true"}},[e._v("#")]),e._v(" Eager Compatibility")]),e._v(" "),a("p",[e._v("When eager execution is enabled, var_list must specify a list or dict of variables to save. Otherwise, a RuntimeError will be raised.\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint",target:"_blank",rel:"noopener noreferrer"}},[e._v("tf.train.Checkpoint"),a("OutboundLink")],1),e._v("Although Saver works in some cases when executing eagerly, it is fragile. Please switch to  or tf.keras.Model.save_weights, which perform a more robust object-based saving. These APIs will load checkpoints written by Saver.")]),e._v(" "),a("h2",{attrs:{id:"properties"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#properties","aria-hidden":"true"}},[e._v("#")]),e._v(" Properties")]),e._v(" "),a("h3",{attrs:{id:"last-checkpoints"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#last-checkpoints","aria-hidden":"true"}},[e._v("#")]),e._v(" last_checkpoints")]),e._v(" "),a("p",[e._v("List of not-yet-deleted checkpoint filenames.")]),e._v(" "),a("p",[e._v("You can pass any of the returned values to restore().")]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A list of checkpoint filenames, sorted from oldest to newest.")]),e._v(" "),a("h2",{attrs:{id:"methods"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#methods","aria-hidden":"true"}},[e._v("#")]),e._v(" Methods")]),e._v(" "),a("h3",{attrs:{id:"as-saver-def"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#as-saver-def","aria-hidden":"true"}},[e._v("#")]),e._v(" as_saver_def")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L967-L973",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" as_saver_def()\n")])])]),a("p",[e._v("Generates a SaverDef representation of this saver.")]),e._v(" "),a("h4",{attrs:{id:"returns-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A SaverDef proto.")]),e._v(" "),a("h3",{attrs:{id:"build"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build","aria-hidden":"true"}},[e._v("#")]),e._v(" build")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L837-L840",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" build()\n")])])]),a("h3",{attrs:{id:"export-meta-graph"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#export-meta-graph","aria-hidden":"true"}},[e._v("#")]),e._v(" export_meta_graph")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1210-L1254",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" export_meta_graph(\n    filename=None,\n    collection_list=None,\n    as_text=False,\n    export_scope=None,\n    clear_devices=False,\n    clear_extraneous_savers=False,\n    strip_default_attrs=False,\n    save_debug_info=False\n)\n")])])]),a("p",[e._v("Writes MetaGraphDef to save_path/filename.")]),e._v(" "),a("h4",{attrs:{id:"args-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("filename")]),e._v(": Optional meta_graph "),a("code",[e._v("filename")]),e._v(" including the path.")]),e._v(" "),a("li",[a("code",[e._v("collection_list")]),e._v(": List of string keys to collect.")]),e._v(" "),a("li",[a("code",[e._v("as_text")]),e._v(": If "),a("code",[e._v("True")]),e._v(", writes the meta_graph as an ASCII proto.")]),e._v(" "),a("li",[a("code",[e._v("export_scope")]),e._v(": Optional "),a("code",[e._v("string")]),e._v(". Name scope to remove.")]),e._v(" "),a("li",[a("code",[e._v("clear_devices")]),e._v(": Whether or not to clear the device field for an "),a("code",[e._v("Operation")]),e._v(" or "),a("code",[e._v("Tensor")]),e._v(" during export.")]),e._v(" "),a("li",[a("code",[e._v("clear_extraneous_savers")]),e._v(": Remove any Saver-related information from the graph (both Save/Restore ops and SaverDefs) that are not associated with this Saver.")]),e._v(" "),a("li",[a("code",[e._v("strip_default_attrs")]),e._v(": Boolean. If "),a("code",[e._v("True")]),e._v(", default-valued attributes will be removed from the NodeDefs. For a detailed guide, see Stripping Default-Valued Attributes.")]),e._v(" "),a("li",[a("code",[e._v("save_debug_info")]),e._v(": If "),a("code",[e._v("True")]),e._v(", save the GraphDebugInfo to a separate file, which in the same directory of "),a("code",[e._v("filename")]),e._v(" and with "),a("code",[e._v("_debug")]),e._v(" added before the file extension.")])]),e._v(" "),a("h4",{attrs:{id:"returns-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A MetaGraphDef proto.")]),e._v(" "),a("h3",{attrs:{id:"from-proto"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-proto","aria-hidden":"true"}},[e._v("#")]),e._v(" from_proto")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1002-L1013",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" @staticmethod\nfrom_proto(\n    saver_def,\n    import_scope=None\n)\n")])])]),a("p",[e._v("Returns a Saver object created from saver_def.")]),e._v(" "),a("h4",{attrs:{id:"args-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("saver_def")]),e._v(": a "),a("code",[e._v("SaverDef")]),e._v(" protocol buffer.")]),e._v(" "),a("li",[a("code",[e._v("import_scope")]),e._v(": Optional "),a("code",[e._v("string")]),e._v(". Name scope to use.")])]),e._v(" "),a("h4",{attrs:{id:"returns-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A Saver built from saver_def.")]),e._v(" "),a("h3",{attrs:{id:"recover-last-checkpoints"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#recover-last-checkpoints","aria-hidden":"true"}},[e._v("#")]),e._v(" recover_last_checkpoints")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1056-L1072",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" recover_last_checkpoints(checkpoint_paths)\n")])])]),a("p",[e._v("Recovers the internal saver state after a crash.")]),e._v(" "),a("p",[e._v('This method is useful for recovering the "self._last_checkpoints" state.')]),e._v(" "),a("p",[e._v("Globs for the checkpoints pointed to by checkpoint_paths. If the files exist, use their mtime as the checkpoint timestamp.")]),e._v(" "),a("h4",{attrs:{id:"args-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("checkpoint_paths")]),e._v(": a list of checkpoint paths.")])]),e._v(" "),a("h3",{attrs:{id:"restore"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#restore","aria-hidden":"true"}},[e._v("#")]),e._v(" restore")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1256-L1326",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" restore(\n    sess,\n    save_path\n)\n")])])]),a("p",[e._v("Restores previously saved variables.")]),e._v(" "),a("p",[e._v("This method runs the ops added by the constructor for restoring variables. It requires a session in which the graph was launched. The variables to restore do not have to have been initialized, as restoring is itself a way to initialize variables.")]),e._v(" "),a("p",[e._v("The save_path argument is typically a value previously returned from a save() call, or a call to latest_checkpoint().")]),e._v(" "),a("h4",{attrs:{id:"args-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("sess")]),e._v(": A "),a("code",[e._v("Session")]),e._v(" to use to restore the parameters. None in eager mode.")]),e._v(" "),a("li",[a("code",[e._v("save_path")]),e._v(": Path where parameters were previously saved.")])]),e._v(" "),a("h4",{attrs:{id:"raises-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": If save_path is None or not a valid checkpoint.")])]),e._v(" "),a("h3",{attrs:{id:"save"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#save","aria-hidden":"true"}},[e._v("#")]),e._v(" save")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1074-L1208",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" save(\n    sess,\n    save_path,\n    global_step=None,\n    latest_filename=None,\n    meta_graph_suffix='meta',\n    write_meta_graph=True,\n    write_state=True,\n    strip_default_attrs=False,\n    save_debug_info=False\n)\n")])])]),a("p",[e._v("Saves variables.")]),e._v(" "),a("p",[e._v("This method runs the ops added by the constructor for saving variables. It requires a session in which the graph was launched. The variables to save must also have been initialized.")]),e._v(" "),a("p",[e._v("The method returns the path prefix of the newly created checkpoint files. This string can be passed directly to a call to restore().")]),e._v(" "),a("h4",{attrs:{id:"args-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-6","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("sess")]),e._v(": A Session to use to save the variables.")]),e._v(" "),a("li",[a("code",[e._v("save_path")]),e._v(": String. Prefix of filenames created for the checkpoint.")]),e._v(" "),a("li",[a("code",[e._v("global_step")]),e._v(": If provided the global step number is appended to "),a("code",[e._v("save_path")]),e._v(" to create the checkpoint filenames. The optional argument can be a "),a("code",[e._v("Tensor")]),e._v(", a "),a("code",[e._v("Tensor")]),e._v(" name or an integer.")]),e._v(" "),a("li",[a("code",[e._v("latest_filename")]),e._v(": Optional name for the protocol buffer file that will contains the list of most recent checkpoints. That file, kept in the same directory as the checkpoint files, is automatically managed by the saver to keep track of recent checkpoints. Defaults to 'checkpoint'.")]),e._v(" "),a("li",[a("code",[e._v("meta_graph_suffix")]),e._v(": Suffix for "),a("code",[e._v("MetaGraphDef")]),e._v(" file. Defaults to 'meta'.")]),e._v(" "),a("li",[a("code",[e._v("write_meta_graph")]),e._v(": "),a("code",[e._v("Boolean")]),e._v(" indicating whether or not to write the meta graph file.")]),e._v(" "),a("li",[a("code",[e._v("write_state")]),e._v(": "),a("code",[e._v("Boolean")]),e._v(" indicating whether or not to write the "),a("code",[e._v("CheckpointStateProto")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("strip_default_attrs")]),e._v(": "),a("code",[e._v("Boolean")]),e._v(". If "),a("code",[e._v("True")]),e._v(", default-valued attributes will be removed from the NodeDefs. For a detailed guide, see Stripping Default-Valued Attributes.")]),e._v(" "),a("li",[a("code",[e._v("save_debug_info")]),e._v(": If "),a("code",[e._v("True")]),e._v(", save the GraphDebugInfo to a separate file, which in the same directory of "),a("code",[e._v("save_path")]),e._v(" and with "),a("code",[e._v("_debug")]),e._v(" added before the file extension. This is only enabled when "),a("code",[e._v("write_meta_graph")]),e._v(" is "),a("code",[e._v("True")])])]),e._v(" "),a("h4",{attrs:{id:"returns-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A string: path prefix used for the checkpoint files. If the saver is sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn' is the number of shards created. If the saver is empty, returns None.")]),e._v(" "),a("h4",{attrs:{id:"raises-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("TypeError")]),e._v(": If "),a("code",[e._v("sess")]),e._v(" is not a "),a("code",[e._v("Session")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": If "),a("code",[e._v("latest_filename")]),e._v(" contains path components, or if it collides with "),a("code",[e._v("save_path")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("RuntimeError")]),e._v(": If save and restore ops weren't built.")])]),e._v(" "),a("h3",{attrs:{id:"set-last-checkpoints"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-last-checkpoints","aria-hidden":"true"}},[e._v("#")]),e._v(" set_last_checkpoints")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1026-L1041",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" set_last_checkpoints(last_checkpoints)\n")])])]),a("p",[e._v("DEPRECATED: Use set_last_checkpoints_with_time.")]),e._v(" "),a("p",[e._v("Sets the list of old checkpoint filenames.")]),e._v(" "),a("h4",{attrs:{id:"args-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-7","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("last_checkpoints")]),e._v(": A list of checkpoint filenames.")])]),e._v(" "),a("h4",{attrs:{id:"raises-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("AssertionError")]),e._v(": If last_checkpoints is not a list.")])]),e._v(" "),a("h3",{attrs:{id:"set-last-checkpoints-with-time"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-last-checkpoints-with-time","aria-hidden":"true"}},[e._v("#")]),e._v(" set_last_checkpoints_with_time")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L1043-L1054",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" set_last_checkpoints_with_time(last_checkpoints_with_time)\n")])])]),a("p",[e._v("Sets the list of old checkpoint filenames and timestamps.")]),e._v(" "),a("h4",{attrs:{id:"args-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-8","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("last_checkpoints_with_time")]),e._v(": A list of tuples of checkpoint filenames and timestamps.")])]),e._v(" "),a("h4",{attrs:{id:"raises-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("AssertionError")]),e._v(": If last_checkpoints_with_time is not a list.")])]),e._v(" "),a("h3",{attrs:{id:"to-proto"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-proto","aria-hidden":"true"}},[e._v("#")]),e._v(" to_proto")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/saver.py#L975-L1000",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" to_proto(export_scope=None)\n")])])]),a("p",[e._v("Converts this Saver to a SaverDef protocol buffer.")]),e._v(" "),a("h4",{attrs:{id:"args-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-9","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("export_scope")]),e._v(": Optional "),a("code",[e._v("string")]),e._v(". Name scope to remove.")])]),e._v(" "),a("h4",{attrs:{id:"returns-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-6","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A SaverDef protocol buffer.")])])}),[],!1,null,null,null);t.default=s.exports}}]);