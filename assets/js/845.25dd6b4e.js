(window.webpackJsonp=window.webpackJsonp||[]).push([[845],{1033:function(e,t,a){"use strict";a.r(t);var i=a(0),r=Object(i.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-adamparameters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-adamparameters","aria-hidden":"true"}},[e._v("#")]),e._v(" Class AdamParameters")]),e._v(" "),a("p",[e._v("Optimization parameters for Adam with TPU embeddings.")]),e._v(" "),a("p",[e._v("Pass this to tf.estimator.tpu.experimental.EmbeddingConfigSpec via the optimization_parameters argument to set the optimizer and its parameters. See the documentation for tf.estimator.tpu.experimental.EmbeddingConfigSpec for more details.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" estimator = tf.estimator.tpu.TPUEstimator(\n    ...\n    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(\n        ...\n        optimization_parameters=tf.tpu.experimental.AdamParameters(0.1),\n        ...))\n")])])]),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("init")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tpu/tpu_embedding.py#L326-L373",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __init__(\n    learning_rate,\n    beta1=0.9,\n    beta2=0.999,\n    epsilon=1e-08,\n    lazy_adam=True,\n    sum_inside_sqrt=True,\n    use_gradient_accumulation=True,\n    clip_weight_min=None,\n    clip_weight_max=None\n)\n")])])]),a("p",[e._v("Optimization parameters for Adam.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("learning_rate")]),e._v(": a floating point value. The learning rate.")]),e._v(" "),a("li",[a("code",[e._v("beta1")]),e._v(": A float value. The exponential decay rate for the 1st moment estimates.")]),e._v(" "),a("li",[a("code",[e._v("beta2")]),e._v(": A float value. The exponential decay rate for the 2nd moment estimates.")]),e._v(" "),a("li",[a("code",[e._v("epsilon")]),e._v(": A small constant for numerical stability.")]),e._v(" "),a("li",[a("code",[e._v("lazy_adam")]),e._v(": Use lazy Adam instead of Adam. Lazy Adam trains faster. Please see "),a("code",[e._v("optimization_parameters.proto")]),e._v(" for details.")]),e._v(" "),a("li",[a("code",[e._v("sum_inside_sqrt")]),e._v(": This improves training speed. Please see "),a("code",[e._v("optimization_parameters.proto")]),e._v(" for details.")]),e._v(" "),a("li",[a("code",[e._v("use_gradient_accumulation")]),e._v(": setting this to "),a("code",[e._v("False")]),e._v(" makes embedding gradients calculation less accurate but faster. Please see "),a("code",[e._v("optimization_parameters.proto")]),e._v(" for details. for details.")]),e._v(" "),a("li",[a("code",[e._v("clip_weight_min")]),e._v(": the minimum value to clip by; None means -infinity.")]),e._v(" "),a("li",[a("code",[e._v("clip_weight_max")]),e._v(": the maximum value to clip by; None means +infinity.")])])])}),[],!1,null,null,null);t.default=r.exports}}]);