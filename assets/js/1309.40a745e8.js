(window.webpackJsonp=window.webpackJsonp||[]).push([[1309],{1498:function(e,t,a){"use strict";a.r(t);var s=a(0),r=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("Represents sparse feature where ids are set by hashing.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("tf.compat.v1.feature_column.categorical_column_with_hash_bucket")])]),e._v(" "),a("li",[a("code",[e._v("tf.compat.v2.feature_column.categorical_column_with_hash_bucket")])])]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" tf.feature_column.categorical_column_with_hash_bucket(\n    key,\n    hash_bucket_size,\n    dtype=tf.dtypes.string\n)\n")])])]),a("h3",{attrs:{id:"used-in-the-guide"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-guide","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the guide:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("t")]),e._v("f"),a("code",[e._v(".")]),e._v("d"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("a")]),e._v(":"),a("code"),e._v("B"),a("code",[e._v("u")]),e._v("i"),a("code",[e._v("l")]),e._v("d"),a("code"),e._v("T"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("s")]),e._v("o"),a("code",[e._v("r")]),e._v("F"),a("code",[e._v("l")]),e._v("o"),a("code",[e._v("w")]),e._v(" "),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("p")]),e._v("u"),a("code",[e._v("t")]),e._v(" "),a("code",[e._v("p")]),e._v("i"),a("code",[e._v("p")]),e._v("e"),a("code",[e._v("l")]),e._v("i"),a("code",[e._v("n")]),e._v("e"),a("code",[e._v("s")])])]),e._v(" "),a("h3",{attrs:{id:"used-in-the-tutorials"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-tutorials","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the tutorials:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("C")]),e._v("l"),a("code",[e._v("a")]),e._v("s"),a("code",[e._v("s")]),e._v("i"),a("code",[e._v("f")]),e._v("y"),a("code"),e._v("s"),a("code",[e._v("t")]),e._v("r"),a("code",[e._v("u")]),e._v("c"),a("code",[e._v("t")]),e._v("u"),a("code",[e._v("r")]),e._v("e"),a("code",[e._v("d")]),e._v(" "),a("code",[e._v("d")]),e._v("a"),a("code",[e._v("t")]),e._v("a"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("f")]),e._v("e"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("u")]),e._v("r"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("c")]),e._v("o"),a("code",[e._v("l")]),e._v("u"),a("code",[e._v("m")]),e._v("n"),a("code",[e._v("s")])])]),e._v(" "),a("p",[e._v("Use this when your sparse features are in string or integer format, and you want to distribute your inputs into a finite number of buckets by hashing. output_id = Hash(input_feature_string) % bucket_size for string type input. For int type input, the value is converted to its string representation first and then hashed by the same formula.")]),e._v(" "),a("p",[e._v("For input dictionary features, features[key] is either Tensor or SparseTensor. If Tensor, missing values can be represented by -1 for int and '' for string, which will be dropped by this feature column.")]),e._v(" "),a("h4",{attrs:{id:"example"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#example","aria-hidden":"true"}},[e._v("#")]),e._v(" Example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(' keywords = categorical_column_with_hash_bucket("keywords", 10K)\ncolumns = [keywords, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\nlinear_prediction = linear_model(features, columns)\n\n# or\nkeywords_embedded = embedding_column(keywords, 16)\ncolumns = [keywords_embedded, ...]\nfeatures = tf.io.parse_example(..., features=make_parse_example_spec(columns))\ndense_tensor = input_layer(features, columns)\n')])])]),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("key")]),e._v(": A unique string identifying the input feature. It is used as the column name and the dictionary "),a("code",[e._v("key")]),e._v(" for feature parsing configs, feature "),a("code",[e._v("Tensor")]),e._v(" objects, and feature columns.")]),e._v(" "),a("li",[a("code",[e._v("hash_bucket_size")]),e._v(": An int > 1. The number of buckets.")]),e._v(" "),a("li",[a("code",[e._v("dtype")]),e._v(": The type of features. Only string and integer types are supported.")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A HashedCategoricalColumn.")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": "),a("code",[e._v("hash_bucket_size")]),e._v(" is not greater than 1.")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": "),a("code",[e._v("dtype")]),e._v(" is neither string nor integer.")])])])}),[],!1,null,null,null);t.default=r.exports}}]);