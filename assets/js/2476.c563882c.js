(window.webpackJsonp=window.webpackJsonp||[]).push([[2476],{2667:function(e,t,a){"use strict";a.r(t);var o=a(0),_=Object(o.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-gradienttape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-gradienttape","aria-hidden":"true"}},[e._v("#")]),e._v(" Class GradientTape")]),e._v(" "),a("p",[e._v("Record operations for automatic differentiation.")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[e._v("Class "),a("code",[e._v("tf.compat.v1.GradientTape")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.compat.v2.GradientTape")])])]),e._v(" "),a("h3",{attrs:{id:"used-in-the-guide"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-guide","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the guide:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("B")]),e._v("e"),a("code",[e._v("t")]),e._v("t"),a("code",[e._v("e")]),e._v("r"),a("code"),e._v("p"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("f")]),e._v("o"),a("code",[e._v("r")]),e._v("m"),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("c")]),e._v("e"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("t")]),e._v("f"),a("code",[e._v(".")]),e._v("f"),a("code",[e._v("u")]),e._v("n"),a("code",[e._v("c")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v(" "),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("d")]),e._v(" "),a("code",[e._v("A")]),e._v("u"),a("code",[e._v("t")]),e._v("o"),a("code",[e._v("G")]),e._v("r"),a("code",[e._v("a")]),e._v("p"),a("code",[e._v("h")])]),e._v(" "),a("li",[a("code",[e._v("D")]),e._v("i"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("r")]),e._v("i"),a("code",[e._v("b")]),e._v("u"),a("code",[e._v("t")]),e._v("e"),a("code",[e._v("d")]),e._v(" "),a("code",[e._v("t")]),e._v("r"),a("code",[e._v("a")]),e._v("i"),a("code",[e._v("n")]),e._v("i"),a("code",[e._v("n")]),e._v("g"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("T")]),e._v("e"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("F")]),e._v("l"),a("code",[e._v("o")]),e._v("w``")]),e._v(" "),a("li",[a("code",[e._v("E")]),e._v("a"),a("code",[e._v("g")]),e._v("e"),a("code",[e._v("r")]),e._v(" "),a("code",[e._v("e")]),e._v("x"),a("code",[e._v("e")]),e._v("c"),a("code",[e._v("u")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")])]),e._v(" "),a("li",[a("code",[e._v("M")]),e._v("i"),a("code",[e._v("g")]),e._v("r"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("y")]),e._v("o"),a("code",[e._v("u")]),e._v("r"),a("code"),e._v("T"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("s")]),e._v("o"),a("code",[e._v("r")]),e._v("F"),a("code",[e._v("l")]),e._v("o"),a("code",[e._v("w")]),e._v(" "),a("code",[e._v("1")]),e._v(" "),a("code",[e._v("c")]),e._v("o"),a("code",[e._v("d")]),e._v("e"),a("code"),e._v("t"),a("code",[e._v("o")]),e._v(" "),a("code",[e._v("T")]),e._v("e"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("F")]),e._v("l"),a("code",[e._v("o")]),e._v("w"),a("code"),e._v("2``")]),e._v(" "),a("li",[a("code",[e._v("T")]),e._v("r"),a("code",[e._v("a")]),e._v("i"),a("code",[e._v("n")]),e._v(" "),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("d")]),e._v(" "),a("code",[e._v("e")]),e._v("v"),a("code",[e._v("a")]),e._v("l"),a("code",[e._v("u")]),e._v("a"),a("code",[e._v("t")]),e._v("e"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("K")]),e._v("e"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("s")])]),e._v(" "),a("li",[a("code",[e._v("T")]),e._v("r"),a("code",[e._v("a")]),e._v("i"),a("code",[e._v("n")]),e._v("i"),a("code",[e._v("n")]),e._v("g"),a("code"),e._v("c"),a("code",[e._v("h")]),e._v("e"),a("code",[e._v("c")]),e._v("k"),a("code",[e._v("p")]),e._v("o"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("t")]),e._v("s``")]),e._v(" "),a("li",[a("code",[e._v("U")]),e._v("s"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(" "),a("code",[e._v("t")]),e._v("h"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("S")]),e._v("a"),a("code",[e._v("v")]),e._v("e"),a("code",[e._v("d")]),e._v("M"),a("code",[e._v("o")]),e._v("d"),a("code",[e._v("e")]),e._v("l"),a("code"),e._v("f"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("m")]),e._v("a"),a("code",[e._v("t")])]),e._v(" "),a("li",[a("code",[e._v("W")]),e._v("r"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(" "),a("code",[e._v("c")]),e._v("u"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("o")]),e._v("m"),a("code"),e._v("l"),a("code",[e._v("a")]),e._v("y"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("s")]),e._v(" "),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("d")]),e._v(" "),a("code",[e._v("m")]),e._v("o"),a("code",[e._v("d")]),e._v("e"),a("code",[e._v("l")]),e._v("s"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("K")]),e._v("e"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("s")])])]),e._v(" "),a("h3",{attrs:{id:"used-in-the-tutorials"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#used-in-the-tutorials","aria-hidden":"true"}},[e._v("#")]),e._v(" Used in the tutorials:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("A")]),e._v("d"),a("code",[e._v("v")]),e._v("e"),a("code",[e._v("r")]),e._v("s"),a("code",[e._v("a")]),e._v("r"),a("code",[e._v("i")]),e._v("a"),a("code",[e._v("l")]),e._v(" "),a("code",[e._v("e")]),e._v("x"),a("code",[e._v("a")]),e._v("m"),a("code",[e._v("p")]),e._v("l"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("u")]),e._v("s"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(" "),a("code",[e._v("F")]),e._v("G"),a("code",[e._v("S")]),e._v("M``")]),e._v(" "),a("li",[a("code",[e._v("A")]),e._v("u"),a("code",[e._v("t")]),e._v("o"),a("code",[e._v("m")]),e._v("a"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("c")]),e._v(" "),a("code",[e._v("d")]),e._v("i"),a("code",[e._v("f")]),e._v("f"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v(" "),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("d")]),e._v(" "),a("code",[e._v("g")]),e._v("r"),a("code",[e._v("a")]),e._v("d"),a("code",[e._v("i")]),e._v("e"),a("code",[e._v("n")]),e._v("t"),a("code"),e._v("t"),a("code",[e._v("a")]),e._v("p"),a("code",[e._v("e")])]),e._v(" "),a("li",[a("code",[e._v("B")]),e._v("e"),a("code",[e._v("t")]),e._v("t"),a("code",[e._v("e")]),e._v("r"),a("code"),e._v("p"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("f")]),e._v("o"),a("code",[e._v("r")]),e._v("m"),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("c")]),e._v("e"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("t")]),e._v("f"),a("code",[e._v(".")]),e._v("f"),a("code",[e._v("u")]),e._v("n"),a("code",[e._v("c")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")])]),e._v(" "),a("li",[a("code",[e._v("C")]),e._v("o"),a("code",[e._v("n")]),e._v("v"),a("code",[e._v("o")]),e._v("l"),a("code",[e._v("u")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v("a"),a("code",[e._v("l")]),e._v(" "),a("code",[e._v("V")]),e._v("a"),a("code",[e._v("r")]),e._v("i"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v("a"),a("code",[e._v("l")]),e._v(" "),a("code",[e._v("A")]),e._v("u"),a("code",[e._v("t")]),e._v("o"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("c")]),e._v("o"),a("code",[e._v("d")]),e._v("e"),a("code",[e._v("r")])]),e._v(" "),a("li",[a("code",[e._v("C")]),e._v("u"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("o")]),e._v("m"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(" "),a("code",[e._v("w")]),e._v("i"),a("code",[e._v("t")]),e._v("h"),a("code"),e._v("t"),a("code",[e._v("f")]),e._v("."),a("code",[e._v("d")]),e._v("i"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("r")]),e._v("i"),a("code",[e._v("b")]),e._v("u"),a("code",[e._v("t")]),e._v("e"),a("code",[e._v(".")]),e._v("S"),a("code",[e._v("t")]),e._v("r"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("e")]),e._v("g"),a("code",[e._v("y")])]),e._v(" "),a("li",[a("code",[e._v("C")]),e._v("u"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("o")]),e._v("m"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(":"),a("code"),e._v("b"),a("code",[e._v("a")]),e._v("s"),a("code",[e._v("i")]),e._v("c"),a("code",[e._v("s")])]),e._v(" "),a("li",[a("code",[e._v("C")]),e._v("u"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("o")]),e._v("m"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("i")]),e._v("n"),a("code",[e._v("g")]),e._v(":"),a("code"),e._v("w"),a("code",[e._v("a")]),e._v("l"),a("code",[e._v("k")]),e._v("t"),a("code",[e._v("h")]),e._v("r"),a("code",[e._v("o")]),e._v("u"),a("code",[e._v("g")]),e._v("h``")]),e._v(" "),a("li",[a("code",[e._v("C")]),e._v("y"),a("code",[e._v("c")]),e._v("l"),a("code",[e._v("e")]),e._v("G"),a("code",[e._v("A")]),e._v("N``")]),e._v(" "),a("li",[a("code",[e._v("D")]),e._v("e"),a("code",[e._v("e")]),e._v("p"),a("code"),e._v("C"),a("code",[e._v("o")]),e._v("n"),a("code",[e._v("v")]),e._v("o"),a("code",[e._v("l")]),e._v("u"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("o")]),e._v("n"),a("code",[e._v("a")]),e._v("l"),a("code"),e._v("G"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("i")]),e._v("v"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("A")]),e._v("d"),a("code",[e._v("v")]),e._v("e"),a("code",[e._v("r")]),e._v("s"),a("code",[e._v("a")]),e._v("r"),a("code",[e._v("i")]),e._v("a"),a("code",[e._v("l")]),e._v(" "),a("code",[e._v("N")]),e._v("e"),a("code",[e._v("t")]),e._v("w"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("k")])]),e._v(" "),a("li",[a("code",[e._v("D")]),e._v("e"),a("code",[e._v("e")]),e._v("p"),a("code",[e._v("D")]),e._v("r"),a("code",[e._v("e")]),e._v("a"),a("code",[e._v("m")])]),e._v(" "),a("li",[a("code",[e._v("I")]),e._v("m"),a("code",[e._v("a")]),e._v("g"),a("code",[e._v("e")]),e._v(" "),a("code",[e._v("c")]),e._v("a"),a("code",[e._v("p")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v("i"),a("code",[e._v("n")]),e._v("g"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("v")]),e._v("i"),a("code",[e._v("s")]),e._v("u"),a("code",[e._v("a")]),e._v("l"),a("code"),e._v("a"),a("code",[e._v("t")]),e._v("t"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("o")]),e._v("n``")]),e._v(" "),a("li",[a("code",[e._v("N")]),e._v("e"),a("code",[e._v("u")]),e._v("r"),a("code",[e._v("a")]),e._v("l"),a("code"),e._v("m"),a("code",[e._v("a")]),e._v("c"),a("code",[e._v("h")]),e._v("i"),a("code",[e._v("n")]),e._v("e"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("l")]),e._v("a"),a("code",[e._v("t")]),e._v("i"),a("code",[e._v("o")]),e._v("n"),a("code"),e._v("w"),a("code",[e._v("i")]),e._v("t"),a("code",[e._v("h")]),e._v(" "),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("t")]),e._v("e"),a("code",[e._v("n")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")])]),e._v(" "),a("li",[a("code",[e._v("N")]),e._v("e"),a("code",[e._v("u")]),e._v("r"),a("code",[e._v("a")]),e._v("l"),a("code"),e._v("s"),a("code",[e._v("t")]),e._v("y"),a("code",[e._v("l")]),e._v("e"),a("code"),e._v("t"),a("code",[e._v("r")]),e._v("a"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("f")]),e._v("e"),a("code",[e._v("r")])]),e._v(" "),a("li",[a("code",[e._v("P")]),e._v("i"),a("code",[e._v("x")]),e._v("2"),a("code",[e._v("P")]),e._v("i"),a("code",[e._v("x")])]),e._v(" "),a("li",[a("code",[e._v("T")]),e._v("e"),a("code",[e._v("n")]),e._v("s"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("F")]),e._v("l"),a("code",[e._v("o")]),e._v("w"),a("code"),e._v("2"),a("code"),e._v("q"),a("code",[e._v("u")]),e._v("i"),a("code",[e._v("c")]),e._v("k"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("a")]),e._v("r"),a("code",[e._v("t")]),e._v(" "),a("code",[e._v("f")]),e._v("o"),a("code",[e._v("r")]),e._v(" "),a("code",[e._v("e")]),e._v("x"),a("code",[e._v("p")]),e._v("e"),a("code",[e._v("r")]),e._v("t"),a("code",[e._v("s")])]),e._v(" "),a("li",[a("code",[e._v("T")]),e._v("e"),a("code",[e._v("x")]),e._v("t"),a("code"),e._v("g"),a("code",[e._v("e")]),e._v("n"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("a")]),e._v("t"),a("code",[e._v("i")]),e._v("o"),a("code",[e._v("n")]),e._v(" "),a("code",[e._v("w")]),e._v("i"),a("code",[e._v("t")]),e._v("h"),a("code"),e._v("a"),a("code",[e._v("n")]),e._v(" "),a("code",[e._v("R")]),e._v("N"),a("code",[e._v("N")])]),e._v(" "),a("li",[a("code",[e._v("T")]),e._v("r"),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("s")]),e._v("f"),a("code",[e._v("o")]),e._v("r"),a("code",[e._v("m")]),e._v("e"),a("code",[e._v("r")]),e._v(" "),a("code",[e._v("m")]),e._v("o"),a("code",[e._v("d")]),e._v("e"),a("code",[e._v("l")]),e._v(" "),a("code",[e._v("f")]),e._v("o"),a("code",[e._v("r")]),e._v(" "),a("code",[e._v("l")]),e._v("a"),a("code",[e._v("n")]),e._v("g"),a("code",[e._v("u")]),e._v("a"),a("code",[e._v("g")]),e._v("e"),a("code"),e._v("u"),a("code",[e._v("n")]),e._v("d"),a("code",[e._v("e")]),e._v("r"),a("code",[e._v("s")]),e._v("t"),a("code",[e._v("a")]),e._v("n"),a("code",[e._v("d")]),e._v("i"),a("code",[e._v("n")]),e._v('g``\nOperations are recorded if they are executed within this context manager and at least one of their inputs is being "watched".\n'),a("a",{attrs:{href:"https://tensorflow.google.cn/api_docs/python/tf/Variable",target:"_blank",rel:"noopener noreferrer"}},[e._v("tf.Variable"),a("OutboundLink")],1),e._v("Trainable variables (created by  or tf.compat.v1.get_variable, where trainable=True is default in both cases) are automatically watched. Tensors can be manually watched by invoking the watch method on this context manager.")])]),e._v(" "),a("p",[e._v("For example, consider the function y = x * x. The gradient at "),a("code",[e._v("x = 3.0")]),e._v(" can be computed as:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" x = tf.constant(3.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ndy_dx = g.gradient(y, x) # Will compute to 6.0\n")])])]),a("p",[e._v("GradientTapes can be nested to compute higher-order derivatives. For example,")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" x = tf.constant(3.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  with tf.GradientTape() as gg:\n    gg.watch(x)\n    y = x * x\n  dy_dx = gg.gradient(y, x)     # Will compute to 6.0\nd2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n")])])]),a("p",[e._v("By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" x = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as g:\n  g.watch(x)\n  y = x * x\n  z = y * y\ndz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\ndy_dx = g.gradient(y, x)  # 6.0\ndel g  # Drop the reference to the tape\n")])])]),a("p",[e._v("By default GradientTape will automatically watch any trainable variables that are accessed inside the context. If you want fine grained control over which variables are watched you can disable automatic tracking by passing "),a("code",[e._v("watch_accessed_variables=False")]),e._v(" to the tape constructor:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" with tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(variable_a)\n  y = variable_a ** 2  # Gradients will be available for `variable_a`.\n  z = variable_b ** 3  # No gradients will be available since `variable_b` is\n                       # not being watched.\n")])])]),a("p",[e._v("Note that when using models you should ensure that your variables exist when using "),a("code",[e._v("watch_accessed_variables=False")]),e._v(". Otherwise it's quite easy to make your first iteration not have any gradients:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" a = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\n")])])]),a("p",[e._v("Note that only tensors with real or complex dtypes are differentiable.")]),e._v(" "),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("init")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L778-L799",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __init__(\n    persistent=False,\n    watch_accessed_variables=True\n)\n")])])]),a("p",[e._v("Creates a new GradientTape.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("persistent")]),e._v(": Boolean controlling whether a "),a("code",[e._v("persistent")]),e._v(" gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object.")]),e._v(" "),a("li",[a("code",[e._v("watch_accessed_variables")]),e._v(": Boolean controlling whether the tape will automatically "),a("code",[e._v("watch")]),e._v(" any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable "),a("code",[e._v("Variable")]),e._v(". If False users must explicitly "),a("code",[e._v("watch")]),e._v(" any "),a("code",[e._v("Variable")]),e._v("s they want to request gradients from.")])]),e._v(" "),a("h2",{attrs:{id:"methods"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#methods","aria-hidden":"true"}},[e._v("#")]),e._v(" Methods")]),e._v(" "),a("h3",{attrs:{id:"enter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#enter","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("enter")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L801-L804",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __enter__()\n")])])]),a("p",[e._v("Enters a context inside which operations are recorded on this tape.")]),e._v(" "),a("h3",{attrs:{id:"exit"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exit","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("exit")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L806-L809",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __exit__(\n    typ,\n    value,\n    traceback\n)\n")])])]),a("p",[e._v("Exits the recording context, no further operations are traced.")]),e._v(" "),a("h3",{attrs:{id:"batch-jacobian"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#batch-jacobian","aria-hidden":"true"}},[e._v("#")]),e._v(" batch_jacobian")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L1126-L1245",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" batch_jacobian(\n    target,\n    source,\n    unconnected_gradients=tf.UnconnectedGradients.NONE,\n    parallel_iterations=None,\n    experimental_use_pfor=True\n)\n")])])]),a("p",[e._v("Computes and stacks per-example jacobians.\n"),a("a",{attrs:{href:"http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant",target:"_blank",rel:"noopener noreferrer"}},[e._v("wikipedia article"),a("OutboundLink")],1),e._v("See  for the definition of a Jacobian. This function is essentially an efficient implementation of the following:")]),e._v(" "),a("p",[e._v("tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])]).\n"),a("a",{attrs:{href:"https://tensorflow.google.cn/api_docs/python/tf/GradientTape#jacobian",target:"_blank",rel:"noopener noreferrer"}},[e._v("GradientTape.jacobian"),a("OutboundLink")],1),e._v("Note that compared to  which computes gradient of each output value w.r.t each input value, this function is useful when target[i,...] is independent of source[j,...] for j != i. This assumption allows more efficient computation as compared to . The output, as well as intermediate activations, are lower dimensional and avoid a bunch of redundant zeros which would result in the jacobian computation given the independence assumption.")]),e._v(" "),a("h4",{attrs:{id:"example-usage"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#example-usage","aria-hidden":"true"}},[e._v("#")]),e._v(" Example usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" with tf.GradientTape() as g:\n  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n  g.watch(x)\n  y = x * x\nbatch_jacobian = g.batch_jacobian(y, x)\n# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n")])])]),a("h4",{attrs:{id:"args-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("target")]),e._v(": A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. "),a("code",[e._v("target")]),e._v("[i,...] should only depend on "),a("code",[e._v("source[i,...]")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("source")]),e._v(": A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].")]),e._v(" "),a("li",[a("code",[e._v("unconnected_gradients")]),e._v(": a value which can either hold 'none' or 'zero' and alters the value which will be returned if the "),a("code",[e._v("target")]),e._v(" and "),a("code",[e._v("source")]),e._v("s are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.")]),e._v(" "),a("li",[a("code",[e._v("parallel_iterations")]),e._v(": A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage.")]),e._v(" "),a("li",[a("code",[e._v("experimental_use_pfor")]),e._v(": If true, uses pfor for computing the Jacobian. Else uses a tf.while_loop.")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A "),a("code",[e._v("t")]),e._v("ensor "),a("code",[e._v("t")]),e._v(" wi"),a("code",[e._v("t")]),e._v("h shape [b, y_1, ..., y_n, x1, ..., x_m] where "),a("code",[e._v("t")]),e._v("[i, ...] is "),a("code",[e._v("t")]),e._v("he jacobian of "),a("code",[e._v("t")]),e._v("arge"),a("code",[e._v("t")]),e._v("[i, ...] w.r."),a("code",[e._v("t")]),e._v(". "),a("code",[e._v("source[i, ...]")]),e._v(", i.e. s"),a("code",[e._v("t")]),e._v("acked per-example jacobians.")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("RuntimeError")]),e._v(": If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor.")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": If vectorization of jacobian computation fails or if first dimension of "),a("code",[e._v("target")]),e._v(" and "),a("code",[e._v("source")]),e._v(" do not match.")])]),e._v(" "),a("h3",{attrs:{id:"gradient"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#gradient","aria-hidden":"true"}},[e._v("#")]),e._v(" gradient")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L935-L1020",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" gradient(\n    target,\n    sources,\n    output_gradients=None,\n    unconnected_gradients=tf.UnconnectedGradients.NONE\n)\n")])])]),a("p",[e._v("Computes the gradient using operations recorded in context of this tape.")]),e._v(" "),a("h4",{attrs:{id:"args-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("target")]),e._v(": Tensor (or list of tensors) to be differentiated.")]),e._v(" "),a("li",[a("code",[e._v("sources")]),e._v(": a list or nested structure of Tensors or Variables. "),a("code",[e._v("target")]),e._v(" will be differentiated against elements in "),a("code",[e._v("sources")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("output_gradients")]),e._v(": a list of gradients, one for each element of "),a("code",[e._v("target")]),e._v(". Defaults to None.")]),e._v(" "),a("li",[a("code",[e._v("unconnected_gradients")]),e._v(": a value which can either hold 'none' or 'zero' and alters the value which will be returned if the "),a("code",[e._v("target")]),e._v(" and "),a("code",[e._v("sources")]),e._v(" are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.")])]),e._v(" "),a("h4",{attrs:{id:"returns-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("a list or nested structure of Tensors (or IndexedSlices, or None), one for each element in "),a("code",[e._v("sources")]),e._v(". Returned structure is the same as the structure of "),a("code",[e._v("sources")]),e._v(".")]),e._v(" "),a("h4",{attrs:{id:"raises-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("RuntimeError")]),e._v(": if called inside the context of the tape, or if called more than once on a non-persistent tape.")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": if the target is a variable or if unconnected gradients is called with an unknown value.")])]),e._v(" "),a("h3",{attrs:{id:"jacobian"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#jacobian","aria-hidden":"true"}},[e._v("#")]),e._v(" jacobian")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L1022-L1124",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" jacobian(\n    target,\n    sources,\n    unconnected_gradients=tf.UnconnectedGradients.NONE,\n    parallel_iterations=None,\n    experimental_use_pfor=True\n)\n")])])]),a("p",[e._v("Computes the jacobian using operations recorded in context of this tape.\n"),a("a",{attrs:{href:"http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant",target:"_blank",rel:"noopener noreferrer"}},[e._v("wikipedia article"),a("OutboundLink")],1),e._v("See  for the definition of a Jacobian.")]),e._v(" "),a("h4",{attrs:{id:"example-usage-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#example-usage-2","aria-hidden":"true"}},[e._v("#")]),e._v(" Example usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" with tf.GradientTape() as g:\n  x  = tf.constant([1.0, 2.0])\n  g.watch(x)\n  y = x * x\njacobian = g.jacobian(y, x)\n# jacobian value is [[2., 0.], [0., 4.]]\n")])])]),a("h4",{attrs:{id:"args-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("target")]),e._v(": Tensor to be differentiated.")]),e._v(" "),a("li",[a("code",[e._v("sources")]),e._v(": a list or nested structure of Tensors or Variables. "),a("code",[e._v("target")]),e._v(" will be differentiated against elements in "),a("code",[e._v("sources")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("unconnected_gradients")]),e._v(": a value which can either hold 'none' or 'zero' and alters the value which will be returned if the "),a("code",[e._v("target")]),e._v(" and "),a("code",[e._v("sources")]),e._v(" are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.")]),e._v(" "),a("li",[a("code",[e._v("parallel_iterations")]),e._v(": A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage.")]),e._v(" "),a("li",[a("code",[e._v("experimental_use_pfor")]),e._v(": If true, vectorizes the jacobian computation. Else falls back to a sequential while_loop. Vectorization can sometimes fail or lead to excessive memory usage. This option can be used to disable vectorization in such cases.")])]),e._v(" "),a("h4",{attrs:{id:"returns-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A list or nested structure of Tensors (or None), one for each element in "),a("code",[e._v("sources")]),e._v(". Returned structure is the same as the structure of "),a("code",[e._v("sources")]),e._v(". Note if any gradient is sparse (IndexedSlices), jacobian function currently makes it dense and returns a Tensor instead. This may change in the future.")]),e._v(" "),a("h4",{attrs:{id:"raises-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-3","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("RuntimeError")]),e._v(": If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor.")]),e._v(" "),a("li",[a("code",[e._v("ValueError")]),e._v(": If vectorization of jacobian computation fails.")])]),e._v(" "),a("h3",{attrs:{id:"reset"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reset","aria-hidden":"true"}},[e._v("#")]),e._v(" reset")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L895-L929",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" reset()\n")])])]),a("p",[e._v("Clears all information stored in this tape.\nEquivalent to exiting and reentering the tape context manager with a new tape. For example, the two following code blocks are equivalent:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" with tf.GradientTape() as t:\n  loss = loss_fn()\nwith tf.GradientTape() as t:\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n\n\n# The following is equivalent to the above\nwith tf.GradientTape() as t:\n  loss = loss_fn()\n  t.reset()\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n")])])]),a("p",[e._v("This is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" with tf.GradientTape() as t:\n  loss = ...\n  if loss > k:\n    t.reset()\n")])])]),a("h3",{attrs:{id:"stop-recording"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stop-recording","aria-hidden":"true"}},[e._v("#")]),e._v(" stop_recording")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L863-L893",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" stop_recording()\n")])])]),a("p",[e._v("Temporarily stops recording operations on this tape.\nOperations executed while this context manager is active will not be recorded on the tape. This is useful for reducing the memory used by tracing all computations.")]),e._v(" "),a("h4",{attrs:{id:"for-example"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#for-example","aria-hidden":"true"}},[e._v("#")]),e._v(" For example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("   with tf.GradientTape(persistent=True) as t:\n    loss = compute_loss(model)\n    with t.stop_recording():\n      # The gradient computation below is not traced, saving memory.\n      grads = t.gradient(loss, model.variables)\n")])])]),a("h4",{attrs:{id:"yields"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#yields","aria-hidden":"true"}},[e._v("#")]),e._v(" Yields:")]),e._v(" "),a("p",[e._v("None")]),e._v(" "),a("h4",{attrs:{id:"raises-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-4","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("RuntimeError")]),e._v(": if the tape is not currently recording.")])]),e._v(" "),a("h3",{attrs:{id:"watch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#watch","aria-hidden":"true"}},[e._v("#")]),e._v(" watch")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L837-L861",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" watch(tensor)\n")])])]),a("p",[e._v("Ensures that "),a("code",[e._v("tensor")]),e._v(" is being traced by this tape.")]),e._v(" "),a("h4",{attrs:{id:"args-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("tensor")]),e._v(": a Tensor or list of Tensors.")])]),e._v(" "),a("h4",{attrs:{id:"raises-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises-5","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": if it encounters something that is not a tensor.")])]),e._v(" "),a("h3",{attrs:{id:"watched-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#watched-variables","aria-hidden":"true"}},[e._v("#")]),e._v(" watched_variables")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/backprop.py#L931-L933",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" watched_variables()\n")])])]),a("p",[e._v("Returns variables watched by this tape in order of construction.")])])}),[],!1,null,null,null);t.default=_.exports}}]);