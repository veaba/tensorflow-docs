(window.webpackJsonp=window.webpackJsonp||[]).push([[2008],{2196:function(a,e,n){"use strict";n.r(e);var t=n(0),i=Object(t.a)({},(function(){var a=this,e=a.$createElement,n=a._self._c||e;return n("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[n("p",[a._v("Returns a batched diagonal tensor with given batched diagonal values.")]),a._v(" "),n("h3",{attrs:{id:"aliases"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[a._v("#")]),a._v(" Aliases:")]),a._v(" "),n("ul",[n("li",[n("code",[a._v("tf.compat.v1.linalg.diag")])]),a._v(" "),n("li",[n("code",[a._v("tf.compat.v1.matrix_diag")])]),a._v(" "),n("li",[n("code",[a._v("tf.compat.v2.linalg.diag")])])]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[a._v(" tf.linalg.diag(\n    diagonal,\n    name='diag',\n    k=0,\n    num_rows=-1,\n    num_cols=-1,\n    padding_value=0\n)\n")])])]),n("p",[a._v("Returns a tensor with the contents in diagonal as k[0]-th to k[1]-th diagonals of a matrix, with everything else padded with padding. num_rows and num_cols specify the dimension of the innermost matrix of the output. If both are not specified, the op assumes the innermost matrix is square and infers its size from k and the innermost dimension of diagonal. If only one of them is specified, the op assumes the unspecified value is the smallest possible based on other criteria.")]),a._v(" "),n("p",[a._v("Let diagonal have r dimensions [I, J, ..., L, M, N]. The output tensor has rank r+1 with shape [I, J, ..., L, M, num_rows, num_cols] when only one diagonal is given (k is an integer or k[0] == k[1]). Otherwise, it has rank r with shape [I, J, ..., L, num_rows, num_cols].")]),a._v(" "),n("p",[a._v("The second innermost dimension of diagonal has double meaning. When k is scalar or k[0] == k[1], M is part of the batch size [I, J, ..., M], and the output tensor is:")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[a._v(" output[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper\n    output[i, j, ..., l, m, n]                ; otherwise\n")])])]),n("p",[a._v("Otherwise, M is treated as the number of diagonals for the matrix in the same batch (M = k[1]-k[0]+1), and the output tensor is:")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[a._v(" output[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, k[1]-d, n-max(d, 0)] ; if d_lower <= d <= d_upper\n    input[i, j, ..., l, m, n]                   ; otherwise\n")])])]),n("p",[a._v("where d = n - m")]),a._v(" "),n("h4",{attrs:{id:"for-example"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#for-example","aria-hidden":"true"}},[a._v("#")]),a._v(" For example:")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[a._v(" # The main diagonal.\ndiagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)\n                     [5, 6, 7, 8]])\ntf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)\n                               [0, 2, 0, 0],\n                               [0, 0, 3, 0],\n                               [0, 0, 0, 4]],\n                              [[5, 0, 0, 0],\n                               [0, 6, 0, 0],\n                               [0, 0, 7, 0],\n                               [0, 0, 0, 8]]]\n\n# A superdiagonal (per batch).\ndiagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_diag(diagonal, k = 1)\n  ==> [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\n        [0, 0, 2, 0],\n        [0, 0, 0, 3],\n        [0, 0, 0, 0]],\n       [[0, 4, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6],\n        [0, 0, 0, 0]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[1, 2, 3],  # Input shape: (2, 2, 3)\n                       [4, 5, 0]],\n                      [[6, 7, 9],\n                       [9, 1, 0]]])\ntf.matrix_diag(diagonals, k = (-1, 0))\n  ==> [[[1, 0, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 0],\n        [0, 5, 3]],\n       [[6, 0, 0],\n        [9, 7, 0],\n        [0, 1, 9]]]\n\n# Rectangular matrix.\ndiagonal = np.array([1, 2])  # Input shape: (2)\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, num_cols = 4)\n  ==> [[0, 0, 0, 0],  # Output shape: (3, 4)\n       [1, 0, 0, 0],\n       [0, 2, 0, 0]]\n\n# Rectangular matrix with inferred num_cols and padding = 9.\ntf.matrix_diag(diagonal, k = -1, num_rows = 3, padding = 9)\n  ==> [[9, 9],  # Output shape: (3, 2)\n       [1, 9],\n       [9, 2]]\n")])])]),n("h4",{attrs:{id:"args"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[a._v("#")]),a._v(" Args:")]),a._v(" "),n("ul",[n("li",[n("code",[a._v("diagonal")]),a._v(": A "),n("code",[a._v("Tensor")]),a._v(" with "),n("code",[a._v("rank k >= 1")]),a._v(".")]),a._v(" "),n("li",[n("code",[a._v("name")]),a._v(": A "),n("code",[a._v("name")]),a._v(" for the operation (optional).")]),a._v(" "),n("li",[n("code",[a._v("k")]),a._v(": Diagonal offset(s). Positive value means super"),n("code",[a._v("diagonal")]),a._v(", 0 refers to the main "),n("code",[a._v("diagonal")]),a._v(", and negative value means sub"),n("code",[a._v("diagonal")]),a._v("s. "),n("code",[a._v("k")]),a._v(" can be a single integer (for a single "),n("code",[a._v("diagonal")]),a._v(") or a pair of integers specifying the low and high ends of a matrix band. "),n("code",[a._v("k")]),a._v("[0] must not be larger than "),n("code",[a._v("k")]),a._v("[1].")]),a._v(" "),n("li",[n("code",[a._v("num_rows")]),a._v(": The number of rows of the output matrix. If it is not provided, the op assumes the output matrix is a square matrix and infers the matrix size from "),n("code",[a._v("d_lower")]),a._v(", "),n("code",[a._v("d_upper")]),a._v(", and the innermost dimension of "),n("code",[a._v("diagonal")]),a._v(".")]),a._v(" "),n("li",[n("code",[a._v("num_cols")]),a._v(": The number of columns of the output matrix. If it is not provided, the op assumes the output matrix is a square matrix and infers the matrix size from "),n("code",[a._v("d_lower")]),a._v(", "),n("code",[a._v("d_upper")]),a._v(", and the innermost dimension of "),n("code",[a._v("diagonal")]),a._v(".")]),a._v(" "),n("li",[n("code",[a._v("padding_value")]),a._v(": The value to fill the area outside the specified "),n("code",[a._v("diagonal")]),a._v(" band with. Default is 0.")])]),a._v(" "),n("h4",{attrs:{id:"returns"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[a._v("#")]),a._v(" Returns:")]),a._v(" "),n("p",[a._v("A Tensor. Has the same type as diagonal.")])])}),[],!1,null,null,null);e.default=i.exports}}]);