(window.webpackJsonp=window.webpackJsonp||[]).push([[1774],{1965:function(e,t,a){"use strict";a.r(t);var o=a(0),s=Object(o.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"class-locallyconnected2d"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#class-locallyconnected2d","aria-hidden":"true"}},[e._v("#")]),e._v(" Class LocallyConnected2D")]),e._v(" "),a("p",[e._v("Locally-connected layer for 2D inputs.\n"),a("a",{attrs:{href:"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer",target:"_blank",rel:"noopener noreferrer"}},[e._v("Layer"),a("OutboundLink")],1),e._v("Inherits From:")]),e._v(" "),a("h3",{attrs:{id:"aliases"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aliases","aria-hidden":"true"}},[e._v("#")]),e._v(" Aliases:")]),e._v(" "),a("ul",[a("li",[e._v("Class "),a("code",[e._v("tf.compat.v1.keras.layers.LocallyConnected2D")])]),e._v(" "),a("li",[e._v("Class "),a("code",[e._v("tf.compat.v2.keras.layers.LocallyConnected2D")])])]),e._v(" "),a("p",[e._v("The LocallyConnected2D layer works similarly to the Conv2D layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.")]),e._v(" "),a("h4",{attrs:{id:"examples"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#examples","aria-hidden":"true"}},[e._v("#")]),e._v(" Examples:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('     # apply a 3x3 unshared weights convolution with 64 output filters on a\n    32x32 image\n    # with `data_format="channels_last"`:\n    model = Sequential()\n    model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))\n    # now model.output_shape == (None, 30, 30, 64)\n    # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64\n    parameters\n\n    # add a 3x3 unshared weights convolution on top, with 32 output filters:\n    model.add(LocallyConnected2D(32, (3, 3)))\n    # now model.output_shape == (None, 28, 28, 32)\n')])])]),a("h4",{attrs:{id:"arguments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arguments","aria-hidden":"true"}},[e._v("#")]),e._v(" Arguments:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("filters")]),e._v(": Integer, the dimensionality of the output space (i.e. the number of output "),a("code",[e._v("filters")]),e._v(" in the convolution).")]),e._v(" "),a("li",[a("code",[e._v("kernel_size")]),e._v(": An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.")]),e._v(" "),a("li",[a("code",[e._v("strides")]),e._v(": An integer or tuple/list of 2 integers, specifying the "),a("code",[e._v("strides")]),e._v(" of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions.")]),e._v(" "),a("li",[a("code",[e._v("padding")]),e._v(": Currently only support "),a("code",[e._v('"valid"')]),e._v(" (case-insensitive). "),a("code",[e._v('"same"')]),e._v(" will be supported in future.")]),e._v(" "),a("li",[a("code",[e._v("data_format")]),e._v(": A string, one of "),a("code",[e._v("channels_last")]),e._v(" (default) or "),a("code",[e._v("channels_first")]),e._v(". The ordering of the dimensions in the inputs. "),a("code",[e._v("channels_last")]),e._v(" corresponds to inputs with shape ("),a("code",[e._v("batch, height, width, channels")]),e._v(") while "),a("code",[e._v("channels_first")]),e._v(" corresponds to inputs with shape ("),a("code",[e._v("batch, channels, height, width")]),e._v("). It defaults to the "),a("code",[e._v("image_data_format")]),e._v(" value found in your Keras config file at "),a("code",[e._v("~/.keras/keras.json")]),e._v('. If you never set it, then it will be "'),a("code",[e._v("channels_last")]),e._v('".')]),e._v(" "),a("li",[a("code",[e._v("activation")]),e._v(": Activation function to use. If you don't specify anything, no "),a("code",[e._v("activation")]),e._v(' is applied (ie. "linear" '),a("code",[e._v("activation")]),e._v(": a(x) = x).")]),e._v(" "),a("li",[a("code",[e._v("use_bias")]),e._v(": Boolean, whether the layer uses a bias vector.")]),e._v(" "),a("li",[a("code",[e._v("kernel_initializer")]),e._v(": Initializer for the "),a("code",[e._v("kernel")]),e._v(" weights matrix.")]),e._v(" "),a("li",[a("code",[e._v("bias_initializer")]),e._v(": Initializer for the bias vector.")]),e._v(" "),a("li",[a("code",[e._v("kernel")]),e._v("_regularizer: Regularizer function applied to the "),a("code",[e._v("kernel")]),e._v(" weights matrix.")]),e._v(" "),a("li",[a("code",[e._v("bias_regularizer")]),e._v(": Regularizer function applied to the bias vector.")]),e._v(" "),a("li",[a("code",[e._v("activity_regularizer")]),e._v(': Regularizer function applied to the output of the layer (its "'),a("code",[e._v("activation")]),e._v('").')]),e._v(" "),a("li",[a("code",[e._v("kernel")]),e._v("_constraint: Constraint function applied to the "),a("code",[e._v("kernel")]),e._v(" matrix.")]),e._v(" "),a("li",[a("code",[e._v("bias_constraint")]),e._v(": Constraint function applied to the bias vector.")]),e._v(" "),a("li",[a("code",[e._v("implementation")]),e._v(": "),a("code",[e._v("implementation")]),e._v(" mode"),a("code",[e._v(",")]),e._v(" either "),a("code",[e._v("1``,")]),e._v(" "),a("code",[e._v("2``,")]),e._v(" or "),a("code",[e._v("3")]),e._v(". "),a("code",[e._v("1")]),e._v(" loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops.\n"),a("code",[e._v("2")]),e._v(" stores layer weights in a dense but sparsely-populated "),a("code",[e._v("2")]),e._v("D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops.\n"),a("code",[e._v("3")]),e._v(" stores layer weights in a sparse tensor and implements the forward pass as a single sparse matrix-multiply.\nHow to choose:\n"),a("code",[e._v("1")]),e._v(": large"),a("code",[e._v(",")]),e._v(" dense models"),a("code",[e._v(",")]),e._v(" "),a("code",[e._v("2")]),e._v(": small models"),a("code",[e._v(",")]),e._v(" "),a("code",[e._v("3")]),e._v(": large"),a("code",[e._v(",")]),e._v(" sparse models"),a("code",[e._v(",")]),e._v('\nwhere "large" stands for large input/output '),a("code",[e._v("activation")]),e._v("s (i.e. many "),a("code",[e._v("filters``,")]),e._v(" "),a("code",[e._v("input_filters``,")]),e._v(" large np.prod(input_size)"),a("code",[e._v(",")]),e._v(" np.prod(output_size))"),a("code",[e._v(",")]),e._v(' and "sparse" stands for few connections between inputs and outputs'),a("code",[e._v(",")]),e._v(" i.e. small ratio "),a("code",[e._v("filters` * `input_filters` * np.prod(`kernel_size`) / (np.prod(input_size) np.prod(`strides`))`, where inputs to and outputs of the layer are assumed to have shapes`input_size + (`input_filters")]),e._v(","),a("code",[e._v(")")]),e._v(","),a("code",[e._v("output_size + (")]),e._v("filters``,"),a("code",[e._v(")")]),e._v(" respectively.\nIt is recommended to benchmark each in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Correct choice of "),a("code",[e._v("implementation")]),e._v(" can lead to dramatic speed improvements (e.g. 50X)"),a("code",[e._v(",")]),e._v(" potentially at the expense of RAM.\nAlso"),a("code",[e._v(",")]),e._v(" only "),a("code",[e._v("padding")]),e._v("="),a("code",[e._v('"valid"')]),e._v(" is supported by "),a("code",[e._v("implementation")]),e._v("="),a("code",[e._v("1")]),e._v(".")]),e._v(" "),a("li",[e._v("np.prod("),a("code",[e._v("strides")]),e._v("))"),a("code",[e._v(", where inputs to and outputs of the layer are assumed to have shapes")]),e._v("input_size + ("),a("code",[e._v("input_filters``,")]),e._v(")"),a("code",[e._v(",")]),e._v("output_size + ("),a("code",[e._v("filters``,")]),e._v(")` respectively.")])]),e._v(" "),a("h4",{attrs:{id:"input-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-shape","aria-hidden":"true"}},[e._v("#")]),e._v(" Input shape:")]),e._v(" "),a("p",[e._v("4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'.")]),e._v(" "),a("h4",{attrs:{id:"output-shape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#output-shape","aria-hidden":"true"}},[e._v("#")]),e._v(" Output shape:")]),e._v(" "),a("p",[e._v("4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding.")]),e._v(" "),a("h2",{attrs:{id:"init"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#init","aria-hidden":"true"}},[e._v("#")]),e._v(" "),a("strong",[e._v("init")])]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/local.py#L442-L479",target:"_blank",rel:"noopener noreferrer"}},[e._v("View source"),a("OutboundLink")],1)]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" __init__(\n    filters,\n    kernel_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    activation=None,\n    use_bias=True,\n    kernel_initializer='glorot_uniform',\n    bias_initializer='zeros',\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    implementation=1,\n    **kwargs\n)\n")])])])])}),[],!1,null,null,null);t.default=s.exports}}]);