(window.webpackJsonp=window.webpackJsonp||[]).push([[840],{1028:function(e,t,a){"use strict";a.r(t);var n=a(0),o=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("Shards computation along the batch dimension for parallel execution.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v(" tf.compat.v1.tpu.batch_parallel(\n    computation,\n    inputs=None,\n    num_shards=1,\n    infeed_queue=None,\n    device_assignment=None,\n    name=None\n)\n")])])]),a("p",[e._v("Convenience wrapper around shard().")]),e._v(" "),a("p",[e._v("inputs must be a list of Tensors or None (equivalent to an empty list). Each input is split into num_shards pieces along the 0-th dimension, and computation is applied to each shard in parallel.")]),e._v(" "),a("p",[e._v("Tensors are broadcast to all shards if they are lexically captured by computation. e.g.,")]),e._v(" "),a("p",[e._v("x = tf.constant(7) def computation(): return x + 3 ... = shard(computation, ...)")]),e._v(" "),a("p",[e._v("The outputs from all shards are concatenated back together along their 0-th dimension.")]),e._v(" "),a("p",[e._v("Inputs and outputs of the computation must be at least rank-1 Tensors.")]),e._v(" "),a("h4",{attrs:{id:"args"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#args","aria-hidden":"true"}},[e._v("#")]),e._v(" Args:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("computation")]),e._v(": A Python function that builds a "),a("code",[e._v("computation")]),e._v(" to apply to each shard of the input.")]),e._v(" "),a("li",[a("code",[e._v("inputs")]),e._v(": A list of input tensors or None (equivalent to an empty list). The 0-th dimension of each Tensor must have size divisible by "),a("code",[e._v("num_shards")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("num_shards")]),e._v(": The number of shards.")]),e._v(" "),a("li",[a("code",[e._v("infeed_queue")]),e._v(": If not "),a("code",[e._v("None")]),e._v(", the "),a("code",[e._v("InfeedQueue")]),e._v(" from which to append a tuple of arguments as "),a("code",[e._v("inputs")]),e._v(" to "),a("code",[e._v("computation")]),e._v(".")]),e._v(" "),a("li",[a("code",[e._v("device_assignment")]),e._v(": If not "),a("code",[e._v("None")]),e._v(", a "),a("code",[e._v("DeviceAssignment")]),e._v(" describing the mapping between logical cores in the "),a("code",[e._v("computation")]),e._v(" with physical cores in the TPU topology. Uses a default device assignment if "),a("code",[e._v("None")]),e._v(". The "),a("code",[e._v("DeviceAssignment")]),e._v(" may be omitted if each shard of the "),a("code",[e._v("computation")]),e._v(" uses only one core, and there is either only one shard, or the number of shards is equal to the number of cores in the TPU system.")]),e._v(" "),a("li",[a("code",[e._v("name")]),e._v(": (Deprecated) Does nothing.")])]),e._v(" "),a("h4",{attrs:{id:"returns"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#returns","aria-hidden":"true"}},[e._v("#")]),e._v(" Returns:")]),e._v(" "),a("p",[e._v("A list of output tensors.")]),e._v(" "),a("h4",{attrs:{id:"raises"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#raises","aria-hidden":"true"}},[e._v("#")]),e._v(" Raises:")]),e._v(" "),a("ul",[a("li",[a("code",[e._v("ValueError")]),e._v(": If "),a("code",[e._v("num_shards <= 0")])])])])}),[],!1,null,null,null);t.default=o.exports}}]);